<!-- build time:Sat Feb 03 2018 23:53:24 GMT+0800 (CST) --><!DOCTYPE html><html class="theme-next mist" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3"><link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222"><meta name="keywords" content="Hexo, NexT"><meta name="description" content="聚类的概念上一章中讲了如何寻找与自己相似的用户，本章在上一章的思想加以扩展，引入聚类的概念。聚类的概念为：将紧密相关的事物、人或观点聚集在一起，并将其可视化的方法。本章节我们将会学习到以下内容：从不同来源中构造算法所需要的数据两种不同的聚类算法更多的有关距离度量(distance metrics)的知识简单的图形可视化"><meta property="og:type" content="article"><meta property="og:title" content="发现群组"><meta property="og:url" content="laiqun.github.io/2017/11/12/cluster/index.html"><meta property="og:site_name" content="广阔天地，大有作为"><meta property="og:description" content="聚类的概念上一章中讲了如何寻找与自己相似的用户，本章在上一章的思想加以扩展，引入聚类的概念。聚类的概念为：将紧密相关的事物、人或观点聚集在一起，并将其可视化的方法。本章节我们将会学习到以下内容：从不同来源中构造算法所需要的数据两种不同的聚类算法更多的有关距离度量(distance metrics)的知识简单的图形可视化，用来观察聚类的结果学习将异常复杂的模型投影到二维空间中聚类的应用：商家用来查找"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="/2017/11/12/cluster/hierarchical.PNG"><meta property="og:image" content="/2017/11/12/cluster/tree.PNG"><meta property="og:image" content="/2017/11/12/cluster/tree.PNG"><meta property="og:image" content="/2017/11/12/cluster/drawtree.PNG"><meta property="og:image" content="/2017/11/12/cluster/wordcluster.PNG"><meta property="og:image" content="/2017/11/12/cluster/kmeans.PNG"><meta property="og:image" content="/2017/11/12/cluster/zebo.PNG"><meta property="og:image" content="/2017/11/12/cluster/init.PNG"><meta property="og:image" content="/2017/11/12/cluster/calcute.PNG"><meta property="og:image" content="/2017/11/12/cluster/move.PNG"><meta property="og:image" content="/2017/11/12/cluster/manha.jpg"><meta property="og:updated_time" content="2017-12-03T02:43:46.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="发现群组"><meta name="twitter:description" content="聚类的概念上一章中讲了如何寻找与自己相似的用户，本章在上一章的思想加以扩展，引入聚类的概念。聚类的概念为：将紧密相关的事物、人或观点聚集在一起，并将其可视化的方法。本章节我们将会学习到以下内容：从不同来源中构造算法所需要的数据两种不同的聚类算法更多的有关距离度量(distance metrics)的知识简单的图形可视化，用来观察聚类的结果学习将异常复杂的模型投影到二维空间中聚类的应用：商家用来查找"><meta name="twitter:image" content="/2017/11/12/cluster/hierarchical.PNG"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",version:"5.1.3",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!0},fancybox:!0,tabs:!0,motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="laiqun.github.io/2017/11/12/cluster/"><title>发现群组 | 广阔天地，大有作为</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">广阔天地，大有作为</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">你看到我的筋斗云了嘛？</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="laiqun.github.io/2017/11/12/cluster/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="倔强的土豆"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="广阔天地，大有作为"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">发现群组</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-12T09:46:33+08:00">2017-11-12 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/11/12/cluster/#comments" itemprop="discussionUrl"><span class="post-comments-count gitment-comments-count" data-xid="/2017/11/12/cluster/" itemprop="commentsCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="聚类的概念"><a href="#聚类的概念" class="headerlink" title="聚类的概念"></a>聚类的概念</h1><p>上一章中讲了如何寻找与自己相似的用户，本章在上一章的思想加以扩展，引入<strong>聚类</strong>的概念。<br>聚类的概念为：</p><blockquote><p>将紧密相关的事物、人或观点聚集在一起，并将其可视化的方法。</p></blockquote><p>本章节我们将会学习到以下内容：</p><ol><li>从不同来源中构造算法所需要的数据</li><li>两种不同的聚类算法</li><li>更多的有关距离度量(distance metrics)的知识</li><li>简单的图形可视化，用来观察聚类的结果</li><li>学习将异常复杂的模型投影到二维空间中</li></ol><p>聚类的应用：</p><ol><li>商家用来查找具有相似购买模式的消费者群体</li><li>寻找不同年龄和收入的着装风格，依照这个结果调整销售策略</li><li>生物学上，寻找相似行为的基因组</li></ol><p>本书讨论的是集体智慧，这章节也不例外，都是由多个人贡献不同的信息的问题。</p><p>本书将会从两个例子入手：</p><ol><li>对博客用户所讨论的话题，以及他们所使用的特殊词汇进行考查，根据用户所写的文字对博客进行分组（博客中由哪些词），也可以根据博客将词汇分组（词出现在哪些博客中）。</li><li>对社区网站进行考查，人们在这些网站上列举他们已经拥有的和希望拥有的物品，用这些信息将人们的意愿进行分组。</li></ol><p>聚类属于<strong>无监督学习</strong>的一种。除了<strong>无监督学习</strong>外，还存在<strong>有监督学习</strong>。这里简要的介绍下有监督学习和无监督学习的概念。</p><ul><li>有监督学习<br>邮件都学习利用<strong>样本输入</strong>和<strong>期望的输出</strong>（作为答案）来学习到<strong>模型</strong>，这个阶段叫<strong>训练</strong>，模型记录着它学习到的知识。学习到<strong>模型</strong>后，将<strong>新数据</strong>输入到<strong>模型</strong>，期望算法能根据之前学习到的知识产生一个输出，输出的的结果(<strong>预测</strong>)是否接近真实值。这里是为了测试学习效果，叫<strong>测试</strong>或者<strong>检验</strong>。<br>本书中，我们会学习到许多监督学习方法，包括：神经网络、决策树、向量机、以及贝叶斯过滤，每个学习方法都有具体的代码实现。充电满满有没有😄！</li><li>无监督学习<br>无监督学习与神经网络或决策树不同，无监督学习方法不少利用带有正确答案的样本数据进行“训练”。无监督学习的目的是在一组数据中寻找某种结构。聚类算法的目标是采集数据，然后将它们分成不同的群组。其他的无监督学习的例子还包括<strong>非负矩阵因式分解</strong>（将在后续章节中讲解）和<strong>自组织映射</strong>。</li></ul><h1 id="对不同用户的博客进行分组"><a href="#对不同用户的博客进行分组" class="headerlink" title="对不同用户的博客进行分组"></a>对不同用户的博客进行分组</h1><p>我们首先要对每个博客，依据其下的所有文章，建立单词频率表。然后依靠这个不同博客的单词词频表，来对博客进行聚类。</p><h2 id="为每篇博客建立“单词频度表”（也叫单词向量表）"><a href="#为每篇博客建立“单词频度表”（也叫单词向量表）" class="headerlink" title="为每篇博客建立“单词频度表”（也叫单词向量表）"></a>为每篇博客建立“单词频度表”（也叫单词向量表）</h2><p>为聚类算法准备数据的常见做法是定义<strong>一组公共的数值型属性</strong>，我们可以利用这些属性对<strong>数据项进行比较</strong>。这和我们推荐系统的做法相同，那里我们比较了评论者对同样一组电影所给出的评分情况。<br>本节将示例一个含有97个博客，为了对这些数据聚类，我们需要<strong>一组指定的词汇表</strong>在每个博客订阅源中出现的次数。</p><p>下表为展示的一小部分子集：</p><table><thead><tr><th>用户给博客起的名字</th><th>“china”</th><th>“kids”</th><th>“music”</th><th>“yahoo”</th></tr></thead><tbody><tr><td>Gothamist</td><td>0</td><td>3</td><td>3</td><td>0</td></tr><tr><td>Gigaom</td><td>6</td><td>0</td><td>0</td><td>2</td></tr><tr><td>Quick Online Tips</td><td>0</td><td>2</td><td>2</td><td>22</td></tr></tbody></table><blockquote><p>说明：Gothamist为纽约城市新闻网；GigaOM是一个科技博客网站，成立于2006年，现已发展成为全球领先的为科技革新者提供互联网传媒、新闻和搜索服务的网站。GigaOM公司将商业和科技结合起来，为用户提供最可信、最有洞察力的新闻资讯和见解。该公司网站每月的绝对访客人数超过四百五十万人次。Qucik online tips是提供在线科技技巧的网站。</p></blockquote><p>根据单词出现的频度对博客进行聚类，可以帮助我们分析出是否存在这样一类的博客用户，他们经常撰写相似的主题，或在写作风格上十分类似。这样的分析结果对搜索、分类和挖掘大量的在线博客而言，可能是非常用价值的。</p><h3 id="下载和处理博客订阅源"><a href="#下载和处理博客订阅源" class="headerlink" title="下载和处理博客订阅源"></a>下载和处理博客订阅源</h3><p>如果你想跳过构造数据集这一环节，可以直接使用blogdata.txt的现成数据集。</p><p>几乎所有的博客都可以在线阅读，或通过<strong>RSS订阅源进行阅读</strong>。<br>一个RSS订阅源是一个xml文件，里面放着一个博客中的所有文章。<br>为了给每个博客的单词计数，首先第一步就是要解析这些订阅源。有一个现成的python库可以使用，“Univeral Feed Parser”，可以从pypi.python.com中下载。</p><p>借助“Universal Feed Parser”，可以轻松的获取博客标题，以及其所有的文章标题，内容和链接。</p><p>接下来，我们编写一个从订阅源中提取所有单词的函数。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文件名generatefeedvector.py</span></span><br><span class="line"><span class="keyword">import</span> feedparser</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># Returns title and dictionary of word counts for an RSS feed</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getwordcounts</span><span class="params">(url)</span>:</span></span><br><span class="line">  <span class="comment"># Parse the feed</span></span><br><span class="line">  d=feedparser.parse(url)  <span class="comment"># 导入博客的url进行解析</span></span><br><span class="line">  wc=&#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Loop over all the entries  对于每一篇文章</span></span><br><span class="line">  <span class="keyword">for</span> e <span class="keyword">in</span> d.entries:<span class="comment"># 如果又摘要，就拿摘要，否则就要全文</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">'summary'</span> <span class="keyword">in</span> e: summary=e.summary </span><br><span class="line">    <span class="keyword">else</span>: summary=e.description</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Extract a list of words# 将文章标题也加入正文，提取单词表</span></span><br><span class="line">    words=getwords(e.title+<span class="string">' '</span>+summary) </span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">      wc.setdefault(word,<span class="number">0</span>)</span><br><span class="line">      wc[word]+=<span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> d.feed.title,wc</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getwords</span><span class="params">(html)</span>:</span></span><br><span class="line">  <span class="comment"># Remove all the HTML tags 去掉html标记</span></span><br><span class="line">  txt=re.compile(<span class="string">r'&lt;[^&gt;]+&gt;'</span>).sub(<span class="string">''</span>,html)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Split words by all non-alpha characters</span></span><br><span class="line">  <span class="comment"># 利用非字母字符作为分隔符，分割单词</span></span><br><span class="line">  words=re.compile(<span class="string">r'[^A-Z^a-z]+'</span>).split(txt)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Convert to lowercase</span></span><br><span class="line">  <span class="keyword">return</span> [word.lower() <span class="keyword">for</span> word <span class="keyword">in</span> words <span class="keyword">if</span> word!=<span class="string">''</span>]</span><br></pre></td></tr></table></figure><p></p><p>以上函数的功能是解释一个url中所有的单词。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">apcount=&#123;&#125;</span><br><span class="line">wordcounts=&#123;&#125;</span><br><span class="line"><span class="comment"># feedlist.txt 是热门博客的集合，并去除了正文为空或是只有图片的博客</span></span><br><span class="line"><span class="comment"># 其中每一行都是一个博客的url</span></span><br><span class="line">f.open(<span class="string">'feedlist.txt'</span>)</span><br><span class="line">feedlist= f.readlines()</span><br><span class="line"><span class="keyword">for</span> feedurl <span class="keyword">in</span> feedlist:</span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    title,wc=getwordcounts(feedurl)</span><br><span class="line">    wordcounts[title]=wc</span><br><span class="line">    <span class="keyword">for</span> word,count <span class="keyword">in</span> wc.items():</span><br><span class="line">      apcount.setdefault(word,<span class="number">0</span>)</span><br><span class="line">      <span class="keyword">if</span> count&gt;<span class="number">1</span>:</span><br><span class="line"><span class="comment"># apcount统计几篇博客中出现过这个词，用来过滤常用，但无意义的词</span></span><br><span class="line"><span class="comment"># 比如‘the’ ‘a’ ‘is’ 等等.</span></span><br><span class="line"><span class="comment"># apcount也用来去掉过于生僻的词</span></span><br><span class="line">        apcount[word]+=<span class="number">1</span></span><br><span class="line">  <span class="keyword">except</span>:</span><br><span class="line">    print(<span class="string">'Failed to parse feed %s'</span> % feedurl)</span><br></pre></td></tr></table></figure><p></p><p>因为像‘the’这样词过于常见，几乎出现在每个博客中，而像‘flim-flam’只出现在少数个别的博客中。故我们需要通过该词出现在几篇博客中，占总博客的半分比来去掉过于生僻和过于常见的词。这里的‘生僻’的界限为百分比小于10%，常见的的界限为大于50%，故我们只保留频率在10%以上、50%以下的。<br>这样做也可以减少需要考查的单词总量。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">wordlist=[]</span><br><span class="line"><span class="comment"># 下一行，w表示单词，bc指几篇博客中出现过这个词</span></span><br><span class="line"><span class="keyword">for</span> w,bc <span class="keyword">in</span> apcount.items():</span><br><span class="line">  frac=float(bc)/len(feedlist)</span><br><span class="line">  <span class="comment"># 取出掉过于常见的词和特别生僻的词</span></span><br><span class="line">  <span class="comment"># 选介于某个百分比范围内的单词</span></span><br><span class="line">  <span class="keyword">if</span> frac&gt;<span class="number">0.1</span> <span class="keyword">and</span> frac&lt;<span class="number">0.5</span>:</span><br><span class="line">    wordlist.append(w)</span><br><span class="line"></span><br><span class="line">out=file(<span class="string">'blogdata1.txt'</span>,<span class="string">'w'</span>)</span><br><span class="line">out.write(<span class="string">'Blog'</span>)</span><br><span class="line"><span class="comment"># 建立表头，   blog     单词1     单词2 ... 单词n</span></span><br><span class="line"><span class="comment"># 每一行格式为 博客名 单词1词频 单词2词频  单词n词频</span></span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> wordlist: out.write(<span class="string">'\t%s'</span> % word)</span><br><span class="line">out.write(<span class="string">'\n'</span>)</span><br><span class="line"><span class="comment"># 建立每一行数据，一行为一个博客的博客名，和构成它的词汇频度表组成</span></span><br><span class="line"><span class="keyword">for</span> blog,wc <span class="keyword">in</span> wordcounts.items():</span><br><span class="line">  print(blog)</span><br><span class="line">  out.write(blog)</span><br><span class="line">  <span class="keyword">for</span> word <span class="keyword">in</span> wordlist:</span><br><span class="line">    <span class="keyword">if</span> word <span class="keyword">in</span> wc: out.write(<span class="string">'\t%d'</span> % wc[word])</span><br><span class="line">    <span class="keyword">else</span>: out.write(<span class="string">'\t0'</span>) <span class="comment"># 没出现，词频写0</span></span><br><span class="line">  out.write(<span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure><p></p><p>生成的数据集每一列为一个单词，每一行为一个博客，以制表符作为分隔符。<br>格式为：</p><table><thead><tr><th>博客名</th><th>单词1</th><th>单词2</th><th>…</th><th>单词n</th></tr></thead><tbody><tr><td>博客1</td><td>单词1频度</td><td>单词2频度</td><td>…</td><td>单词n频度</td></tr><tr><td>博客2</td><td>单词1频度</td><td>单词2频度</td><td>…</td><td>单词n频度</td></tr><tr><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td></tr></tbody></table><h2 id="对博客使用分级聚类"><a href="#对博客使用分级聚类" class="headerlink" title="对博客使用分级聚类"></a>对博客使用分级聚类</h2><h3 id="分级聚类的概念"><a href="#分级聚类的概念" class="headerlink" title="分级聚类的概念"></a>分级聚类的概念</h3><p>分级聚类通过连续不断地将最为相似的群组两两合并，来构造出一个层级结构。一开始每个群组都是单一元素，这里的单一元素指博客。在每次迭代的过程中，分级聚类算法会计算两个群组间的距离，并将距离最近的两个群组合并成一个新的群组，<strong>合并得到新群组所在的位置为这两个元素的中间</strong>。这个过程会一直重复下去，直到只剩一个群组为止。<br><img src="/2017/11/12/cluster/hierarchical.PNG" title="分级聚类"><br>在上图中，元素的相似度是通过他们的相对位置来体现的——两个元素越近，它们就越相似。开始时，每个群组都是单一元素。在第一步中，可以看到A和B，这两个紧靠在一起的元素，已经合并成一个新的群组，新群组所在的位置位于这两个元素的中间。在第二步中，上一步得到的群组又与C进行了合并，同时计算新群组的位置。现在‘D和E之间的距离’小于‘第二步得到的群组与D或E的距离’，故将D和E合并为一个群组。最后将剩下的两个群组合并在一起，合并到只剩一个聚类为止。<br>分级聚类完成之后，我们可以用<strong>树形图</strong>将其图形化来展现结果。<br><img src="/2017/11/12/cluster/tree.PNG" title="树状图"><br>上图中的-1，-2，-3，-4为按顺序依次聚类形成的新群组（数字前的负号该群组不在原始集合中），即描述分级聚类过程的第一步合并得到的群组、第二步合并得到的群组、第三步合并得到的群组、第四步合并得到的群组。</p><p>连线长度表达聚类情况，也表达聚类的各个元素间的远近。“群组-1到A和B之间的距离”相对于“群组-3到群组D和E之间的距离” 要更短，这表明A和B之间关系更加<strong>紧密</strong>。另外，我们还注意到‘群组-1到A的距离’和‘群组-1到B的距离’是一样长的，因为<strong>合并得到新群组所在的位置为这两个元素的中间</strong>，你会看到所有合并得到的群组，到子群组的距离都是一样长。</p><h3 id="对博客数据即应用分级聚类"><a href="#对博客数据即应用分级聚类" class="headerlink" title="对博客数据即应用分级聚类"></a>对博客数据即应用分级聚类</h3><h4 id="处理数据文件"><a href="#处理数据文件" class="headerlink" title="处理数据文件"></a>处理数据文件</h4><p>数据格式为：</p><table><thead><tr><th>博客名</th><th>单词1</th><th>单词2</th><th>…</th><th>单词n</th></tr></thead><tbody><tr><td>博客1</td><td>单词1频度</td><td>单词2频度</td><td>…</td><td>单词n频度</td></tr><tr><td>博客2</td><td>单词1频度</td><td>单词2频度</td><td>…</td><td>单词n频度</td></tr><tr><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td></tr></tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readfile</span><span class="params">(filename)</span>:</span></span><br><span class="line">  lines=[line <span class="keyword">for</span> line <span class="keyword">in</span> file(filename)]</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># First line is the column titles</span></span><br><span class="line">  colnames=lines[<span class="number">0</span>].strip().split(<span class="string">'\t'</span>)[<span class="number">1</span>:]  <span class="comment"># 第一行，排除博客名，剩下的就是单词列表</span></span><br><span class="line">  rownames=[]</span><br><span class="line">  data=[]</span><br><span class="line">  <span class="keyword">for</span> line <span class="keyword">in</span> lines[<span class="number">1</span>:]:</span><br><span class="line">    p=line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">    <span class="comment"># 每行的第一列代表博客名</span></span><br><span class="line">    rownames.append(p[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 每行剩下的列代表词频</span></span><br><span class="line">    data.append([float(x) <span class="keyword">for</span> x <span class="keyword">in</span> p[<span class="number">1</span>:]])</span><br><span class="line">  <span class="keyword">return</span> rownames,colnames,data</span><br><span class="line">  <span class="comment"># rownames为博客名列表，colnames为单词列表，data为每个博客的词频列表</span></span><br><span class="line">  <span class="comment"># 根据博客名在博客名列表中的序号，单词在单词列表中的序号，可以索引data中的一个博客的词频数据</span></span><br><span class="line">  <span class="comment"># 可以根据data[x][y] 来定位博客中一个单词出现的次数，其中x序号可以rownames[x]来找到博客名,colnames[y]可找到单词名</span></span><br></pre></td></tr></table></figure><h4 id="定义相似度计算公式"><a href="#定义相似度计算公式" class="headerlink" title="定义相似度计算公式"></a>定义相似度计算公式</h4><p>这个我们在提供推荐那一章节已经讨论过欧几里德距离和皮尔逊相似度，这里不在叙述。</p><p>在本章中的博客数据中，由可能一些博客发表的日志更多一些，并且日志的长度相对其他博客的文章更长。因此用皮尔逊相关系数更好一些，因为它只是描述两组数据的相关性，正相关（0-1之间，越接近1越正相关）、负相关（-1-0之间，越接近-1越是负相关）、或者为0（两组数据毫无关系）。</p><p>本例中，皮尔逊系数越大，表明相似度越高，[-1,1],而我们本章描述的是相似度越近，越应该聚合在一起。即我们想要的是皮尔逊相似度计算结果越大的距离越小，故我们用1减去皮尔逊相似度的结果，这样完全正相关时结果为1.0-1.0=0，两组数据完全无关1.0-0=1，两组数据完全相反1.0-(-1.0)=2.0。</p><h4 id="构建层级树"><a href="#构建层级树" class="headerlink" title="构建层级树"></a>构建层级树</h4><p>我们用二叉树来描述这一结构，每个节点要包含实际的特征数据（词频特征），用于计算相似度，我们用vec变量来保存特征数据。除此之外，我们还要标记一个位置信息，其中叶节点代表的是一个博客的词频特征，这里用data的行号来标识是哪个博客；非叶节点应该是两个群组聚合的产物，我们用负数来标识（-1表示第一步聚合的结果，-2代表第二部聚合的结果，以此类推），我们用变量id来保存这个位置信息。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">bicluster</span>:</span>   <span class="comment"># 描述节点的结构</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>:</span>(self,vec,left=<span class="keyword">None</span>,right=<span class="keyword">None</span>,distance=<span class="number">0</span>,id=<span class="keyword">None</span>):</span><br><span class="line">    self.left = left</span><br><span class="line">    self.right = right</span><br><span class="line">    <span class="comment">#词频特征（vec），用来计算相似度的</span></span><br><span class="line">    <span class="comment">#如果是合并得到的群组，其词频特征应该将两个子群组相应单词相加，除上2</span></span><br><span class="line">    <span class="comment">#因为新群组的位置位于两个子群组中间</span></span><br><span class="line">    self.vec = vec</span><br><span class="line">    <span class="comment"># 叶子节点id为data数据的行号，非叶子节点表示第几步聚合的结果 </span></span><br><span class="line">    self.id = id </span><br><span class="line">    <span class="comment"># 到两个孩子节点之间的连线长度，体现在树状图中的线长</span></span><br><span class="line">    self.distance = distance</span><br></pre></td></tr></table></figure><p></p><p>下面我们开始来写构建二叉树的算法：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hcluster</span>:</span>(rows,pearson):</span><br><span class="line">    distances=&#123;&#125;</span><br><span class="line">    currentclustid=<span class="number">-1</span>  <span class="comment">#非叶节点的起始id号,代表聚类的形成的第几个群组</span></span><br><span class="line">    <span class="comment"># 最开始的数据是，数据集中的行</span></span><br><span class="line">    clust = [bicluster(vec=rows[i],id=i) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(rows))]</span><br><span class="line">    <span class="keyword">while</span> len(clust)&gt;<span class="number">1</span>:</span><br><span class="line">      lowestpair=(<span class="number">0</span>,<span class="number">1</span>)                            <span class="comment"># 初始化最小距离的配对</span></span><br><span class="line">      closest = pearson(clust[<span class="number">0</span>].vec,clust[<span class="number">1</span>].vec) <span class="comment"># 初始化最小距离配对的距离值</span></span><br><span class="line">      <span class="comment"># 依次选出两两组合，计算相似度，更新最小距离的配对</span></span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> range(len(clust)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>,len(clust)):</span><br><span class="line">          <span class="comment">#用distances字典来缓存距离的计算值</span></span><br><span class="line">          <span class="keyword">if</span> (clust[i].id,clust[j].id) <span class="keyword">not</span> <span class="keyword">in</span> distances:</span><br><span class="line">            distances[(clust[i].id,clust[j].id)]=pearson(clust[i].vec,clust[j].vec)</span><br><span class="line">          d = distances[(clust[i].id,clust[j].id)]</span><br><span class="line">          <span class="keyword">if</span> d&lt;closest:</span><br><span class="line">            closest=d</span><br><span class="line">            lowestpair=(i,j)</span><br><span class="line">      <span class="comment"># 计算新群组的词频平均值——为两个子群组的平均值</span></span><br><span class="line">      mergevec = [(clust[lowestpair[<span class="number">0</span>]].vec[x]+clust[lowestpair[<span class="number">1</span>]].vec[x])/<span class="number">2</span>) <span class="keyword">for</span> x <span class="keyword">in</span> range[len(clust[<span class="number">0</span>].vec)]]</span><br><span class="line">      <span class="comment"># 建立新的群组</span></span><br><span class="line">      newcluster = bicluster(vec=mergevec,left=clust[lowestpair[<span class="number">0</span>]],right=clust[lowestpair[<span class="number">1</span>]],distance=closest,id=currentclustid)</span><br><span class="line">      <span class="comment"># 不在原始集合中的聚类，其id为负数，currentclustid减一，代表下一步形成的新群组id</span></span><br><span class="line">      currentclustid -=<span class="number">1</span></span><br><span class="line">      <span class="keyword">del</span> clust[lowestpair[<span class="number">0</span>]]</span><br><span class="line">      <span class="keyword">del</span> clust[lowestpair[<span class="number">1</span>]]</span><br><span class="line">      clust.append(newcluster)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> clust[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p></p><p>测试该方法：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sites,words,data=readfile(<span class="string">'blogdata.txt'</span>)</span><br><span class="line">clust = hcluster(data)</span><br></pre></td></tr></table></figure><p></p><p>为了观察执行的记过，我们编写一个简单的函数，来打印执行结果。</p><h4 id="打印分级聚类结果"><a href="#打印分级聚类结果" class="headerlink" title="打印分级聚类结果"></a>打印分级聚类结果</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printclust</span><span class="params">(clust,labels=None,n=<span class="number">0</span>)</span>:</span><span class="comment"># n代表层级</span></span><br><span class="line">    <span class="comment"># 利用缩进来建立层级布局</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n): print(<span class="string">' '</span>)</span><br><span class="line">    <span class="keyword">if</span> clust.id&lt;<span class="number">0</span>:<span class="comment"># 表示这是一个经过合并得到的群组</span></span><br><span class="line">      print(<span class="string">'-'</span>)</span><br><span class="line">    <span class="keyword">else</span>: <span class="comment">#正数表示这时一个叶节点</span></span><br><span class="line">      <span class="keyword">if</span> labels==<span class="keyword">None</span>:</span><br><span class="line">        print(clust.id)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        print(labels[clust.id])</span><br><span class="line">    <span class="comment"># 现在开始打印左分支和右分支</span></span><br><span class="line">    <span class="keyword">if</span> clust.left!=<span class="keyword">None</span>:</span><br><span class="line">      printclust(clust.left,labels=labels,n=n+<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> clust.right!=<span class="keyword">None</span>:</span><br><span class="line">      printclust(clust.right,labels=labels,n=n+<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>上面的算法使用的是二叉树的先序遍历来打印输出结果。你会发现结果并不容易阅读，起码我觉得层级遍历的输出会更美观一些。<br>有时候你会发现一个博客网站被错误的划分到一个看似不相关的主题，因为相关性的判断用的是单词频度，可能是作者写作用词风格的一种反应，也可能是当天数据的“巧合”。</p><h4 id="绘制树状图"><a href="#绘制树状图" class="headerlink" title="绘制树状图"></a>绘制树状图</h4><p>我们首先要安装绘图库Pillow，可以通过“pip install pillow”来安装，然后到处相关函数<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL imort Image,ImageDraw</span><br></pre></td></tr></table></figure><p></p><p>观察树状图，我们发现其根节点位于左边，孩子节点位于右边，图像的宽度由其这棵树的深度决定。<br>而每个节点占有的垂直距离，为其孩子节点所占有的垂直距离相加得到。<br><img src="/2017/11/12/cluster/tree.PNG" title="树状图"><br>写出计算树深度的函数<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getdepth</span><span class="params">(clust)</span>:</span></span><br><span class="line">  <span class="comment">#一个叶子节点的深度是0</span></span><br><span class="line">  <span class="keyword">if</span> clust.left ==<span class="keyword">None</span> <span class="keyword">and</span> clust.right==<span class="keyword">None</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  <span class="comment"># 父节点的深度为左右子树中距离较大者加上该节点到孩子的距离</span></span><br><span class="line">  <span class="keyword">return</span> max(getdepth(clust.left),getdepth(clust.right))+clust.distance</span><br></pre></td></tr></table></figure><p></p><p>写出计算节点高度（占有的垂直距离）的函数<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getheight</span><span class="params">(clust)</span>:</span></span><br><span class="line">  <span class="comment">#叶子节点高度为1</span></span><br><span class="line">  <span class="keyword">if</span> clust.left ==<span class="keyword">None</span> <span class="keyword">and</span> clust.right==<span class="keyword">None</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span>  getheight(clust.left)+getheight(clust.right)</span><br></pre></td></tr></table></figure><p></p><p>我们要将树状图绘制在宽度为1200的图片上，故需要根据图像的深度按比例进行缩放；图像占用的垂直距离由根节点的垂直距离得到，我们规定叶节点的高度为20像素<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">drawendrogram</span><span class="params">(clust,labels,jpeg=<span class="string">'clusters.jpg'</span>)</span>:</span></span><br><span class="line">  <span class="comment"># 获得垂直距离,决定图像的总高度</span></span><br><span class="line">  h = getheight(clust)*<span class="number">20</span>  <span class="comment"># 乘上叶节点的高度，为总高度</span></span><br><span class="line">  w = <span class="number">1200</span></span><br><span class="line">  depth = getdepth(clust)</span><br><span class="line">  <span class="comment"># 计算缩放因子,因为宽度是固定的，根据树深度按比例进行缩放</span></span><br><span class="line">  scaling = float(w<span class="number">-150</span>)/depth  <span class="comment"># -150是为了左右留边</span></span><br><span class="line">  <span class="comment"># 新建一个白色背景的图片</span></span><br><span class="line">  img = Image.new(<span class="string">'RGB'</span>,(w,h),(<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>))</span><br><span class="line">  draw = ImageDraw.Draw(img)</span><br><span class="line">  <span class="comment"># 绘制根节点的主脉,位于图像的垂直中央，占10个像素的水平宽度</span></span><br><span class="line">  draw.line((<span class="number">0</span>,h/<span class="number">2</span>,<span class="number">1</span>-,h/<span class="number">2</span>),fill=(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">  <span class="comment"># 传给drawnode的10和(h/2),代表除了排除了‘10像素水平宽度的根节点’之外的节点的起始位置</span></span><br><span class="line">  drawnode(draw,clust,<span class="number">10</span>,(h/<span class="number">2</span>),scaling,labels)</span><br><span class="line">  img.save(jpeg,<span class="string">'JPEG'</span>)</span><br></pre></td></tr></table></figure><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">drawnode</span><span class="params">(draw,clust,x,y,scaling,labels)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> clust.id&lt;<span class="number">0</span>:</span><br><span class="line">    h1 = getheight(clust.left)*<span class="number">20</span></span><br><span class="line">    h2 = getheight(clust.right)*<span class="number">20</span></span><br><span class="line">    top = y-(h1+h2)/<span class="number">2</span>  <span class="comment"># 转换为top和bootm容易计算坐标点，不然就要相对于y（中间位置）进行计算</span></span><br><span class="line">    bottom = y+(h1+h2)/<span class="number">2</span></span><br><span class="line">    horizontal_length = clust.distance*scaling</span><br><span class="line">    draw.line((x,top+h1/<span class="number">2</span>,x,bottom-h2/<span class="number">2</span>),fill=(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>))<span class="comment"># 绘制垂直线</span></span><br><span class="line">    draw.line((x,top+h1/<span class="number">2</span>,x+horizontal_length,top+h1/<span class="number">2</span>),fill=(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>))<span class="comment"># 绘制左分支线，线长与相似度有关</span></span><br><span class="line">    draw.line((x,bottom-h2/<span class="number">2</span>,x+horizontal_length,bottom-h2/<span class="number">2</span>),fill=(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>))<span class="comment"># 绘制右分支线，线长与相似度有关</span></span><br><span class="line">    <span class="comment"># 递归的绘制左右节点</span></span><br><span class="line">    drawnode(draw,clust.left,x+horizontal_length,top+h1/<span class="number">2</span>,scaling,labels)</span><br><span class="line">    drawnode(draw,clust.right,x+horizontal_length,bottom-h2/<span class="number">2</span>,scaling,labels)</span><br><span class="line">  <span class="keyword">else</span>:<span class="comment">#如果是叶节点，绘制节点的标签</span></span><br><span class="line">  draw.text((x+<span class="number">5</span>,y<span class="number">-7</span>),labels[clust.id],(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>))</span><br></pre></td></tr></table></figure><p>调用这个函数进行测试：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drawendrogram(clust,sites,jpeg=<span class="string">'blogclust.jpg'</span>)</span><br></pre></td></tr></table></figure><p></p><p>效果图如下：<br><img src="/2017/11/12/cluster/drawtree.PNG" title="树状图"></p><h2 id="对词汇使用分级聚类"><a href="#对词汇使用分级聚类" class="headerlink" title="对词汇使用分级聚类"></a>对词汇使用分级聚类</h2><p>该方法要将数据集的行列互换。这种做法和上一章的推荐系统相同，计算人与人之间像素度，需要物品作为轴，人变成坐标轴上的点；反过来，计算物品相似度，以人作为轴，物品作为坐标轴上的点。<br>前面，我们以词频为轴，博客为坐标轴上的点，计算博客之间的相似度。现在，我们将数据的行列互换，变成以博客为轴，单词为坐标轴上的点，这样就可以计算单词相似度，对单词进行聚类啦！<br>同时对行和列都做聚类往往是很有必要的！ 比如对消费群体进行聚类可以让我们了解这个群体的偏好，对物品进行聚类我们可以向其推荐相似物品或者了解哪些商品可以捆绑销售。在这里，我们对单词进行聚类，会了解那些单词会组合在一起使用。<br>编写行列互换的代码：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rotatematrix</span><span class="params">(data)</span>:</span></span><br><span class="line">  newdata=[]</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data[<span class="number">0</span>])):<span class="comment">#对于每个单词</span></span><br><span class="line">    newrow = [data[j][i] <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data))]</span><br><span class="line">    newdata.append(newrow)</span><br><span class="line">  <span class="keyword">return</span> newdata</span><br></pre></td></tr></table></figure><p></p><p>因为单词的数量比较多，两两比较花费的时间会更久。因为矩阵被转置，现在是对单词进行聚类。</p><p>当聚类的原始群组增加的时候，出现无意义的聚类可能就越多。由于单词的聚类比博客要多，我们会发现博客的聚类比单词的聚类更合理。</p><p>我们仍然能找到一些有意思的聚类，如下图所示：<br><img src="/2017/11/12/cluster/wordcluster.PNG" title="单词聚类"><br>我们会发现在讨论internet相关话题时，会经常用到的一组词汇。另外，我们也会找到一些写作风格的的“模式”，比如经常使用“fact”、“very”、“think”。表明作者的写作风格是偏向于主观的。</p><p>分级聚类的结果为我们返回了一个形象直观的树，但这种方法有两个缺点：</p><ol><li>树形的结果并没有将数据拆分成不同的组，而是形成一种层级的聚合结构</li><li>算法必须计算每两个配对项之间的关系，并且在合并项后，子群组聚合生成的新群组的位置要重新计算，下次聚类还要算每两个配对项之间的关系，导致算法计算量较大</li></ol><h1 id="根据希望拥有的和已经拥有的信息，将人们的意愿进行分组"><a href="#根据希望拥有的和已经拥有的信息，将人们的意愿进行分组" class="headerlink" title="根据希望拥有的和已经拥有的信息，将人们的意愿进行分组"></a>根据希望拥有的和已经拥有的信息，将人们的意愿进行分组</h1><h2 id="获取数据集"><a href="#获取数据集" class="headerlink" title="获取数据集"></a>获取数据集</h2><p>Zebo 这样网站收集了人们已经有的和希望拥有的物品。从广告商角度来看，这些是非常有价值的信息。<br>Zebo 没有提供API直接获取数据，我们不得不下载大量网页，并从中提取每位用户的信息。</p><h3 id="使用beautiful-soup解析网页"><a href="#使用beautiful-soup解析网页" class="headerlink" title="使用beautiful soup解析网页"></a>使用beautiful soup解析网页</h3><p>Beautiful Soup 是一个可以从HTML或XML文件中提取数据的Python库。<br>有了它，你可以使用类似前端的语法通过元素ID、class、tage等属性获取元素，提取元素内容。<br>我们可以使用“pip install beautifulsoup4”来安装，或者从pypi.python.org来检索安装。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line">c=urllib.request.urlopen(<span class="string">'http://kiwitobes.com'</span>)</span><br><span class="line">soup = BeautifulSoup(c.read()) <span class="comment"># 用页面内容初始化一个beautifulsoup对象</span></span><br><span class="line"><span class="comment">#soup将标签类型作为参数，返回一个属于该标签类型的对象列表</span></span><br><span class="line">body = soup(<span class="string">'body'</span>)<span class="comment"># 或者tag为body的元素</span></span><br></pre></td></tr></table></figure><p></p><h3 id="开始解释zebo网站的数据"><a href="#开始解释zebo网站的数据" class="headerlink" title="开始解释zebo网站的数据"></a>开始解释zebo网站的数据</h3><p>已知带有CSS类名为“bgverdanasmall”为物品列表。我们将搜索50个页面，将每个用户希望拥有的物品列表进行统计。由于物品的文字是可以随意输入的，我们需要一些清理工作，包括去掉‘a’和‘the’之类的词，去掉标点符号，去掉左右两边的空格，以及把字母转换成小写。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line">chare = re.compile(<span class="string">r'[!-\.&amp;]'</span>)<span class="comment"># 匹配点号、叹号、'-'和与(&amp;)号,这些奇怪符合直接去掉</span></span><br><span class="line">itemowners = &#123;&#125;</span><br><span class="line"><span class="comment"># 要去掉的单词</span></span><br><span class="line">dropwords = &#123;<span class="string">'a'</span>,<span class="string">'new'</span>,<span class="string">'some'</span>,<span class="string">'more'</span>,<span class="string">'my'</span>,<span class="string">'own'</span>,<span class="string">'the'</span>,<span class="string">'many'</span>,<span class="string">'other'</span>,<span class="string">'another'</span>&#125;</span><br><span class="line">currentuser=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>:<span class="number">51</span>):</span><br><span class="line">c=urllib.request.urlopen(<span class="string">'http://member.zebo.com/Main?event_key=USERSEARCH&amp;wiowiw=wiw&amp;keyword=car&amp;page=%d'</span> % (i)</span><br><span class="line">soup = BeautifulSoup(c.read())</span><br><span class="line"><span class="keyword">for</span> td <span class="keyword">in</span> soup(<span class="string">'td'</span>)</span><br><span class="line">    <span class="keyword">if</span> (<span class="string">'class'</span> <span class="keyword">in</span> dict(td.attrs)) <span class="keyword">and</span> (td[<span class="string">'class'</span>]==<span class="string">'bgverdanasmall'</span>):</span><br><span class="line">      items=[re.sub(chare,<span class="string">''</span>a.contents[<span class="number">0</span>].lower().strip()) <span class="keyword">for</span> a <span class="keyword">in</span> td(<span class="string">'a'</span>)]</span><br><span class="line">      <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        txt = <span class="string">' '</span>.join([t <span class="keyword">for</span> t <span class="keyword">in</span> item.split(<span class="string">' '</span>) <span class="keyword">if</span> t <span class="keyword">not</span> <span class="keyword">in</span> dropwords])</span><br><span class="line">        <span class="keyword">if</span> len(txt)&lt;<span class="number">2</span>:<span class="comment"># 去掉字母数过少的</span></span><br><span class="line">          <span class="keyword">continue</span></span><br><span class="line">        itemowner.setdefault(txt,&#123;&#125;)</span><br><span class="line">        itemowner[txt][currentuser]=<span class="number">1</span><span class="comment">#一个用户想拥有的全部物品,1表示想要</span></span><br><span class="line">      currentuser+=<span class="number">1</span>                 <span class="comment">#统计下一个用户期望拥有的全部物品</span></span><br></pre></td></tr></table></figure><p></p><p>我们得到了一组用户想要的物品列表之后，我们将其进行格式转换，保存到zebo.txt文件中。这里，我们剔除掉那些很少有人想要的东西，目的是去掉一些比较生疏的物品，我们规定一个物品至少要有10个人‘想要’这件物品才会被记录，少于10个人‘想要’的物品不会被记录。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">out = open(<span class="string">'zebo.txt'</span>,<span class="string">'w'</span>)</span><br><span class="line">out.write(<span class="string">'Item'</span>)</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> range(<span class="number">0</span>,currentuser)</span><br><span class="line">  out.write(<span class="string">'\tU%d'</span> % user)</span><br><span class="line">out.write(<span class="string">'\n'</span>)</span><br><span class="line"><span class="keyword">for</span> item,owners <span class="keyword">in</span> itemowners.items():</span><br><span class="line">  <span class="keyword">if</span> len(owners)&gt;<span class="number">10</span>:</span><br><span class="line">    out.write(item)</span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> range(<span class="number">0</span>,currentuser):</span><br><span class="line">      <span class="keyword">if</span> user <span class="keyword">in</span> owners:</span><br><span class="line">        out.write(<span class="string">'\t1'</span>)  <span class="comment"># 想要标记为1</span></span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">        cout.write(<span class="string">'\t0'</span>) <span class="comment"># 不想要标记为0</span></span><br><span class="line">    cout.write(<span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure><p></p><p>最终的结果格式如下：<br>数据集中的‘想要’标记为1，‘不想要’标记为0</p><table><thead><tr><th>物品名</th><th>人物1</th><th>人物2</th><th>…</th><th>人物n</th></tr></thead><tbody><tr><td>物品1</td><td>想要</td><td>不想</td><td>…</td><td>想要</td></tr><tr><td>物品2</td><td>想要</td><td>想要</td><td>…</td><td>不想要</td></tr><tr><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td></tr></tbody></table><h2 id="使用K-均值聚类"><a href="#使用K-均值聚类" class="headerlink" title="使用K-均值聚类"></a>使用K-均值聚类</h2><h3 id="K-均值聚类的概念"><a href="#K-均值聚类的概念" class="headerlink" title="K-均值聚类的概念"></a>K-均值聚类的概念</h3><p>K-均值聚类与分级聚类不同，它需要预先指定希望生成的聚类数量，算法会自动确定每个聚类集合中元素的个数。</p><p>K-均值聚类的算法流程如下：</p><ol><li>随机生成k个中心位置（代表聚类中心的点）</li><li>将各个数据项分配给‘k个中心位置’中距离该数据项最近的那个中心位置，形成k个群组</li><li>重新计算k各中心位置的坐标，改为该群组中所有数据项的中心位置，这时由于k个中心位置的坐标被重新计算了，故各个中心位置与数据项距离也会发生变化，这会引起数据项的重新分配</li><li>重复执行1、2、3，直到分配过程不再变化为止</li></ol><p>举例：<br><img src="/2017/11/12/cluster/kmeans.PNG" title="K-均值聚类"></p><ol><li>左上角第一幅图，两个中心点（两个黑圈圈）是随机选择的</li><li>第二幅图，将各个数据项分给两个中心点，其中A和B被分给了上方的中心点，C、D、E被分给了下方的中心点</li><li>第三幅图，重新计算两个中心点的位置，修正为其包含数据项的中心位置</li><li>第四幅图，由于中心点位置的变化，使得C聚类上方的中心点比下方的中心点要近，故将C重新分配给上方的中心点</li><li>重新计算两个中心点的位置，此时数据项的分配已经不再发生变化，程序结束</li></ol><p>下面我们来书写K-均值聚类的代码：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kcluster</span><span class="params">(rows,distance=pearson,k=<span class="number">4</span>)</span>:</span></span><br><span class="line">  <span class="comment"># 确定中心点再每个维度上的范围</span></span><br><span class="line">  ranges=[ (min([row[i] <span class="keyword">for</span> row <span class="keyword">in</span> rows),max([row[i] <span class="keyword">for</span> row <span class="keyword">in</span> rows)) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(rows[<span class="number">0</span>]))]</span><br><span class="line">  <span class="comment"># 随机创建k个中心点</span></span><br><span class="line">  clusters=[ [random.random()*(ranges[i][<span class="number">1</span>]-ranges[i][<span class="number">0</span>])+ranges[i][<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> range( len(rows[<span class="number">0</span>]) )] <span class="keyword">for</span> j <span class="keyword">in</span> range(k)]</span><br><span class="line">  <span class="comment"># 上述的表达式中ranges[i][1]代表第i个维度上的最大值，ranges[i][0]代表第i个维度上的最小值</span></span><br><span class="line">  <span class="comment"># (ranges[i][1]-ranges[i][0])为第i个维度上的最大值减去最小值，就是第i个维度的数据可变范围，</span></span><br><span class="line">  <span class="comment"># 数据可变范围乘上0到1之间的随机数后，加上最小值，即为最大值和最小值中间的随机数</span></span><br><span class="line">  lastmatches = <span class="keyword">None</span></span><br><span class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    print(<span class="string">'Iteration %d'</span> % t)</span><br><span class="line">    bestmatches=[[] <span class="keyword">for</span> i <span class="keyword">in</span> range(k)]<span class="comment"># 创建k个列表</span></span><br><span class="line">    <span class="comment"># 将各个数据项分给中心点，从k个中心点中选择该数据项最近的那个</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(rows)):</span><br><span class="line">      row = rows[j]</span><br><span class="line">      best_k = <span class="number">0</span>       <span class="comment"># 保存最佳匹配的中心点的索引 </span></span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        d = distance(clusters[i],row) <span class="comment"># clusters含有k个中心点的列表</span></span><br><span class="line">        <span class="keyword">if</span> d&lt;distance(clusters[best_k],row):</span><br><span class="line">          best_k=i</span><br><span class="line">      bestmatches[best_k].append(j)  <span class="comment"># 将数据线分给最佳匹配的中心点</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> bestmatches == lastmatches: <span class="comment">#如果数据线的分配不再发生变化，表示聚类结束</span></span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">    lastmatches = bestmatches</span><br><span class="line">    <span class="comment"># 把k个中心点，移动到其内部所有成员的平均位置处</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">      avgs=[<span class="number">0.0</span>]*len(rows[<span class="number">0</span>])</span><br><span class="line">      <span class="keyword">if</span> len(bestmatches[i])&gt;<span class="number">0</span>: <span class="comment"># 如果第i个聚类中存在数据项</span></span><br><span class="line">        <span class="keyword">for</span> rowid <span class="keyword">in</span> bestmatches[i]:</span><br><span class="line">          <span class="keyword">for</span> m <span class="keyword">in</span> range(len(rows[rowid]))<span class="comment"># 取出其列数</span></span><br><span class="line">            avgs[m]+=rows[rowid][m]  <span class="comment"># 将所有数据行的列数据加到一起</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(avgs)):</span><br><span class="line">          avgs[j]/=len(bestmatches[i])</span><br><span class="line">        clusters[i]=avgs</span><br><span class="line">  <span class="keyword">return</span> bestmatches</span><br></pre></td></tr></table></figure><p></p><p>上述代码在每个变量的值域范围内随机构造了一组聚类。在每一迭代的时候，算法会将每一个数据项分给某个中心点，然后将中心点的数据更新为其含有的所有数据项的相应数据的平均值。当数据项的分配不再发生变化时，迭代过程就结束了，算法返回k个序列，每个序列代表一个聚类。</p><p>由于函数选用随机的中心点，因为每次运行中心点的初始位置不同，所以每次返回的聚类结果几乎总是不同的。</p><h3 id="对数据集使用K-均值聚类"><a href="#对数据集使用K-均值聚类" class="headerlink" title="对数据集使用K-均值聚类"></a>对数据集使用K-均值聚类</h3><h4 id="定义相似度计算公式-1"><a href="#定义相似度计算公式-1" class="headerlink" title="定义相似度计算公式"></a>定义相似度计算公式</h4><p>皮尔逊相关度很适合于博客数据集，该数据集中所包含的单词的实际统计值。而此时，数据集只有1或者0两种取值，分别代表用户“想要”和“不想要”。<br>如果有两个人，他们想要的物品的重叠程度越打，表明他俩就越相似。<br>此时我们可以使用<strong>Tanimoto系数</strong>来度量，它代表的是两个集合的交集（两个集合中都出现的项）与并集（出现在任一集合中的项）的比率。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tanimoto</span><span class="params">(v1,v2)</span>:</span></span><br><span class="line">  c1,c2,share=<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(v1)):</span><br><span class="line">    <span class="keyword">if</span> v1[i]!=<span class="number">0</span>: c1+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> v2[i]!=<span class="number">0</span>: c2+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> v1[i]!=<span class="number">0</span> <span class="keyword">and</span> v2[i]!=<span class="number">0</span>: share+=<span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> <span class="number">1.0</span>-(float(share)/(c1+c2-share))</span><br></pre></td></tr></table></figure><p></p><p>上述函数返回一个介于0.0到1.0之间的值，其中1.0表示没有交集，0.0表示他们想要的完全重叠。</p><h4 id="测试K-均值聚类"><a href="#测试K-均值聚类" class="headerlink" title="测试K-均值聚类"></a>测试K-均值聚类</h4><h4 id="测试之前的博客数据集"><a href="#测试之前的博客数据集" class="headerlink" title="测试之前的博客数据集"></a>测试之前的博客数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">clust = kcluster(data,k=<span class="number">10</span>)<span class="comment"># 这里使用的是pearson作为距离度量</span></span><br><span class="line">[blogname[r] <span class="keyword">for</span> r <span class="keyword">in</span> kclust[<span class="number">0</span>]]</span><br><span class="line"><span class="comment"># 返回的是['The Viral Garden','Copyblogger','Creating Passionate Users','Oilman']...</span></span><br></pre></td></tr></table></figure><h4 id="使用分级聚类测试zebo的数据集"><a href="#使用分级聚类测试zebo的数据集" class="headerlink" title="使用分级聚类测试zebo的数据集"></a>使用分级聚类测试zebo的数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wants,people,data = readfile(<span class="string">'zebo.txt'</span>)</span><br><span class="line">clust=hcluster(data,distance=tanimoto)</span><br><span class="line">drawdendrogram(clust,wants)<span class="comment"># 绘制树状图</span></span><br></pre></td></tr></table></figure><p>根据返回的树状图可以发现，希望得到Xbox、PlayStation Portable和Play Station 3都属于同一类。我们也可以发现一些明显的群组，比如：有的人雄心勃勃（船、飞机、岛屿），有些人热衷于心灵的慰藉（朋友、爱情、幸福）。还会发现一些有趣的现象：希望有钱（money）只想要一栋房子；而希望拥有许多钱（lots of money）倾向于得到好房子（nice house）。<br>我们可以将数据的行列转置，把用户进行聚类，依据是他们想要拥有的物品，即以物品为坐标轴，人为坐标轴上的点。这和之前的物品聚类，依据人为坐标轴，物品为坐标轴上的点的做法类似。调查某个聚类群组中用户的统计信息我们也许会得到一些有趣的结论。<br><img src="/2017/11/12/cluster/zebo.PNG" title="Zebo数据分级聚类效果"></p><h4 id="以二维的方式展现聚类结果的类间差异"><a href="#以二维的方式展现聚类结果的类间差异" class="headerlink" title="以二维的方式展现聚类结果的类间差异"></a>以二维的方式展现聚类结果的类间差异</h4><p>前面我们学会了”树状图“的图形化方法，这里再介绍一种将采集的数据集映射到二维图像、用数据项之间的距离代表数据项之间的相似程度的方法，叫做<strong>多维缩放</strong>。<br>第一步：为了做到这一点，我们首先要计算数据项两两之间的目标距离，可以采用皮尔逊相关度、欧几里得距离等等度量公式。<br>这里举个例子：</p><table><thead><tr><th></th><th>A</th><th>B</th><th>C</th><th>D</th></tr></thead><tbody><tr><td>A</td><td>0.0</td><td>0.2</td><td>0.8</td><td>0.7</td></tr><tr><td>B</td><td>0.2</td><td>0.0</td><td>0.9</td><td>0.8</td></tr><tr><td>C</td><td>0.8</td><td>0.9</td><td>0.0</td><td>0.1</td></tr><tr><td>D</td><td>0.7</td><td>0.8</td><td>0.1</td><td>0.0</td></tr></tbody></table><p>从表中我们可以看到，主对角线上的数据都是0.0，自己跟自己的距离当然是0啦！ 还可以看到数据关于主对角线对称，这也是当然的，A到B的距离与B到A的距离是一回事。<br>第二步：我们将所有的数据项随机放置在二维图上，如下图所示：</p><img src="/2017/11/12/cluster/init.PNG" title="投影到二维坐标上的各数据项的初始位置"><p>对于这幅二维坐标图来说，各个数据项之间的距离是根据欧几里得距离来度量的。欧几里得距离的计算方式为两个数据项各个坐标轴上的数据相减，然后平方，即差的平方和。</p><p>第三步：计算一下各项之间的在二维坐标上的距离，如下图所示：<br><img src="/2017/11/12/cluster/calcute.PNG" title="各数据项再二维图上距离"></p><p>第四步：对每两两数据项，将他们的相似度与它们在图上的距离做比较，得到一个误差值。我们会根据误差的情况，来改变数据项的位置。我们一次调整一个节点，改变这个节点的位置来调节这个节点与其他节点的距离。<br>举个例子：<br><img src="/2017/11/12/cluster/move.PNG" title="点A的受力情况"><br>图上A与B之间的距离为0.5，而相似度表中为0.2，故需要将A的坐标移动，让其更接近B。图上A与C的距离为0.4，相似度表中A与C的相似度为0.8，故需要将A推离C。<br>每一个节点的移动，都是所有的其他节点施加在该节点上推或拉的综合效应。节点每移动一次，其图像中的距离与表中的相似度距离的差距会更小一些。这个过程会不断的重复多次，直到无法再通过移动节点来减少总体误差为止。</p><p>我们来写函数实现上述过程：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scaledown</span><span class="params">(data,distance=pearson,rate=<span class="number">0.01</span>)</span>:</span><span class="comment">#rate表示每次移动的步长</span></span><br><span class="line">  n=len(data)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># The real distances between every pair of items </span></span><br><span class="line">  <span class="comment"># 计算两两之间的相似度</span></span><br><span class="line">  realdist=[[distance(data[i],data[j]) <span class="keyword">for</span> j <span class="keyword">in</span> range(n)] </span><br><span class="line">             <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,n)]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Randomly initialize the starting points of the locations in 2D</span></span><br><span class="line">  <span class="comment"># 在二维图上随机初始化各个数据项的位置</span></span><br><span class="line">  loc=[[random.random(),random.random()] <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line">  fakedist=[[<span class="number">0.0</span> <span class="keyword">for</span> j <span class="keyword">in</span> range(n)] <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line">  </span><br><span class="line">  lasterror=<span class="keyword">None</span></span><br><span class="line">  <span class="keyword">for</span> m <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># Find projected distances</span></span><br><span class="line">    <span class="comment"># 计算二维图像中的距离</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">      <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        fakedist[i][j]=sqrt(sum([pow(loc[i][x]-loc[j][x],<span class="number">2</span>) </span><br><span class="line">                                 <span class="keyword">for</span> x <span class="keyword">in</span> range(len(loc[i]))]))</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 记录每个节点，x轴和y轴的移动量</span></span><br><span class="line">    grad=[[<span class="number">0.0</span>,<span class="number">0.0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line">    </span><br><span class="line">    totalerror=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(n):<span class="comment"># 对于每个节点，要参照所有其他节点进行移动</span></span><br><span class="line">      <span class="keyword">for</span> j <span class="keyword">in</span> range(n):<span class="comment"># 参照其他节点</span></span><br><span class="line">        <span class="keyword">if</span> j==k: <span class="keyword">continue</span>  <span class="comment"># 排除自己</span></span><br><span class="line">        <span class="comment"># 目标是使得图上的距离就是相似度的距离</span></span><br><span class="line">        <span class="comment"># 误差用图像距离减去相似度的距离表示，除上相似度距离，就是得到了差值的百分比</span></span><br><span class="line">        errorterm=(fakedist[j][k]-realdist[j][k])/realdist[j][k]</span><br><span class="line">        <span class="comment"># 一个节点，要参照所有其他节点，来移动坐标</span></span><br><span class="line">        <span class="comment"># 要调整的节点，相对于其他所有点中的一个点来说，是x轴和y轴同比例的调整，会引起两点之间的连线（距离）拉长或缩短</span></span><br><span class="line">        <span class="comment">#(loc[k][0]-loc[j][0])/fakedist[j][k]，表示j坐标点和k坐标点在x轴上的误差，除上两点之间的距离，得到的百分比</span></span><br><span class="line">        <span class="comment">#(loc[k][1]-loc[j][1])/fakedist[j][k]，表示j坐标点和k坐标点在y轴上的误差，除上两点之间的距离，得到的百分比</span></span><br><span class="line">        grad[k][<span class="number">0</span>]+=((loc[k][<span class="number">0</span>]-loc[j][<span class="number">0</span>])/fakedist[j][k])*errorterm</span><br><span class="line">        grad[k][<span class="number">1</span>]+=((loc[k][<span class="number">1</span>]-loc[j][<span class="number">1</span>])/fakedist[j][k])*errorterm</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 记录总的误差值</span></span><br><span class="line">        totalerror+=abs(errorterm)</span><br><span class="line">    <span class="keyword">print</span> totalerror</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果这次的误差比上次的误差还要大，程序结束</span></span><br><span class="line">    <span class="keyword">if</span> lasterror <span class="keyword">and</span> lasterror&lt;totalerror: <span class="keyword">break</span></span><br><span class="line">    lasterror=totalerror</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 移动每一个节点到目标位置上</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(n):</span><br><span class="line">      loc[k][<span class="number">0</span>]-=rate*grad[k][<span class="number">0</span>] </span><br><span class="line">      loc[k][<span class="number">1</span>]-=rate*grad[k][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> loc</span><br></pre></td></tr></table></figure><p></p><p>我们来生成一幅图像来观察<strong>多维缩放</strong>的执行结果。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw2d</span><span class="params">(data,labels,jpeg=<span class="string">'mds2d.jpg'</span>)</span>:</span></span><br><span class="line">  img=Image.new(<span class="string">'RGB'</span>,(<span class="number">2000</span>,<span class="number">2000</span>),(<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>))</span><br><span class="line">  draw=ImageDraw.Draw(img)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data)):</span><br><span class="line">    x=(data[i][<span class="number">0</span>]+<span class="number">0.5</span>)*<span class="number">1000</span></span><br><span class="line">    y=(data[i][<span class="number">1</span>]+<span class="number">0.5</span>)*<span class="number">1000</span></span><br><span class="line">    draw.text((x,y),labels[i],(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">  img.save(jpeg,<span class="string">'JPEG'</span>)</span><br></pre></td></tr></table></figure><p></p><p>我们可以传入博客的相似度数据来建立博客数据集的<strong>多维缩放</strong>的效果，我们会发现正如二维图中表示的那样，相似的博客会距离很近，而不相似的博客之间距离很远。<br>多维缩放将数据集以一种易于解释的方式形象化的绘制在了空间坐标系中，缩放过程中信息可能会丢失，但坐标化的数据集却有利于我们观察聚类的结果。</p><h1 id="课后练习提示"><a href="#课后练习提示" class="headerlink" title="课后练习提示"></a>课后练习提示</h1><ol><li>针对del.icio.us数据集，分别运行分级聚类和K-均值聚类<br>略，本章已经在博客的单词词频数据集中运行过分级和K-均值聚类</li><li>修改博客数据处理部分的代码，改成将一个博客下的所有文章进行聚类 来自同一个博客的不同文章能否聚类在一起？相同日期的文章聚类效果又如何？<br>略，修改数据预处理的脚本即可完成</li><li>尝试使用<strong>毕达哥拉斯</strong>对博客进行聚类 这样会对结果产生怎样影响呢？<br><strong>毕达哥拉斯</strong>距离为一个点到原点的欧几里得距离。</li><li>找出曼哈顿距离的定义<img src="/2017/11/12/cluster/manha.jpg" title="曼哈顿距离"> 曼哈顿距离是欧几里得距离的近似，它的出现是因为早期计算机算浮点数很慢，而算整数比较快，曼哈顿距离就是将坐标画成棋盘格，两点之间的距离，为一个点到另一个点穿过的横线和竖线的总和。<br>上图中绿色为欧几里得距离，紫色为曼哈顿距离。</li><li>修改K-均值聚类函数 在返回聚类结果的同时，一并返回所有数据项到他们的聚类中心的距离总和。<br>略</li><li>完成第5题后，尝试选择不同的k值来运行K-均值聚类，看看距离总和如何随着聚类数的增加而改变？什么时候聚类数的增加对距离总和结果的影响变得越来越轻微？<br>想象一个极端情况，k值就是数据项的个数，此时每个数据项各成一个聚类群组，数据项到其中心聚类点的距离为0。再想一个情况，数据集其实是一个数据集的复制，那么聚类中心到各个数据项的距离也是一直为0的。</li><li>本章的<strong>多维缩放</strong>技术，是在二维图形上进行的，请将其修改为一维或者三维（立体）<br>只要将构造随机点的地方修改为一维或三维，并在计算一个节点与其他所有节点的时候，改为统计一维或三维的误差。最后修正各个节点位置的时候，代码也要修改成一维或三维的。</li></ol></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>如果您觉得读完本文有收获，不妨小额赞助我一下，让我有动力继续写出高质量的教程！</div><button id="rewardButton" disable="enable"><span>打赏</span></button><div id="QR" style="display:block"><div id="wechat" style="display:inline-block"><img id="wechat_qr" src="/images/smacker.jpg" alt="倔强的土豆 微信支付"><p>微信支付</p></div></div></div></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2017/11/05/Marking-Recommendations/" rel="next" title="提供推荐"><i class="fa fa-chevron-left"></i> 提供推荐</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2017/12/02/SearchAndRanking/" rel="prev" title="搜索与排名">搜索与排名 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div><div class="comments" id="comments"><div id="gitment-container"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div id="sidebar-dimmer"></div><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">倔强的土豆</p><p class="site-description motion-element" itemprop="description">分享机器学习、深度学习的点滴</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">8</span> <span class="site-state-item-name">日志</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/laiqun" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:laiqun@msn.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#聚类的概念"><span class="nav-number">1.</span> <span class="nav-text">聚类的概念</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#对不同用户的博客进行分组"><span class="nav-number">2.</span> <span class="nav-text">对不同用户的博客进行分组</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#为每篇博客建立“单词频度表”（也叫单词向量表）"><span class="nav-number">2.1.</span> <span class="nav-text">为每篇博客建立“单词频度表”（也叫单词向量表）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#下载和处理博客订阅源"><span class="nav-number">2.1.1.</span> <span class="nav-text">下载和处理博客订阅源</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#对博客使用分级聚类"><span class="nav-number">2.2.</span> <span class="nav-text">对博客使用分级聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#分级聚类的概念"><span class="nav-number">2.2.1.</span> <span class="nav-text">分级聚类的概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对博客数据即应用分级聚类"><span class="nav-number">2.2.2.</span> <span class="nav-text">对博客数据即应用分级聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#处理数据文件"><span class="nav-number">2.2.2.1.</span> <span class="nav-text">处理数据文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#定义相似度计算公式"><span class="nav-number">2.2.2.2.</span> <span class="nav-text">定义相似度计算公式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#构建层级树"><span class="nav-number">2.2.2.3.</span> <span class="nav-text">构建层级树</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#打印分级聚类结果"><span class="nav-number">2.2.2.4.</span> <span class="nav-text">打印分级聚类结果</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#绘制树状图"><span class="nav-number">2.2.2.5.</span> <span class="nav-text">绘制树状图</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#对词汇使用分级聚类"><span class="nav-number">2.3.</span> <span class="nav-text">对词汇使用分级聚类</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#根据希望拥有的和已经拥有的信息，将人们的意愿进行分组"><span class="nav-number">3.</span> <span class="nav-text">根据希望拥有的和已经拥有的信息，将人们的意愿进行分组</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#获取数据集"><span class="nav-number">3.1.</span> <span class="nav-text">获取数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#使用beautiful-soup解析网页"><span class="nav-number">3.1.1.</span> <span class="nav-text">使用beautiful soup解析网页</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#开始解释zebo网站的数据"><span class="nav-number">3.1.2.</span> <span class="nav-text">开始解释zebo网站的数据</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用K-均值聚类"><span class="nav-number">3.2.</span> <span class="nav-text">使用K-均值聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#K-均值聚类的概念"><span class="nav-number">3.2.1.</span> <span class="nav-text">K-均值聚类的概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对数据集使用K-均值聚类"><span class="nav-number">3.2.2.</span> <span class="nav-text">对数据集使用K-均值聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#定义相似度计算公式-1"><span class="nav-number">3.2.2.1.</span> <span class="nav-text">定义相似度计算公式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#测试K-均值聚类"><span class="nav-number">3.2.2.2.</span> <span class="nav-text">测试K-均值聚类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#测试之前的博客数据集"><span class="nav-number">3.2.2.3.</span> <span class="nav-text">测试之前的博客数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用分级聚类测试zebo的数据集"><span class="nav-number">3.2.2.4.</span> <span class="nav-text">使用分级聚类测试zebo的数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#以二维的方式展现聚类结果的类间差异"><span class="nav-number">3.2.2.5.</span> <span class="nav-text">以二维的方式展现聚类结果的类间差异</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#课后练习提示"><span class="nav-number">4.</span> <span class="nav-text">课后练习提示</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">倔强的土豆</span></div><div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div><span class="post-meta-divider">|</span><div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.3</div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script><link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css"><script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script><script type="text/javascript">function renderGitment(){var a=new Gitmint({id:window.location.pathname,owner:"laiqun",repo:"laiqun.github.io",lang:navigator.language||navigator.systemLanguage||navigator.userLanguage,oauth:{client_secret:"55aaeb736714431ea52109dd66461b1644ca6177",client_id:"c90dfa80285ea91b9120"}});a.render("gitment-container")}renderGitment()</script></body></html><!-- rebuild by neat -->