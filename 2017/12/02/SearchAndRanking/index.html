<!-- build time:Sun Feb 25 2018 20:37:17 GMT+0800 (CST) --><!DOCTYPE html><html class="theme-next mist" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3"><link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222"><meta name="keywords" content="Hexo, NexT"><meta name="description" content="搜索引擎的定义与组成在大量的文档中搜索一系列单词，根据文档与这些单词的相关程度，对搜索结果进行排名。本章中我们将将学到如何搜索引起的数据处理与数据检索的方法：数据处理：抓取网页（crawl）、建立索引（index）数据检索：从多种角度度量不同文档与搜索关键词的相关程度，根据相关程度对文档进行排序。数据处理过程目的是获取"><meta property="og:type" content="article"><meta property="og:title" content="搜索与排名"><meta property="og:url" content="laiqun.github.io/2017/12/02/SearchAndRanking/index.html"><meta property="og:site_name" content="广阔天地，大有作为"><meta property="og:description" content="搜索引擎的定义与组成在大量的文档中搜索一系列单词，根据文档与这些单词的相关程度，对搜索结果进行排名。本章中我们将将学到如何搜索引起的数据处理与数据检索的方法：数据处理：抓取网页（crawl）、建立索引（index）数据检索：从多种角度度量不同文档与搜索关键词的相关程度，根据相关程度对文档进行排序。数据处理过程目的是获取数据，并将数据存储到数据库以供检索时使用。数据检索过程是用户输入关键词，得到搜索"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="/2017/12/02/SearchAndRanking/ER.png"><meta property="og:image" content="/2017/12/02/SearchAndRanking/ER2.png"><meta property="og:image" content="/2017/12/02/SearchAndRanking/ER_LINK.png"><meta property="og:image" content="/2017/12/02/SearchAndRanking/sch.PNG"><meta property="og:image" content="/2017/12/02/SearchAndRanking/sch2.png"><meta property="og:image" content="/2017/12/02/SearchAndRanking/multiword.jpg"><meta property="og:image" content="/2017/12/02/SearchAndRanking/zuni.png"><meta property="og:image" content="/2017/12/02/SearchAndRanking/links.png"><meta property="og:image" content="/2017/12/02/SearchAndRanking/clicknetwork.jpg"><meta property="og:image" content="/2017/12/02/SearchAndRanking/network.jpg"><meta property="og:image" content="/2017/12/02/SearchAndRanking/tanh.jpg"><meta property="og:image" content="/2017/12/02/SearchAndRanking/linecompute.png"><meta property="og:image" content="/2017/12/02/SearchAndRanking/allone.jpg"><meta property="og:updated_time" content="2018-01-07T08:20:26.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="搜索与排名"><meta name="twitter:description" content="搜索引擎的定义与组成在大量的文档中搜索一系列单词，根据文档与这些单词的相关程度，对搜索结果进行排名。本章中我们将将学到如何搜索引起的数据处理与数据检索的方法：数据处理：抓取网页（crawl）、建立索引（index）数据检索：从多种角度度量不同文档与搜索关键词的相关程度，根据相关程度对文档进行排序。数据处理过程目的是获取数据，并将数据存储到数据库以供检索时使用。数据检索过程是用户输入关键词，得到搜索"><meta name="twitter:image" content="/2017/12/02/SearchAndRanking/ER.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",version:"5.1.3",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!0},fancybox:!0,tabs:!0,motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="laiqun.github.io/2017/12/02/SearchAndRanking/"><title>搜索与排名 | 广阔天地，大有作为</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">广阔天地，大有作为</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">你看到我的筋斗云了嘛？</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="laiqun.github.io/2017/12/02/SearchAndRanking/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="倔强的土豆"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="广阔天地，大有作为"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">搜索与排名</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-02T16:27:03+08:00">2017-12-02 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/12/02/SearchAndRanking/#comments" itemprop="discussionUrl"><span class="post-comments-count gitment-comments-count" data-xid="/2017/12/02/SearchAndRanking/" itemprop="commentsCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="搜索引擎的定义与组成"><a href="#搜索引擎的定义与组成" class="headerlink" title="搜索引擎的定义与组成"></a>搜索引擎的定义与组成</h1><p>在大量的文档中<strong>搜索</strong>一系列单词，根据文档与这些单词的相关程度，对搜索结果进行排名。<br>本章中我们将将学到如何搜索引起的数据处理与数据检索的方法：<br>数据处理：<strong>抓取网页</strong>（crawl）、<strong>建立索引</strong>（index）<br>数据检索：从多种角度<strong>度量</strong>不同文档与搜索关键词的<strong>相关程度</strong>，根据相关程度对文档进行排序。<br>数据处理过程目的是获取数据，并将数据存储到数据库以供检索时使用。<br>数据检索过程是用户输入关键词，得到搜索结果的过程，我们要优化搜索结果的排序，确保用户想用的文档排在首位。</p><h1 id="建立搜索引擎的步骤"><a href="#建立搜索引擎的步骤" class="headerlink" title="建立搜索引擎的步骤"></a>建立搜索引擎的步骤</h1><h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><p>第一步： 我们首先要有数据来供搜索引擎来检索才行，即我们要有一种收集文档的方法，也叫做网页抓取。我们先从抓取一组网页开始，然后根据这些网页内的链接逐渐抓取这些链接中的页面。<br>第二步： 为收集到的文档建立索引，因为我们要根据输入的关键字来检索文档，故这里我们将文档拆成一个个单词，创建一个单词与文档的表格。</p><table><thead><tr><th>单词</th><th>文档名</th></tr></thead><tbody><tr><td>单词1</td><td>文档1</td></tr><tr><td>单词1</td><td>文档2</td></tr><tr><td>单词2</td><td>文档2</td></tr></tbody></table><p>以上数据的含义为单词1在文档1和文档2中出现过。单词2在文档2中出现过。<br>但这里我们会发现好像漏掉了一些信息，单词的位置是对搜索结果进行排序的重要依据，比如有两篇文档，一篇文档在标题的时候就涵盖了我们要搜索的关键字，而另一篇只在末尾提到了我们要搜索的关键字，这时候，标题中包含关键字的文档往往就是我们要找的，故<strong>单词在文档中的位置</strong>是对搜索结果进行排序的一个重要指标。</p><p>我们改进一下表格的结构，添加上单词在文档中的出现的位置。值得注意的是，一个单词可能在一篇文档中被提到多次。</p><table><thead><tr><th>单词</th><th>文档名</th><th>文档位置</th></tr></thead><tbody><tr><td>单词1</td><td>文档1</td><td>第1次出现的位置</td></tr><tr><td>单词1</td><td>文档1</td><td>第2次出现的位置</td></tr><tr><td>单词1</td><td>文档1</td><td>第n次出现的位置</td></tr><tr><td>单词1</td><td>文档2</td><td>第1次出现的位置</td></tr><tr><td>单词1</td><td>文档2</td><td>第n次出现的位置</td></tr><tr><td>单词2</td><td>文档2</td><td>第1次出现的位置</td></tr></tbody></table><p>根据数据库的表格的设计原则，当一个表中的数据出现大量重复的时候，这时候往往需要拆分表格。</p><p>我们将上述表格修改为三个表格，单词表、文档名表和‘单词所处文档名与在文档中的位置描述表’。</p><img src="/2017/12/02/SearchAndRanking/ER.png" title="ER图"><p><em>上图为ER图，方形表示实体，圆圈表示属性，一个实体有多个属性。棱形表示联系，连线上的数量表示两个实体之间的关系。</em></p><p>图中有两对关系。</p><ol><li>关系1： 单词表与‘单词所处文档名与在文档中的位置描述表’两个表之间的关系是1：N，因为一个单词可以在一篇文档的多个位置出现。有了这两个表格，这样单词就不会重复书写多次啦。</li><li>关系2： 文档列表与‘单词所处文档名与在文档中的位置描述表’两个表之间的关系为1：N，因为一个文档名可能在‘单词所处文档名与在文档中的位置描述表’中被引用很多次。‘单词所处文档名与在文档中的位置描述表’中引用了文档名，如果不拆分，那么同一个文档名会被写多次。如果将文档名单独划分成一个表，引用一个文档的话只要使用文档id就可以啦，避免反复拼写完整的文档名。</li></ol><p>到目前为止，我们已经可以根据单词来检索到哪些文档中含有这些单词、以及他们在文档中的位置。</p><p>不过现在只能一次处理一个单词，而且只能以文档当初被加载的顺序返回文档。</p><p>文档的顺序还需要优化，我们需要计算搜索关键字和文档之间的相关程度，相关程度越大，就应该越靠前显示。这些将在<strong>数据检索</strong>中完成。</p><h2 id="数据检索"><a href="#数据检索" class="headerlink" title="数据检索"></a>数据检索</h2><p>第一步：数据检索即数据查询，我们要根据输入的一系列单词，返回一个经过排序的文档列表。对搜索结果进行排序，我们需要几个度量指标来量化它们与搜索关键词的匹配程度。我们将学习到不同的量化指标，比如单词频率、单词出现的位置、单词位置之间的距离（如果用户输入了多个关键词的话）、其他页面对本页面的引用次数（google的pagerank算法）等。<br>HTML页面中的链接用a标签表示，为了能够表述页面引用情况，我们要对表格关系再做修改，将链接信息单独提取出来。</p><img src="/2017/12/02/SearchAndRanking/ER2.png" title="增加衔接描述"><p>上图表示一个链接包含三个属性：衔接处的描述单词、链接来自、衔接去向。衔接处的描述单词我们可以在单词表中找到。链接来自与衔接去向可在文档列表中找到对应。一个链接可能包含多个单词描述。比如<br></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"www.google.com"</span>&gt;</span></span><br><span class="line">  google good search</span><br><span class="line"><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br></pre></td></tr></table></figure><p></p><p>上述代码中‘google’、‘good’、‘search’都指向链接到google的a标签,制作成表格如下：</p><table><thead><tr><th>单词</th><th>去哪</th></tr></thead><tbody><tr><td>google</td><td>www.google.com</td></tr><tr><td>good</td><td>www.google.com</td></tr><tr><td>search</td><td>www.google.com</td></tr></tbody></table><p>我们注意到‘去哪’所处的列有很大的重复性，‘www.google.com’被重复写了三次，这意味着我们可以拆分表格来减少冗余。<br><img src="/2017/12/02/SearchAndRanking/ER_LINK.png" title="拆分链接实体表"><br>经过上述拆分之后，我们写‘www.google.com’的时候，只需要引用它的id即可，避免了多次重复书写。</p><p>第二步：我们把搜索结果呈递给用户之后，用户会点击自己想要的链接，我们需要<strong>学习</strong>用户的点击信息来对搜索结果中文档的顺序进行调整。我们会依靠用户的点击信息来建立<strong>神经网络</strong>来优化搜索结果的排名，神经网络会学习到人们过去的点击情况。</p><h1 id="抓取数据"><a href="#抓取数据" class="headerlink" title="抓取数据"></a>抓取数据</h1><p>假设我们硬盘中没有成堆的HTML文档在等待索引，那我们就写一个简单的爬虫程序来下载HTML文档，它接收一组等待下载的网页URL，然后根据这些网站中的链接来找到其他页面，这个过程叫<strong>蛛行</strong>（Spidering）。</p><h2 id="使用urllib来下载网页"><a href="#使用urllib来下载网页" class="headerlink" title="使用urllib来下载网页"></a>使用urllib来下载网页</h2><p>urllib是一个python库，作用是方便网页下载，我们只需要传入网站的URL。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line">c=urllib.request.urlopen(<span class="string">'http://test.com'</span>)</span><br><span class="line">contents = c.read()</span><br><span class="line">print(contents[<span class="number">0</span>:<span class="number">50</span>])</span><br></pre></td></tr></table></figure><p></p><h2 id="书写爬虫"><a href="#书写爬虫" class="headerlink" title="书写爬虫"></a>书写爬虫</h2><p>这个程序使用BeautifulSoup，它能为网页建立结构化的表现形式，并且对书写不规范的页面容错性较好。<br>使用urllib和BeautifulSoup，传入一组初始的URL列表，根据URL得到网页之后，可以根据网页中的链接信息找到其他网页。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> urlparse <span class="keyword">import</span> urljoin</span><br><span class="line"><span class="comment"># 构建一个单词列表，这些单词将被忽略</span></span><br><span class="line">ingorewords=([<span class="string">'the'</span>,<span class="string">'of'</span>,<span class="string">'to'</span>,<span class="string">'and'</span>,<span class="string">'a'</span>,<span class="string">'in'</span>,<span class="string">'is'</span>,<span class="string">'it'</span>])</span><br></pre></td></tr></table></figure><p></p><p>现在，我们可以书写爬虫代码了。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">crawler</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">crawl</span><span class="params">(self,pages,depth=<span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(depth):</span><br><span class="line">      newpages = set()</span><br><span class="line">      <span class="keyword">for</span> page <span class="keyword">in</span> pages:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">          c = urllib.request.urlopen(page)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">          print(<span class="string">"Could not open %s"</span>% page)</span><br><span class="line">          <span class="keyword">continue</span></span><br><span class="line">        soup = BeautifulSoup(c.read()) <span class="comment"># 调用c.read()下载网页</span></span><br><span class="line">        self.addtoindex(page,soup) <span class="comment"># 下载后建立索引，这是下一小节中的函数,将在下一小节讲解</span></span><br><span class="line">        <span class="comment"># 下面我们来处理这个页面中的链接</span></span><br><span class="line">        links = soup(<span class="string">'a'</span>)</span><br><span class="line">        <span class="keyword">for</span> link <span class="keyword">in</span> links:</span><br><span class="line">          <span class="keyword">if</span> (<span class="string">'href'</span> <span class="keyword">in</span> dict(link.attrs)): <span class="comment">#&lt;a href=""&gt; </span></span><br><span class="line">            url = urljoin(page,link[<span class="string">'href'</span>])</span><br><span class="line">          <span class="comment">#如果有单引号，说明该url和当前的URL的根域名不是同一个域名，如 baidu.com 与jd.com</span></span><br><span class="line">            <span class="keyword">if</span> url.find(<span class="string">"'"</span>)!=<span class="number">-1</span>: </span><br><span class="line">              <span class="keyword">continue</span></span><br><span class="line">            url = url.split(<span class="string">'#'</span>)[<span class="number">0</span>] <span class="comment"># 去掉页内定位部分，url中#后为锚点</span></span><br><span class="line">            <span class="comment"># 如果是一个网页链接，并且没有被索引过</span></span><br><span class="line">            <span class="comment"># isindexed判断是否已经是否已经索引过，避免重复索引</span></span><br><span class="line">            <span class="keyword">if</span> url[<span class="number">0</span>:<span class="number">4</span>] ==<span class="string">'http'</span> <span class="keyword">and</span> <span class="keyword">not</span> self.isindexed(url): </span><br><span class="line">              newpages.add(url)</span><br><span class="line">            linkText = self.gettextonly(link) <span class="comment">#获取a标签的描述文字,这将在下一小节中讲解</span></span><br><span class="line">      <span class="comment"># page代表当前url，url代表去向，即’去哪‘，linkText为链接上的文本</span></span><br><span class="line">            self.addlinkref(page,url,linkText)  <span class="comment">#这是建立索引部分，下一小节讲解</span></span><br><span class="line">        self.dbcommit()<span class="comment"># 下一小节讲解</span></span><br><span class="line">      <span class="comment"># 输入的URL搜索完后，要根据页面中的链接来找到其他页面  </span></span><br><span class="line">      pages = newpages</span><br></pre></td></tr></table></figure><p></p><p>该函数循环遍历网页列表，并针对每个网页调用addtoindex函数。addtoindex是建立索引部分，故我们这里暂时只把url打印出来<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addtoindex</span><span class="params">(self,url,soup)</span>:</span></span><br><span class="line">  print(<span class="string">'Indexing %s'</span> % url)</span><br></pre></td></tr></table></figure><p></p><p>随后，该函数利用BeautifulSoup取到网页中的所有链接，并将这些链接加入到一个名为newpages的集合中，一组url处理完毕后，我们将得到的newpages集合复制给pages，然后开启新的循环来处理这些url。</p><p>如果你学过二叉树遍历，那么很容易就知道上述过程其实就是广度优先搜索，我们也可以将其改成递归的深度优先搜索的形式，但递归层级较深时容易导致栈溢出。</p><p>我们来测试一下上述函数:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pageslist=[<span class="string">'http://kiwitobes.com/wiki/Perl.html'</span>]</span><br><span class="line">c = crawler(<span class="string">''</span>)</span><br><span class="line">crawl(pagelist)</span><br></pre></td></tr></table></figure><p></p><h1 id="建立索引"><a href="#建立索引" class="headerlink" title="建立索引"></a>建立索引</h1><p>建立索引的过程如前文所述，我们要建立一个数据表，其中包含了不同的单词、这些单词所在的文档和单词在文档中出现的位置。</p><p>在本例中，我们真正的文本内容进行考察，忽略非文本元素（如标点符号）。将文本内容拆解为单词，并给单词建立索引。</p><p>我们将使用sqlite来建立数据库，python3的标准库中已经有这个库了。</p><p>我们修改crawler类，增加数据库操作：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sqlite3</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">crawler</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,dbname)</span>:</span></span><br><span class="line">    self.con = sqlite3.connect(dbname)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__del__</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.con.close()</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">dbcommit</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.con.commit()<span class="comment"># 保存修改</span></span><br></pre></td></tr></table></figure><p></p><h2 id="设计数据库中的数据表"><a href="#设计数据库中的数据表" class="headerlink" title="设计数据库中的数据表"></a>设计数据库中的数据表</h2><p>根据前面的数据库的ER图，我们来设计数据库的表格。</p><table><thead><tr><th>表格名</th><th>表格描述</th></tr></thead><tbody><tr><td>urllist</td><td>保存已经索引过的URL列表</td></tr><tr><td>wordlist</td><td>单词列表</td></tr><tr><td>wordlocation</td><td>单词所在文档与在文档中所处的位置的列表</td></tr><tr><td>link</td><td>链接表，保存两个URL ID，指明从一个文档到另一个文档的链接关系</td></tr><tr><td>linkwords</td><td>哪些单词与链接有关</td></tr></tbody></table><img src="/2017/12/02/SearchAndRanking/sch.PNG" title="数据表设计图"><p>我们在这个图上更新一下对应关系：<br><img src="/2017/12/02/SearchAndRanking/sch2.png" title="数据表设计图"><br><em>上图链接表明，单词列表与’单词所处位置表‘的关系为1：N，即一个单词可以在文档中的多个位置出现。一个链接上可以有多个单词，所以link和linkwords的关系为1:N。文档名会被’单词所处位置表‘引用多次，所以关系为1：N。链接单词表中的单词在单词表中都能找到唯一的对应关系，故关系为1：1。链接表的的来自和去向分别对应url中的文档，故关系都是1：1。</em></p><p>所有的sqlite的表默认都有一个名为rowid的字段，故没有必要显示指定ID字段。</p><p>下面我们来写程序建立数据表：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">crawler</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createindextables</span><span class="params">(self)</span>:</span> </span><br><span class="line">    self.con.execute(<span class="string">'create table urllist(url)'</span>)</span><br><span class="line">    self.con.execute(<span class="string">'create table wordlist(word)'</span>)</span><br><span class="line">    self.con.execute(<span class="string">'create table wordlocation(urlid,wordid,location)'</span>)</span><br><span class="line">    self.con.execute(<span class="string">'create table link(fromid integer,toid integer)'</span>)</span><br><span class="line">    self.con.execute(<span class="string">'create table linkwords(wordid,linkid)'</span>)</span><br><span class="line">    <span class="comment"># 建立索引，加快搜索速度，该操作很重要，因为数据集会很大</span></span><br><span class="line">    self.con.execute(<span class="string">'create index wordidx on wordlist(word)'</span>)</span><br><span class="line">    self.con.execute(<span class="string">'create index urlidx on urllist(url)'</span>)</span><br><span class="line">    self.con.execute(<span class="string">'create index wordurlidx on wordlocation(wordid)'</span>)</span><br><span class="line">    self.con.execute(<span class="string">'create index urltoidx on link(toid)'</span>)</span><br><span class="line">    self.con.execute(<span class="string">'create index urlfromidx on link(fromid)'</span>)</span><br><span class="line">    self.dbcommit()</span><br></pre></td></tr></table></figure><p></p><p>测试代码，创建一个名为searchindex.db的数据库文件。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c=crawler(<span class="string">'searchindex.db'</span>)</span><br><span class="line">c.createindextables()</span><br></pre></td></tr></table></figure><p></p><p>我们会在查询阶段再加入一张表，它根据指向该文档的其他文档(外部回指链接-inbound link)的计数情况来作为度量。</p><h2 id="在网页中查找单词"><a href="#在网页中查找单词" class="headerlink" title="在网页中查找单词"></a>在网页中查找单词</h2><p>由于网页是HTML文档，其中有大量的标签、属性、以及其他不在索引范围内的信息，而我们只需要网页中所有的文字部分即可。下面我们来编写函数，搜集所有的文本内容。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gettextonly</span><span class="params">(self,soup)</span>:</span></span><br><span class="line">  v = soup.string</span><br><span class="line">  <span class="keyword">if</span> v ==<span class="keyword">None</span>:</span><br><span class="line">    c= soup.contents</span><br><span class="line">    resulttext=<span class="string">''</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> c:</span><br><span class="line">      subtext = self.gettextonly(t)</span><br><span class="line">      resulttext=subtext+<span class="string">'\n'</span></span><br><span class="line">    <span class="keyword">return</span> resulttext</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> v.strip()</span><br></pre></td></tr></table></figure><p></p><p>该函数返回一个长字符串，其中包含了网页中的所有文字。它以递归的方式对网页中的文档对象进行遍历，寻找其中的文本节点。网页的文档分为不同的段落，保持各段落的先后顺序是很重要的。因为我们搜索时，要考虑位置信息作为度量，来优化搜索结果中的排序。<br>我们搜索的时候，是输入单词来得到结果，故我们还需要将gettextonly返回的长字符串拆成一堆独立的单词。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">separatewords</span><span class="params">(self,text)</span>:</span></span><br><span class="line">  splitter = re.compile(<span class="string">'\\W*'</span>)<span class="comment"># 以任何非字母非数字的字符作为分割符号</span></span><br><span class="line">  <span class="keyword">return</span> [s.lower() <span class="keyword">for</span> s <span class="keyword">in</span> splitter.split(text) <span class="keyword">if</span> s!=<span class="string">''</span>]<span class="comment"># 去掉空单词</span></span><br></pre></td></tr></table></figure><p></p><p>由于上述拆分单词的方式把任何非字母非数字的字符都看做分隔符，对于英文单词的提取不会有问题，但这种形式无法正确处理类似“c++”这样的词汇，要完美的抽取单词并非一件简单的事，已经有大量的相关研究来改进这项技术。</p><blockquote><p>提示：我们可以使用词干提取法(Stemming algorithm)来去掉单词的后缀。词干提取算法试图将单词转换成对应的词干。例如：将单词“indexing”变成“index“。这样，人们在搜索”index“时也会得到包含单词”indexing“的文档。什么时候使用呢？ 我们可以在检索文档的时候提取单词的词干，也在在搜索时提取用户输入的查询词的词干。我们可以在<a href="http://www.tartarus.org/~martin/PorterStemmer/index.html" target="_blank" rel="noopener">Porter Stemmer</a> 找到一个很有名的词干提取算法的Python实现——Porter Stemmer。</p></blockquote><h2 id="加入索引"><a href="#加入索引" class="headerlink" title="加入索引"></a>加入索引</h2><h3 id="将普通文本加入索引"><a href="#将普通文本加入索引" class="headerlink" title="将普通文本加入索引"></a>将普通文本加入索引</h3><p>得了一个出现于网页中单词的列表之后，我们应该讲这些单词都加入到数据库中，并加入索引，在网页文档和单词之间建立关联，并保存单词在文档中出现的位置。在本文中，单词的位置就是其在单词列表中的序号。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addtoindex</span><span class="params">(self,url,soup)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> self.isindexed(url): <span class="keyword">return</span> <span class="comment"># 如果这个网页文档已经处理过了，略过</span></span><br><span class="line">  print(<span class="string">"indexing "</span>+url)</span><br><span class="line">  <span class="comment">#获得网页中的所有文本，并将其拆解为单词</span></span><br><span class="line">  text = self.gettextonly(soup)</span><br><span class="line">  words = self.separatewords(text)</span><br><span class="line">  <span class="comment"># 得到网页文档（用url来标识）的id，</span></span><br><span class="line">  <span class="comment"># getentryid返回文档在文档表中的id，如果不存在则文档表中新建一条记录，并将其ID返回</span></span><br><span class="line">  urlid = self.getentryid(<span class="string">'urllist'</span>,<span class="string">'url'</span>,url)<span class="comment"># 参数分别代表： 数据库中的表格名 字段 值</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 将每个单词加入数据库，并与文档进行管理</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(words)):</span><br><span class="line">    word = words[i]</span><br><span class="line">    <span class="keyword">if</span> word <span class="keyword">in</span> ingorewords:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    wordid = self.getentryid(<span class="string">'wordlist'</span>,<span class="string">'word'</span>,word)<span class="comment"># 将单词加入数据库中的单词表</span></span><br><span class="line">    <span class="comment"># 填充单词的所在文档与单词在文档位置表</span></span><br><span class="line">    self.con.execute(<span class="string">"insert into wordlocation(urlid,wordid,location)  values (%d,%d,%d)"</span> % (urlid,wordid,i))</span><br></pre></td></tr></table></figure><p></p><p>我们还需要写出两个辅助函数，getentryid和isindexed.</p><p>getentryid的作用是返回某个条目的ID，如果条目不存在，就会在相应的表中插入一条新纪录，并将ID返回。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getentryid</span><span class="params">(self,table,field,value)</span>:</span></span><br><span class="line">  cur = self.con.execute(<span class="string">"select rowid form %s where %s='%s' "</span> % (table,field,value))</span><br><span class="line">  res = cur.fetchone()</span><br><span class="line">  <span class="keyword">if</span> res == <span class="keyword">None</span>:</span><br><span class="line">    cur = self.con.execute(<span class="string">"insert into %s (%s) values ('%s') "</span>% (table,field,value))</span><br><span class="line">    <span class="keyword">return</span> cur.lastrowid</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> res[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p></p><p>isindexed 函数是判断网页文档是否加入了文档表中，如果在文档表中，判断是否与任何单词关联，如果没有关联，则认为该网页文档未被索引。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isindexed</span><span class="params">(self,url)</span>:</span></span><br><span class="line">  u = self.con.execute(<span class="string">"select rowid from urllist where url='%s' "</span> % url).fetchone()</span><br><span class="line">  <span class="keyword">if</span> u!=<span class="keyword">None</span>:</span><br><span class="line">    <span class="comment"># 检查是否有单词与这个文档关联</span></span><br><span class="line">    v = self.con.execute(<span class="string">"select * from wordlocation where urlid=%d"</span> %u[<span class="number">0</span>]).fetchone()</span><br><span class="line">    <span class="keyword">if</span> v!=<span class="keyword">None</span>:</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">False</span></span><br></pre></td></tr></table></figure><p></p><h3 id="将链接处的文本加入索引"><a href="#将链接处的文本加入索引" class="headerlink" title="将链接处的文本加入索引"></a>将链接处的文本加入索引</h3><p>由于pagerank算法参照了文档中的链接来优化搜索结果，故我们需要对链接文本进行记录。<br>这里先举个例子：<br></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"www.google.com"</span>&gt;</span>google good search<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br></pre></td></tr></table></figure><p></p><p>我们要处理这个链接，步骤是：</p><ol><li>将单词分解为3个单词，分别是google、good、search</li><li>获取当前的url（代表from）和这个链接指向的url(www.google.com)</li><li>如果链接是到自身的，什么都不操作</li><li>将from和to的关系插入到link表，返回该条目的序号</li><li>将第一步得到的3个单词插入到linkwords，将每个单词与第四步中返回的序号关联</li></ol><p>我们将上述的步骤写成代码:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addlinkref</span><span class="params">(self,urlFrom,urlTo,linkText)</span>:</span></span><br><span class="line">  words = self.separatewords(linkText)</span><br><span class="line">  fromid = self.getentryid(<span class="string">'urllist'</span>,<span class="string">'url'</span>,<span class="string">'urlFrom'</span>)</span><br><span class="line">  toid = self.getentryid(<span class="string">'urllist'</span>,<span class="string">'url'</span>,<span class="string">'urlTo'</span>)</span><br><span class="line">  <span class="keyword">if</span> fromid == toid:</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">  cur = self.con.execute(<span class="string">"insert into link(fromid,toid) values(%d,%d)"</span> % (fromid,toid))</span><br><span class="line">  linkid = cur.lastrowid</span><br><span class="line">  <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">    <span class="keyword">if</span> word <span class="keyword">in</span> ignorewords:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    wordid = self.getentryid(<span class="string">'wordlist'</span>,<span class="string">'word'</span>,word)</span><br><span class="line">    self.con.execute(<span class="string">"insert into linkwords (linkid,wordid) values(%d,%d)"</span> % (linkid,wordid))</span><br></pre></td></tr></table></figure><p></p><h3 id="测试加入索引功能"><a href="#测试加入索引功能" class="headerlink" title="测试加入索引功能"></a>测试加入索引功能</h3><p>再次执行crawler，现在已经将网页的内容添加到数据库并建立索引了。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pageslist=[<span class="string">'http://kiwitobes.com/wiki/Perl.html'</span>]</span><br><span class="line">c = crawler(<span class="string">'searchindex.db'</span>)</span><br><span class="line">crawl(pagelist)</span><br></pre></td></tr></table></figure><p></p><p>craler可能会运行较长时间，我们可以下载一份已经加载好的serachindex.db。</p><p>我们可以通过数据库查询操作，来得知检索的结果。下面我们试着检查某个单词的对应条目<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[row <span class="keyword">for</span> row <span class="keyword">in</span> crawerler.con.execute(<span class="string">'select urlid from wordlocation where wordid=1 '</span>)]</span><br><span class="line"><span class="comment"># 返回[(1,),(46),....]</span></span><br></pre></td></tr></table></figure><p></p><p>此处返回的包含单词id为1的所有文档的列表，这表示我们已经成功的进行了一次全文搜索。不过现在的代码只能输入一个单词来检索，而且只能够按照文档当初被加入索引的顺序来返回文档。<br>下面我们会扩展查询功能，实现可以同时查找多个单词。</p><h1 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h1><p>现在我们有了crawler程序来下载网页文档，并将网页文档加入到数据库。接下来应该实现的是搜索（查询）的部分了。我们来新建一个用于搜索的类。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">searcher</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,dbname)</span>:</span></span><br><span class="line">    self.con = sqlite.connect(dbname)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__del__</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.con.close()</span><br></pre></td></tr></table></figure><p></p><p>wordlocation数据表提供了单词与文档的联系。<br>上一节中我们实现了一个单词的搜索，下面我们来实现多个单词的同时搜索。<br>我们要写一个查询函数，接受一个查询字符串作为参数，我们需要将查询字符串拆分为多个单词，然后构造一个sql查询语句，只查找包含所有查询词的文档。<br>问题是怎样实现多词查找呢？ 这里先举个例子：<br>假设我们要做一个涉及三个单词的查找：单词id为10号和17号和21号。<br><img src="/2017/12/02/SearchAndRanking/multiword.jpg" title="三个单词的查找"><br>我们为每个单词建立一个wordlocation的表的引用，分别是w0，w1，w2。由于3个单词必须在一个文档中全部包含，故这三个词的urlid必须相同，如图上的虚线的箭头所示的那样。因为为wordlocation表的不同引用，故urlid必须相同这个条件可以写为”where w0.urlid=w1.urlid=w2.urlid”。<br>然后我们便可以写出如下完整的sql语句。<br></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> w0.urlid,w0.location,w1.location,w2.location </span><br><span class="line"><span class="keyword">from</span> wordlocation w0,wordlocation w1,wordlocation w2 </span><br><span class="line"><span class="keyword">where</span> w0.urlid=w1.urlid=w2.urlid </span><br><span class="line"><span class="keyword">and</span> w0.wordid=<span class="number">10</span> <span class="keyword">and</span> w1.wordid=<span class="number">17</span> <span class="keyword">and</span> w2.wordid=<span class="number">21</span></span><br><span class="line"><span class="comment">/*返回示例[(1,327,23,33),(1,327,23,39),(1,327,23,143) 等等]*/</span></span><br><span class="line"><span class="comment">/*每一项的含义为(文档名id，单词1位置，单词2位置，单词3位置)*/</span></span><br><span class="line"><span class="comment">/*也可以写作*/</span></span><br><span class="line"><span class="keyword">select</span> w0.urlid,w0.location,w1.location,w2.location </span><br><span class="line"><span class="keyword">from</span> wordlocation w0,wordlocation w1,wordlocation w2 </span><br><span class="line"><span class="keyword">where</span> w0.urlid=w1.urlid </span><br><span class="line"><span class="keyword">and</span> w0.wordid=<span class="number">10</span> <span class="keyword">and</span> w1.wordid=<span class="number">17</span> </span><br><span class="line"><span class="keyword">and</span> w1.urlid = w2.urlid <span class="keyword">and</span> w2.wordid=<span class="number">21</span></span><br></pre></td></tr></table></figure><p></p><p>我们注意到，根据单位位置的不同组合，每个网页文档的id会返回多次。上述结果返回的都是文档1中三个单词的不同位置的组合。<br>我们来写程序，来自动根据输入单词的个数来建立可以同时查询多词的sql语句。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getmatchrows</span><span class="params">(self,q)</span>:</span></span><br><span class="line">  <span class="comment"># 构造查询字符串</span></span><br><span class="line">  fieldlist = <span class="string">'w0.urlid'</span> <span class="comment">#要得到的结果字段</span></span><br><span class="line">  tablelist = <span class="string">''</span> <span class="comment">#要建立的引用表的个数，2个单词的情况下，wordlocation的三个引用分别为w0，w1</span></span><br><span class="line">  clauselist=<span class="string">''</span> <span class="comment"># where后的条件限定字符串</span></span><br><span class="line">  wordids=[]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 根据空格拆分单词</span></span><br><span class="line">  words = s.split(<span class="string">' '</span>)</span><br><span class="line">  tablenumber = <span class="number">0</span> <span class="comment"># 要构建的引用表的数量，为查询单词的数量</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">    <span class="comment"># 获取单词的id</span></span><br><span class="line">    wordrow = self.con.execute(<span class="string">"select rowid from wordlist where word=%s"</span> % word).fetchone()</span><br><span class="line">    <span class="keyword">if</span> wordrow!=<span class="keyword">None</span>:</span><br><span class="line">      wordid=wordrow[<span class="number">0</span>]</span><br><span class="line">      wordids.append(wordid)</span><br><span class="line">      <span class="keyword">if</span> tablenumber&gt;<span class="number">0</span>:</span><br><span class="line">        tablelist = <span class="string">','</span></span><br><span class="line">        clauselist+=<span class="string">' and '</span></span><br><span class="line">        clauselist+=<span class="string">'w%d.urlid=w%d.urlid and '</span> % (tablenumber<span class="number">-1</span>,tablenumber)</span><br><span class="line">      fieldlist+= <span class="string">',w%d.location'</span> % tablenumber</span><br><span class="line">      tablelist+=<span class="string">'wordlocation w%d'</span> % tablenumber</span><br><span class="line">      clauselist+=<span class="string">'w%d.wordid=%d'</span> % (tablenumber,wordid)</span><br><span class="line">      tablenumber+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 根据各个组成部分，构建查询语句</span></span><br><span class="line">  fullquery = <span class="string">'select %s from %s where %s'</span> % (fieldlist,tablelist,clauselist)</span><br><span class="line">  cur = self.con.execute(fullquery)</span><br><span class="line">  rows = [row <span class="keyword">for</span> row <span class="keyword">in</span> cur]</span><br><span class="line">  <span class="keyword">return</span> rows,wordids</span><br></pre></td></tr></table></figure><p>此时我们已经可以同时搜索多个单词，下面我应该考虑几种度量方法，来读搜索结果进行排序。<br>本文中会介绍三种度量方法：</p><ol><li>基于内容的排名法(Content-based ranking) 该方法主要根据网页的内容，利用某些可行的度量方式对查询结果进行判断。</li><li>基于外部回指链接的排名法(Inbound-link ranking) 利用网页之间的链接结构来决定查询结果中各个文档的重要性。</li><li>利用人民搜索时，对搜索结果的实际点击情况，进行关联学习的神经网络方法。</li></ol><h2 id="基于内容的排名"><a href="#基于内容的排名" class="headerlink" title="基于内容的排名"></a>基于内容的排名</h2><p>到目前为止，我们已经成功获得了与查询条件相匹配的网页。不过，其返回结果的排列顺序却很简单，即其被检索时的顺序。<br>我们需要在大量提及到你查询词的网页中，逐一的浏览直到你找到真正想要的页面。<br>为了解决这一问题，我们需要一种对网页评分的方法，并将评价最高的网页排在最前面。<br>本小节将提到3种基于查询词与文档内容的相关程度来计算评分的方法。分别是：</p><ol><li>单词频度 查询条件中的单词在文档中出现的次数有助于我们判断文档的相关程度。</li><li>文档位置 文档的主题一般会在文档的开始处出现。</li><li>单词距离 如果查询条件中有多个单词，在文档中它们之间的位置应该很近。</li></ol><p>我们将会使用3个度量做为对搜过结果的排序依据。这里，我们写一个方法，来调用这三个函数,并对这3种度量方法加上一个权重。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># getscroedlist是调用3个评价函数，将将这3个评价函数的评分按权重加起来</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getscoredlist</span><span class="params">(self,rows,wordids)</span>:</span></span><br><span class="line">  <span class="comment"># rows种的数据项格式为rows[0]为url row[1]为第一个单词的位置</span></span><br><span class="line">  totalscores = dict([(rows[<span class="number">0</span>],<span class="number">0</span>) <span class="keyword">for</span> row <span class="keyword">in</span> rows])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 调用评价函数的地方</span></span><br><span class="line">  weights=[]<span class="comment">#每一项的格式为(权重，调用评价函数后的评分)</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (weight,scores) <span class="keyword">in</span> weights:</span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> totalscores:</span><br><span class="line">      totalscores[url]+=weight*scores[url]</span><br><span class="line">  <span class="keyword">return</span> totalscores</span><br><span class="line"><span class="comment">#根据文档的id获取文档完整的url</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">geturlname</span><span class="params">(self,id)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> self.con.execute(<span class="string">"select url from urllist where rowid=%d"</span> % id).fetchone[<span class="number">0</span>]</span><br><span class="line"><span class="comment">#查询函数的实现</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">query</span><span class="params">(self,q)</span></span></span><br><span class="line">  rows,wordids=self.getmatchrows(q)</span><br><span class="line">  scores=self.getscoredlist(rows,wordids)</span><br><span class="line">  rankedscores=sorted([(score,url) <span class="keyword">for</span> (url,score) <span class="keyword">in</span> scores.items()],reverse=<span class="number">1</span>)</span><br><span class="line">  <span class="keyword">for</span> (score,urlid) <span class="keyword">in</span> rankedscores[<span class="number">0</span>:<span class="number">10</span>]: <span class="comment"># 只取评分最高的前10个</span></span><br><span class="line">    print(<span class="string">"%f\t%s"</span> % (score,self.geturlname(urlid)))</span><br></pre></td></tr></table></figure><p></p><p>query方法调用的 getscoredlist 还没有调用任何评价方法，但是它已经可以打印出评分和url。<br>格式为:<br>网页1的评分 网页1<br>网页2的评分 网页2</p><h3 id="归一化的方法"><a href="#归一化的方法" class="headerlink" title="归一化的方法"></a>归一化的方法</h3><p>在书写各个评价方法的函数之前，我们提前想一下，给予单词频度的方法，单词出现的次数越多表明相关程度越大；而基于单词位置的方法，是单词越靠前越大，也就是代表位置的数据越小越好。<br>为了对不同方法的返回结果进行比较，我们需要一种归一化方法，即，使它们有相关的值域和统一的方向，比如：这些评价函数都是值越大越好，为了做到这点，我们可以给单词位置函数的调用结果取个倒数。<br>该函数接受一个代表文档的id和文档评分的字典为参数，指明了值越大越好还是越小越好之后，调用该函数会返回一个带有相同文档id，评价值介于0到1之间的新字典。<br>函数根据每个评价值与最佳结果的接近程度，对各个记录做了缩放处理。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalizescores</span><span class="params">(self,scores,smallIsBetter=<span class="number">0</span>)</span>:</span></span><br><span class="line">  vsmall = <span class="number">0.00001</span><span class="comment"># 避免除数为0</span></span><br><span class="line">  <span class="keyword">if</span> smallIsBetter:<span class="comment"># 如果数字越小越好，则取分数的倒数，由于评分最低应该划分到1，故乘上最小值</span></span><br><span class="line">    minscore = min(scores.values()) </span><br><span class="line">    <span class="comment"># 下一行中的max是为了避免除数为0的问题</span></span><br><span class="line">    <span class="keyword">return</span> dict([(url,float(minscore)/max(vsmall,score) ) <span class="keyword">for</span> (url,score) <span class="keyword">in</span> scores.items()])</span><br><span class="line">  <span class="keyword">else</span>:<span class="comment"># 如果是越大越好，每个数都和最大值相除</span></span><br><span class="line">    maxscore = max(scores.values())</span><br><span class="line">    <span class="keyword">if</span> maxscore==<span class="number">0</span>:</span><br><span class="line">      maxscore=vsmall</span><br><span class="line">    <span class="keyword">return</span> dict([(url,float(score)/maxscore ) <span class="keyword">for</span> (url,score) <span class="keyword">in</span> scores.items()])</span><br></pre></td></tr></table></figure><p></p><p>每个评价函数都会调用该函数，将结果进行归一化处理，得到一个介于0-1之间的值。</p><h3 id="单词频度"><a href="#单词频度" class="headerlink" title="单词频度"></a>单词频度</h3><p>这种方法以单词频度作为度量手段，根据查询条件中的单词在网页中出现的次数对网页进行评分。<br>假如搜索“python”，我们更希望得到的是一个内容中多次提到该单词的页面，比如有关python语言的网页；而不是一个关于某个音乐家的网页，可能这位音乐家在文章的末尾处偶尔提到了他有一条宠物蟒蛇(英文是：python)。<br>我们来编写函数实现这个功能：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">frequencyscore</span><span class="params">(self,rows)</span>:</span></span><br><span class="line">  counts=dict([(row[<span class="number">0</span>],<span class="number">0</span>)<span class="keyword">for</span> row <span class="keyword">in</span> rows])<span class="comment"># rows中的每一项格式为： url 单词1出现位置 单词2出现位置</span></span><br><span class="line">  <span class="keyword">for</span> row <span class="keyword">in</span> rows:</span><br><span class="line">    counts[row[<span class="number">0</span>]]+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> self.normalizescores(counts)</span><br></pre></td></tr></table></figure><p></p><p>该函数创建了一个字典，其键为网页文档的URL，值为该文档中‘包含查询单词的’单词数量。最后进行归一化处理，在本例中，分值越大越好。<br>我们修改getscoredlist的weights一行:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weights=[(<span class="number">1.0</span>,self.frequencyscore(rows))]</span><br></pre></td></tr></table></figure><p></p><p>测试一下效果:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">e=searcher(<span class="string">'searchindex.db'</span>)</span><br><span class="line">e.query(<span class="string">'functional programming'</span>)</span><br><span class="line"><span class="comment">#1.000000 http://kiwitobes.com/wiki/Functional_programming.html</span></span><br><span class="line"><span class="comment">#0.262476 http://kiwitobes.com/wiki/Categorical_list_of_programming_languages.html 0.062310 #http://kiwitobes.com/wiki/Programming_language.html</span></span><br><span class="line"><span class="comment">#0.043976 http://kiwitobes.com/wiki/Lisp_programming_language.html</span></span><br><span class="line"><span class="comment">#0.036394 http://kiwitobes.com/wiki/Programming_paradigm.html</span></span><br></pre></td></tr></table></figure><p></p><p>上述返回结果中”Functional programming”的网页放在了最前面，后面是另外几个相关的网页，从结果中我们可以查出，“function programming”的评分结果是其他评分结果的4倍。大多数的搜索引擎不会将最终的评价结果给用户，但这些评分信息是很重要的。利用评分信息，我们可以设定仅仅显示评分超过某个阈值的网页；我们也可以根据网页的相关程度，按一定比例的字体太小来显示不同条目。</p><h3 id="单词位置"><a href="#单词位置" class="headerlink" title="单词位置"></a>单词位置</h3><p>另一个判断网页与查询条件相关程度的简单度量方法，是搜索单词在网页中的位置。<br>通常，如果一个网页与待搜索的单词相关，则该单词就更有可能在靠近网页开始处的位置出现，甚至出现在标题中。<br>利用这一点，搜索引擎可以待查单词出现越早的情况给予越高的评价。<br>由于我们建立索引时保持了单词的位置信息，现在我们可以利用位置信息来评价网页了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">locationscore</span><span class="params">(self,rows)</span>:</span><span class="comment"># rows的每一项格式为：网页URL 单词1出现的位置 单词2出现的位置 ...</span></span><br><span class="line">  locations = dict([(row[<span class="number">0</span>],<span class="number">100000</span>) <span class="keyword">for</span> row <span class="keyword">in</span> rows])<span class="comment">#1000000代表默认的最靠后的位置</span></span><br><span class="line">  <span class="keyword">for</span> row <span class="keyword">in</span> rows:</span><br><span class="line">    loc = sum(row[<span class="number">1</span>:])<span class="comment">#这里将一个单词的所有位置的序号都加起来。</span></span><br><span class="line">    <span class="keyword">if</span> loc&lt;locations[row[<span class="number">0</span>]]:</span><br><span class="line">      locations[row[<span class="number">0</span>]] = loc</span><br><span class="line">  <span class="keyword">return</span> self.normalizescores(locations,smallIsBetter=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>一个文档中的一个单词可能会多次出现，我们计算一个文档中的一个单词的出现的所有位置之和，作为度量，将这个结果和过去最好的结果进行比较判断，得到一个单词的位置信息之和改为最小的。我们对结果进行归一化，smallIsBetter意味着，位置之和最小的网页或的评价值为1.0。<br>我们修改getscoredlist的weights一行:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weights=[(<span class="number">1.0</span>,self.locationscore(rows))]</span><br></pre></td></tr></table></figure><p></p><p>测试一下效果:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">e=searcher(<span class="string">'searchindex.db'</span>)</span><br><span class="line">e.query(<span class="string">'functional programming'</span>)</span><br></pre></td></tr></table></figure><p></p><p>到目前为止，我们所介绍的所有度量方法中，没有任何一种方式对于每一种情况而言都是最有的。对于搜索者而言，各种度量方法都是有效的；而在一组特定的文档中，为了给出最佳结果，不同的加权组合也是必要的。我们可以对weights一行按照如下的方式修改，为两种不同的度量分配不同的权重。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weights =[(<span class="number">1.0</span>,self.frequencyscore),(<span class="number">1.5</span>,self.locationscore)]</span><br></pre></td></tr></table></figure><p></p><p>请尝试不同权重和查询，看看对查询结果的影响如何。<br>相对于单词频度，单词位置信息更难以作假，我们可以在文档的开始处不断重复待查询的单词，但这对基于单词位置的度量方法影响很小。</p><h3 id="单词距离"><a href="#单词距离" class="headerlink" title="单词距离"></a>单词距离</h3><p>当查询中包含多个单词时，寻找单词彼此之间距离很近的网页往往是有意义的。<br>大多数时候在进行多次查询时，人们时常会关注于哪些在概念意义上与这些词有关联的网页。<br>这和搜索引擎的引号用法相同，双引号代表仅显示完全匹配结果，搜索出来的信息要包含所有查询词，而且顺序也必须保持一致，而且查询词之间不能夹带任何额外的单词。<br>在这里，我们使用一种较为宽松的度量，允许单词的顺序与查询词的顺序不一致，也允许其中间夹带其他的单词。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distancescore</span><span class="params">(self,rows)</span>:</span></span><br><span class="line">  <span class="comment">#rows每一项的格式为：url 单词1出现的位置 单词2出现的位置</span></span><br><span class="line">  <span class="comment"># 如果用户输入只有一个单词的情况,rows[0] 此时情况为(url,单词1位置)</span></span><br><span class="line">  <span class="keyword">if</span> len(rows[<span class="number">0</span>]&lt;=<span class="number">2</span>):</span><br><span class="line">  <span class="comment"># 只有一个单词，所有文档的得分都一样</span></span><br><span class="line">    <span class="keyword">return</span> dict([(row[<span class="number">0</span>],<span class="number">1.0</span>) <span class="keyword">for</span> row <span class="keyword">in</span> rows])</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 初始化字典，填入一个很大的数作为初始值</span></span><br><span class="line">  mindistance = dict([(rows[<span class="number">0</span>],<span class="number">1000000</span>) <span class="keyword">for</span> row <span class="keyword">in</span> rows])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> row <span class="keyword">in</span> rows:</span><br><span class="line">    dist=sum([  abs(row[i]-row[i<span class="number">-1</span>]) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,len(row))  ])</span><br><span class="line">    <span class="keyword">if</span> dist&lt;mindistance[row[<span class="number">0</span>]]:</span><br><span class="line">      mindistance[row[<span class="number">0</span>]]=dist</span><br><span class="line">  <span class="keyword">return</span> self.normalizescores(mindistance,smallIsBetter=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p></p><p>当函数遍历单词位置时，它会计算处每个单词位置和上一个单词位置之间的差距。<br>由于查询会遍历每一种距离组合，因此函数将确保找出总距离的最小值。<br>我们修改getscoredlist的weights一行来测试这个度量方法，不过建议将该度量方法与其他度量组合使用，才能取得更好的效果。</p><h2 id="基于外部回指链接的排名"><a href="#基于外部回指链接的排名" class="headerlink" title="基于外部回指链接的排名"></a>基于外部回指链接的排名</h2><p>前面我们介绍的方法都是基于网页中的文本内容的。尽管许多搜索引擎依然采用这些方法，但如果我们能够参照其他网站对目标网站的引用情况时，搜索结果会被进一步改善。<br>我们会考察哪些网站链接到了目标网站，以及他们对网页的评价来作为度量。<br>这种方法对于屏蔽一些垃圾网站和钓鱼网站特别管用，因为和有意义的网站和真实网站相比，这些网页被其他网站引用的可能性很小。<br>前面我们已经对链接处的文本进行了特殊处理——links表记录了‘来源’和‘去处’的相应的文档的URL ID，而且linkswords还记录了单词与链接的关联。<br>我们会考察外部回指链接的数量和质量。即这个网页被其他网站引用的数量的多少；这个网站被知名的大网站引用的情况如何（因为如果一个权威网站引用的网页，其网页的质量一般会比较好）。</p><h3 id="数量上–简单计数"><a href="#数量上–简单计数" class="headerlink" title="数量上–简单计数"></a>数量上–简单计数</h3><p>这是一种很简单的做法，统计一下有几个网站引用了目标页面即可。<br>科研论文的评价就是采用这种方式，人们将论文的重要程度与该论文的被引用次数进行关联，论文被引用的次数越多，表明这篇论文的重要程度越高。<br>下面，我们书写函数，统计links表中每个页面没引用的次数，然后建立起一个键为网页的URL ID，值为被引用次数的字典。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inboundlinkscore</span><span class="params">(self,rows)</span>:</span></span><br><span class="line">  uniqueurls = set([row[<span class="number">0</span>] <span class="keyword">for</span> row <span class="keyword">in</span> rows])<span class="comment"># 使用set来去重</span></span><br><span class="line">  inboundcount = dict([  (url,self.con.execute(<span class="string">' \</span></span><br><span class="line"><span class="string">  select count(*) from link where toid=%d'</span> %url).fetchone()[<span class="number">0</span>])  \</span><br><span class="line">  <span class="keyword">for</span> url <span class="keyword">in</span> uniqueurls ])</span><br><span class="line">  <span class="keyword">return</span> self.normalizescores(inboundcount)</span><br></pre></td></tr></table></figure><p></p><p>该方法对于符合查询条件的所有网页，根据外部回指链接数进行排序。可见，这种方法是有前提的，即我们必须先得到符合查询条件的所有网页，然后才能通过网页的被引用数来对结果进行排序。所以，我们需要将该方法集合先前介绍的任何一种度量方法一起使用。<br>上述统计方法对于所有的外部回指链接都使用了相同的权重，但这种方法存在问题，如果一个人想故意刷排名的话，他只需要建立起很多站点，然后引用他想提高排名的网站即可。<br>我们也会发现，一些被权威网站引用的网页，往往会是我们感兴趣的网站，对于权威网站上被引用的网页，我们应该给与更高的权重值才合理。</p><h3 id="质量上–pagerank算法"><a href="#质量上–pagerank算法" class="headerlink" title="质量上–pagerank算法"></a>质量上–pagerank算法</h3><p>PageRank算法是由Google的创始人发明的，现在基于这一思路的各种变体已经被所有大型的搜索引擎采用。<br>该算法为每个网页都赋予了一个指示网页重要程度的评价值。<br>网页的重要性的评价依据是指向该网页的其它网站的重要性，以及这些网页中所包含的连接数求得。</p><blockquote><p>PageRank计算的是某个人在任意次链接单击之后到达想要页面的可能性。如果一个页面被很多热门网站引用，那么用户很有可能“很幸运的”发现这个网页。如果用户不停的单击，他们会把每个页面都点击一遍，但大多数人，经过一定次数的点击，找到自己想要的页面后，就不再点击链接来跳转到其他页面了。为了反映这一情况，PageRank算法使用了一个值为0.85的阻尼因子，用来指示用户持续点击每个网页中链接的概率为85%。<br><img src="/2017/12/02/SearchAndRanking/zuni.png" title="阻尼系数的含义"><br>上图中，我们用位于中心的最小的圆视为初始页面，用户点击当前页面的链接的概率为0.85，此时到达了一个新的页面，然后那个新页面中的链接到达其他的页面的概率也是0.85，而相对于初始页面，我们点击到第二层的概率为0.85*0.85=0.7225，经过不断的点击到达新页面，那么从初始页面到这个新页面的概率要不断的乘0.85，直到为接近0，此时便可以视为用户停止点击。</p></blockquote><p>下面，我们展示一组网页之间链接的例子，来表述PageRank评分的计算方法：<br><img src="/2017/12/02/SearchAndRanking/links.png" title="计算A的PageRank值"><br>A、B、C、D代表4个页面，箭头表示它们的链接。网页B、C、D都指向了A，它们的PageRank值已经计算得出。<br>B除了指向A之外，还指向其它的3个页面；C除了指向A之外，还指向其它4个页面；D只指向A。<br>为了得到A的PageRank值，我们将指向A页面的每个网页的PageRank(PR)评分除上这些网页中的链接总数，然后将这个数字乘上阻尼因子0.85,再加上0.15的最小值。<br>PR(A)的计算公式如下所示：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">PR(A)=<span class="number">0.15</span>+<span class="number">0.85</span>*(  PR(B)/links(B)+PR(C)/links(C)+PR(D)/links(D)  )</span><br><span class="line">     =<span class="number">0.15</span>+<span class="number">0.85</span>*(  <span class="number">0.5</span>/<span class="number">4</span> + <span class="number">0.7</span>/<span class="number">5</span> +<span class="number">0.2</span>/<span class="number">1</span>  )</span><br><span class="line">     =<span class="number">0.15</span>+<span class="number">0.85</span>(<span class="number">0.125</span>+<span class="number">0.14</span>+<span class="number">0.2</span>)</span><br><span class="line">     =<span class="number">0.15</span>+<span class="number">0.85</span>*<span class="number">0.465</span></span><br><span class="line">     =<span class="number">0.54525</span></span><br></pre></td></tr></table></figure><p></p><p>我们发现，D只指向A，所以D贡献了全部都PageRank分值给A，尽管D相比对B或C的PageRank评分较低，但D对A的PageRank评分值贡献最大，为0.2；而B贡献了0.125，C贡献了0.14。<br>很简单的计算方法吧！ 不过还有一个问题，在计算A的PageRank评分之前，指向A的网页都已经有了PageRank评分。换句话说，在计算一个网页的PageRank评分时，必须提前得知指向这个网页的其他网页的PageRank评分才行。<br>那么如何对一组还没有PageRank评分值的网页进行PageRank计算呢?<br>为了解决这个问题，我们可以先为所有的页面设置一个随机的PageRank评分。由于比较热门的网站，很多网站会引用它；那么计算PageRank评分的时候，热门网站的分值会提高。修正了PageRank评分之后，我们再迭代这一过程，PageRank的评分又会被修正一些，这样每个网页的PageRank评分会慢慢接近真实值。<br>PageRank的计算是一项耗时的工作，而且计算结果不会因为查询条件的变化而变化，所以，我们可以建立一个函数为每个URL提前计算PageRank评分，并将评分数据放入到数据表中。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculatepagerank</span><span class="params">(self,iterations=<span class="number">20</span>)</span>:</span></span><br><span class="line">  <span class="comment"># 清除现有的PageRank表</span></span><br><span class="line">  self.con.execute(<span class="string">'drop table if exists pagarank'</span>)</span><br><span class="line">  self.con.execute(<span class="string">'create table pagerank(urlid primary key,score)'</span>)</span><br><span class="line">  <span class="comment"># 初始化每个url，将其PageRank值设置为1（初始化为多少都可以，如果迭代次数足够多，PageRank评分会慢慢接近真实值）</span></span><br><span class="line">  self.con.execute(<span class="string">'insert into pagerank select rowid,1.0 from urllist'</span>)</span><br><span class="line">  self.dbcommit()</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(iterations):</span><br><span class="line">    print(<span class="string">'Iteration %d'</span> % i)</span><br><span class="line">    <span class="keyword">for</span> (urlid,) <span class="keyword">in</span> self.con.execute(<span class="string">'select rowid from urllist'</span>):</span><br><span class="line">      pr = <span class="number">0.15</span> <span class="comment"># 最小值，85%的概率可能继续点击，15%的概率不会再点击</span></span><br><span class="line">      <span class="comment">#获得引用过当前页面的所有其他页面</span></span><br><span class="line">      <span class="keyword">for</span> (linker,) <span class="keyword">in</span> self.con.execute(<span class="string">'select distinct fromid from link where toid=%d'</span> %urlid):</span><br><span class="line">        <span class="comment">#得到‘其他页面’的PageRank评分</span></span><br><span class="line">        linkingpr=self.con.execute(<span class="string">'select score from pagerank where urlid=%d'</span> % linker).fetchone()[<span class="number">0</span>]</span><br><span class="line">        <span class="comment">#得到‘其他页面’的总链接数</span></span><br><span class="line">        linkingcount=self.con.execute(<span class="string">'select count(*) from link where fromid=%d'</span> %d linker).fetone()[<span class="number">0</span>]</span><br><span class="line">        pr+=<span class="number">0.85</span>*(linkingpr/linkingcount)</span><br><span class="line">      self.con.execute(<span class="string">'update pagerank set score=%f where urlid=%d'</span> % (pr,urlid))</span><br><span class="line">      self.dbcommit()</span><br></pre></td></tr></table></figure><p></p><p>我们来运行该函数:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">crawler = crawler(<span class="string">'searchindex.db'</span>)</span><br><span class="line">crawler.calculatepagerank()</span><br></pre></td></tr></table></figure><p></p><p>如果我们想知道示例数据集中的哪个网页PageRank值最高，可以直接查询数据库。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cur = crawler.con.execute(<span class="string">'select * from pagerank order by score desc'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">  print(cur.next())</span><br><span class="line"><span class="comment"># 打印值为：</span></span><br><span class="line"><span class="comment">#(438, 2.5285160000000002)</span></span><br><span class="line"><span class="comment">#(2, 1.1614640000000001)</span></span><br><span class="line"><span class="comment">#(543, 1.064252)</span></span><br><span class="line">crawler.geturlname(<span class="number">438</span>)</span><br><span class="line"><span class="comment">#u'http://kiwitobes.com/wiki/Main_Page.html'</span></span><br></pre></td></tr></table></figure><p></p><p>从上述结果可以得知，“Main page”是分值是最高的，这也很正常，因为Wikipedia中的每一个网页都链接到该页面。<br>既然我们已经得到了每个网页的PageRank评分，那么我们应该将其加入评价函数中。<br>我们先写一个函数从数据库取出PageRank评分，并做归一化处理：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pagerankscore</span><span class="params">(self,rows)</span>:</span></span><br><span class="line">  pageranks=dict( [row[<span class="number">0</span>],self.con.execute(<span class="string">'select score from pagerank where urlid=%d'</span> % row[<span class="number">0</span>]).fetchone()[<span class="number">0</span>] \</span><br><span class="line">  <span class="keyword">for</span> row <span class="keyword">in</span> rows]  )</span><br><span class="line">  maxrank=max(pageranks.values())</span><br><span class="line">  normalizedscores = dict( [(url,float(score)/maxrank) <span class="keyword">for</span> (url,score) <span class="keyword">in</span> pageranks.items() ] )</span><br></pre></td></tr></table></figure><p></p><p>再次修改weights表，将PageRank算法加入其中：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weights=[(<span class="number">1.0</span>,self.locationscore(rows)),(<span class="number">1.0</span>,self.frequencyscore(rows)),(<span class="number">1.0</span>,self.pagerankscore(rows))]</span><br></pre></td></tr></table></figure><p></p><p>现在搜索结果不仅考虑了PageRank评分，还考虑了网页内容的评分，现在我们搜索”Functional Programming”，发现搜索结果的排列结果显得更加合理了。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2.318146</span> http://kiwitobes.com/wiki/Functional_programming.html</span><br><span class="line"><span class="number">1.074506</span> http://kiwitobes.com/wiki/Programming_language.html</span><br><span class="line"><span class="number">0.517633</span> http://kiwitobes.com/wiki/Categorical_list_of_programming_languages.html </span><br><span class="line"><span class="number">0.439568</span> http://kiwitobes.com/wiki/Programming_paradigm.html</span><br><span class="line"><span class="number">0.426817</span> http://kiwitobes.com/wiki/Lisp_programming_language.html</span><br></pre></td></tr></table></figure><p></p><h3 id="利用衔接文本信息"><a href="#利用衔接文本信息" class="headerlink" title="利用衔接文本信息"></a>利用衔接文本信息</h3><p>在这一小节开始之前，我们看一个例子：<br></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;a href="www.google.com"&gt;best search engine&lt;/a&gt;</span><br></pre></td></tr></table></figure><p></p><p>上述的代表表示一个指向www.google.com的链接，链接的描述文字为“best search engine”，如果你写过网页，你会知道写链接的时候我们会给链接网址一个解释其内容的简短描述。所以，衔接文本上的信息会比较有价值，我们要利用这一信息，改善对搜索结果的排序。<br>对于一个目标网页，我们要参照所有其他网页对该网页的简短描述信息，来为目标网站打分。<br>我们按步骤来进行该过程：</p><ol><li>找到所有引用了目标网站的‘其他页面’。</li><li>查看‘其他页面’对目标网站的简短描述。</li><li>将用户的查询条件视为’用户描述’ 与 第二步中得到的‘其他网页’对目标页面的简短描述做对比，如果‘其他页面’的简短描述中包含用户描述的单词，那么就将‘简介单词中包含用户输入查询词’的引用目标的页面的PageRank评分，作为对该网页的评分。<br>一个目标页面可能被多个网站链接并描述，故我们需要将符合条件的网页的PageRank评分累加起来,作为目标页面的评分。</li></ol><p>我们也可以换一个流程，效果是一样的：</p><ol><li>根据查询词，找到链接表中含有查询词的所有链接。因为使用查询词做搜索条件，所以这些链接的描述与查询词一致。</li><li>如果第一步得到的链接目标中包含了搜索结果中的数据项，说明这个链接的引用页描述与用户描述一致，应该加分。</li><li>如果第一步中有多个链接，映射到搜索结果中的同一个数据项，则评分应该累加。</li></ol><p>我们的程序就是按照上述的流程写的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rows为搜索结果,wordids是用户输入的查询词</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linktextscore</span><span class="params">(self,rows,wordids)</span>:</span></span><br><span class="line">  linkscores=dict([(rows[<span class="number">0</span>],<span class="number">0</span>)<span class="keyword">for</span> row <span class="keyword">in</span> rows]) <span class="comment"># rows每一项格式为:url 第一个单词的位置 第二个单词的位置...</span></span><br><span class="line">  <span class="keyword">for</span> wordid <span class="keyword">in</span> wordids:</span><br><span class="line">    <span class="comment"># 查看查询词中有哪些链接,得到'其他页面',而rows的第一列都是搜索结果页面，即'目标页面'</span></span><br><span class="line">    cur = self.con.execute(<span class="string">'select link.fromid,link.toid from linkwords,\</span></span><br><span class="line"><span class="string">    link where wordid=%d and linkwords.linkid=link.rowid'</span> %wordid)</span><br><span class="line">    <span class="keyword">for</span> (fromid,toid) <span class="keyword">in</span> cur:</span><br><span class="line">      <span class="keyword">if</span> toid <span class="keyword">in</span> linksscore:<span class="comment">#查询词得到的‘其他页面’，涵盖了搜索结果中的目标页面</span></span><br><span class="line">        <span class="comment">#得到pagerank，计算累加和</span></span><br><span class="line">        pr=self.con.execute(<span class="string">'select score from pagerank where urlid=%d'</span> % fromid).fetchone()[<span class="number">0</span>]</span><br><span class="line">        linkscores[toid]+=pr</span><br><span class="line">    maxscore=max(linkscores.values())</span><br><span class="line">    normalizedscores=dict( [(url,float(score)/maxscore <span class="keyword">for</span> (url,score) <span class="keyword">in</span> linkscores.items() ] )</span><br><span class="line">    <span class="keyword">return</span> normalizedscores</span><br></pre></td></tr></table></figure><p>如果一个页面，没有被其他页面描述过，或者用户的描述与‘引用过该目标页面’的网页的描述不相符，那么这种度量方法，会得到该页面的评分为0的结果。<br>为了能够利用链接文本改善搜索结果的排名，我们需要修改weights表。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1.0</span>,self.linktextscore(rows,wordids))</span><br></pre></td></tr></table></figure><p></p><p>这些度量之间，并不存在一组标准的权重，即使是大型的搜索引擎，也会时常改变他们对搜索结果的排名方法。<br>每种度量方法应该赋予多大的权重，取决于我们要创建的应用。</p><h2 id="利用神经网络学习数据之间的关联"><a href="#利用神经网络学习数据之间的关联" class="headerlink" title="利用神经网络学习数据之间的关联"></a>利用神经网络学习数据之间的关联</h2><p>在线应用的一个最大的优势在于，它会持续的获得用户行为信息，利用这些用户信息可以进一步改善用户体验。<br>对搜索引擎而言，每一个用户都可以即时提供他们对搜索结果的偏好。因为搜索结果很多，他们只点击了其中的一个链接，而没有点击其它的链接，这便是偏好。<br>本文将使用神经网络来记录用户查询结果的偏好，根据用户的偏好来调整搜索结果的排名。<br>神经网络由3个抽象层组成，下面依次介绍他们：</p><ol><li>输入层 用户输入的查询词。</li><li>隐藏层 负责对输入层的数据进行组合。该层无法直接与外界进行交互。</li><li>输出层 根据查询词得到的查询结果列表中每一个文档的评分。<br>为了训练神经网络，我们需要给神经网络提供反馈信息，即‘用户在搜索结果中点击了哪一个’。<br>神经网络经过许多不同查询的训练后，便可以根据用户的历史点击情况来改善搜索结果中的排序。</li></ol><h3 id="一个点击跟踪神经网络的设计"><a href="#一个点击跟踪神经网络的设计" class="headerlink" title="一个点击跟踪神经网络的设计"></a>一个点击跟踪神经网络的设计</h3><p>神经网络由神经元组成，每个神经元代表一个节点。这里我们使用的网络为<strong>感知机</strong>。<br>感知机由多层的神经元组成，包括输入层，它用来接收用户输入层查询词；中间层（也叫隐藏层），负责对输入进行权重组合（类似前面的计算人与人计算相似度时的权重组合一致）。输出层为针对输入的查询词得到的查询结果中每一个文档的评分。评分的计算方法是神经网络根据用户的历史点击信息学到的。<br>本节使用的网络结构为：<br><img src="/2017/12/02/SearchAndRanking/clicknetwork.jpg" title="点击跟踪神经网络示意图"><br>这里的网络是全连接网络，什么含义呢？ 下一层的每一个节点，和前面一层的所有节点都相连。比如hidden1与前一层中的word1、word2、word3都有一个连线；url1与前一层的hidden1、hidden2、hidden3都有连线。经过这样的描述之后，你应该能够自己手画一个全连接的网络。</p><blockquote><p>这里先只介绍全连接网络，后面会接触到非全连接的网络，比如卷积神经网络中的卷积层，就不是全连接的结构。</p></blockquote><p>询问神经网络根据查询词得到的查询结果中哪一个最好，我们需要将输入转换为神经网络输入层可以看得懂的形式，即将输入层中代表查询词的节点设置为1，其他的都设置为0。<br>输入层的数据会传入激活层，该操作会师徒激活隐藏层中的相应节点；然后隐藏层会试图激活输出层的节点。<br>输出层中的每个节点的激活程度，代表搜索结果中每一个文档的评分。评分的计算方法是神经网络从过去的数据中学习到的，这个学习的过程叫做“训练”。<br>举个例子:<br>搜索”world bank“，搜索引擎给出了3个搜索结果，分别是”World Bank“、”River“、”Earth“。我们用连线表示节点之间的关联程序，实线为强连接，虚线为弱连接。节点中粗体表示激活程度强。<br><img src="/2017/12/02/SearchAndRanking/network.jpg" title="举例"><br>当然最终的结果还要取决于被逐渐纠正的连接强度。只要有人进行搜索，并从搜索结果中点击某个连接，网络便会不断修正节点之间的连接强度，这个过程叫“训练”。<br>图中所示，许多人搜索“world bank”时，从搜索结果中选择了“World Bank“的文档，这种反馈信息会加强单词和文档之间的关联。那么训练采用的算法是啥？ 用的是反向传播算法，这回在后续的小节中进行讲解。</p><p>你可能会好奇为什么我们使用神经网络这样复杂的技术，而不是简单的记录下查询条件和相应查询条件的搜索结果中每个链接被用户点击的次数呢？因为神经网络的威力在于，对于它从未看到的搜索条件，它会根据以往相似的查询，给搜索结果中的每一个文档给与一个合理的预测的分值。</p><blockquote><p>目前十分火热的深度学习，其基础便是神经网络。击败李世石的AlphaGo便是深度学习与强化学习的混合产物。</p></blockquote><h3 id="设置数据库"><a href="#设置数据库" class="headerlink" title="设置数据库"></a>设置数据库</h3><p>当有用户进行查询的时候，神经网络便会被训练，故我们需要将反映网络现状的信息存储到数据库中。<br>数据库的“单词位置表”和“单词表”已经涉及了全部的神经网络的输入和输出。输入为单词，输出为根据单词找到的文档列表中的每一项的评分。<br>我们还需要一张代表隐藏层的数据表,给它起名字叫“hiddennode”表。还需要反映层之间的连接强度的表，分别是“输入层到隐藏层”表、“隐藏层到输出层”表。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> tanh</span><br><span class="line"><span class="keyword">import</span> sqlite3</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">searchnet</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,dbname)</span>:</span></span><br><span class="line">    self.con = sqlite3.connect(dbname)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__del__</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.con.close()</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">maketables</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.con.execute(<span class="string">'create table hiddennode(create_key)'</span>)</span><br><span class="line">    self.con.execute(<span class="string">'create table wordhidden(fromid,toid,strength)'</span>)</span><br><span class="line">    self.con.execute(<span class="string">'create table hiddenurl(fromid,toid,strength)'</span>)</span><br><span class="line">    self.con.commit()</span><br></pre></td></tr></table></figure><p></p><p>这些表并没有使用索引，当效率问题非常突出，我们可以再给他们添加上索引。<br>我们还需要写两个方法来访问和修改数据库中的参数。<br>第一个方法名为getstrengtth,用来判断连接的强度。由于新连接只有在必要时才会创建，因此该方法在连接不存在的时候会返回一个默认值。<br>对于单词层到隐藏层的连接，默认值为-0.2，所以在默认情况下，附加的单词将会对隐藏层的相应节点在激活程度上产生轻微的负面影响。<br>对于隐藏层到输出层的链接，默认值为 0 。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getstrength</span><span class="params">(self,fromid,toid,layer)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> layer == <span class="number">0</span>:</span><br><span class="line">    table = <span class="string">'wordhidden'</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    table = <span class="string">'hiddenurl'</span></span><br><span class="line">  res = self.con.execute(<span class="string">'select strength from %s where fromid=%d and toid=%d'</span> % (table,fromid,toid)).fetchone()</span><br><span class="line">  <span class="keyword">if</span> res = <span class="keyword">None</span>:</span><br><span class="line">    <span class="keyword">if</span> layer==<span class="number">0</span>：</span><br><span class="line">      <span class="keyword">return</span> <span class="number">-0.2</span></span><br><span class="line">    <span class="keyword">if</span> layer==<span class="number">1</span>:</span><br><span class="line">      <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  <span class="keyword">return</span> res[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p></p><blockquote><p><strong>问题，formid和toid的含义是什么，怎么确定？</strong><br>fromid为前一层的一个节点，toid为前面fromid的后一层的节点的id</p></blockquote><p>第二个方法名为setstrength方法，用来判断链接是否已经存在，并利用新的强度值更新链接或创建链接。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setstrength</span><span class="params">(self,fromid,toid,layer,strength)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> layer==<span class="number">0</span>:</span><br><span class="line">    table = <span class="string">'wordhidden'</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    table = <span class="string">'hiddenurl'</span></span><br><span class="line">  res = self.con.execute(<span class="string">'select rowid from %s where fromid=%d and toid=%d'</span> % (table,fromid,toid)).fetchone()</span><br><span class="line">  <span class="keyword">if</span> res==<span class="keyword">None</span>:<span class="comment">#如果连接不存在，创建连接</span></span><br><span class="line">    self.con.execute(<span class="string">'insert into %s (fromid,toid,strength) values(%d,%d,%f)'</span> % (table,fromid,toid,strength))</span><br><span class="line">  <span class="keyword">else</span>: <span class="comment">#链接存在，更新连接权重</span></span><br><span class="line">    rowid = res[<span class="number">0</span>]</span><br><span class="line">    self.con.execute(<span class="string">'upate %s set strength=%f where rowid=%d'</span>,(table,strength,rowid))</span><br></pre></td></tr></table></figure><p></p><p>大多数时候，当我们在构建神经网络时，网络中的节点都是预先建好的。我们可以预先建立一个隐藏层中有上千个节点，全部连接都已经建好的巨大网络。<br>不过在本例中，只在需要建立新的隐藏节点会更高效、也更简单。<br>那什么时候建立隐藏层节点，以及处理新加入节点与其他节点之间的连接呢？<br>每传入一组从前从未见过的单词组合，就应该在隐藏层中建立一个新的节点。随后，应该在单词层相关节点与该隐藏层节点之间建立连接，然后在该隐藏节点与所有的输出层之间建立连接。连接的权重采用默认值。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generatehiddennode</span><span class="params">(self,wordids,urls)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> len(wordids)&gt;<span class="number">3</span>:<span class="comment">#如果单词数大于3，就不创建节点</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">  <span class="comment"># 检查是否已经为这组单词创建好了一个节点</span></span><br><span class="line">  createkey=<span class="string">'_'</span>.join(sorted[str(wi) <span class="keyword">for</span> wi <span class="keyword">in</span> wordids])</span><br><span class="line">  res=self.con.execute(<span class="string">"select rowid from hiddennode where create_key='%s'"</span> % createkey).fetchone()</span><br><span class="line">  <span class="keyword">if</span> res == <span class="keyword">None</span>:<span class="comment"># 如果没有就建立</span></span><br><span class="line">    cur = self.con.execute(<span class="string">"insert into hiddennode (create_key) values ('%s')"</span> % createkey)</span><br><span class="line">    hiddenid = cur.lastrowdid</span><br><span class="line">    <span class="comment"># 设置默认权重</span></span><br><span class="line">    <span class="keyword">for</span> wordid <span class="keyword">in</span> wordids:<span class="comment">#  建立查询词与隐藏节点之间的连接</span></span><br><span class="line">      self.setstrength(wordid,hiddenid,<span class="number">0</span>,<span class="number">1.0</span>/len(wordids))<span class="comment"># setstrength格式为源节点、目标节点、层号、默认权重</span></span><br><span class="line">    <span class="keyword">for</span> urlid <span class="keyword">in</span> urls:</span><br><span class="line">      self.setstrength(hidden,urlid,<span class="number">1</span>,<span class="number">0.1</span>)<span class="comment"># setstrength格式为源节点、目标节点、层号、默认权重</span></span><br><span class="line">    self.con.commit()</span><br></pre></td></tr></table></figure><p></p><p>下面，我们测试一下上述代码，建立一个样例的查询与查询结果，尝试一下建立隐藏节点的过程：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">mynet = searchnet(<span class="string">'nn.db'</span>)</span><br><span class="line">mynet = maketables()</span><br><span class="line">wWorld,wRiver,wBank=<span class="number">101</span>,<span class="number">102</span>,<span class="number">103</span></span><br><span class="line">uWorldBank,uRiver,uEarch=<span class="number">201</span>,<span class="number">202</span>,<span class="number">203</span></span><br><span class="line">mynet.generatehiddennode([wWorld,wBank],[uWorldBank,uRiver,uEarch])</span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> mynet.con.execute(<span class="string">'select * from wordhidden'</span>):</span><br><span class="line">  print(c)</span><br><span class="line"><span class="comment">#(101,1,0.5) 格式为单词id、隐藏层的节点id、默认权重</span></span><br><span class="line"><span class="comment">#(103,1,0.5)</span></span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> mynet.con.execute(<span class="string">'select * from hiddenurl'</span>):</span><br><span class="line">  print(c)</span><br><span class="line"><span class="comment">#(1,201,0.1) 格式为隐藏层的节点id、输出层的节点id、默认权重</span></span><br><span class="line"><span class="comment">#(1,202,0.1)</span></span><br><span class="line"><span class="comment">#(1,203,0.1)</span></span><br></pre></td></tr></table></figure><p></p><p>上述执行过程在隐藏层建立了一个新的节点，还建立了一个指向该新建节点的带默认值的连接。<br>当”world bank“作为查询词时，就会执行上述的响应，建立隐藏层的节点、建立输入层与该隐藏层节点的连接、建立输出层与该隐藏层节点的连接。<br>有了连接和权重之后，我们可以计算神经网络的输出结果了，计算的方法叫<strong>前向传播</strong>。<br>但是由于这里都是采用的默认值，并不存在学习的过程，后面小节我们会讲如何利用<strong>反向传播</strong>来根据反馈信息来修正连接权重，这便是神经网络“学习“或者说是”训练”的过程。</p><h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><h4 id="激活函数的来由"><a href="#激活函数的来由" class="headerlink" title="激活函数的来由"></a>激活函数的来由</h4><p>接下来我们编写相关的函数，接受一组单词作为输入，激活神经网络中的相应节点，并用输出层的每个节点来代表每个文档的评价值。<br>怎样来模拟神经元的激活程度呢？<br>对于神经元来说只有激活和抑制两种状态，为了模拟神经元的激活状态，我们需要一个函数来描述节点的激活程度。<br>此处采用的是反双曲正切变换函数(hypoperbolic tangent,tanh),如下图所示：<br><img src="/2017/12/02/SearchAndRanking/tanh.jpg" title="tanh函数"><br>我们用文字来描述一下上述曲线，X代表及诶单的总输入，在X的近0点出函数的输出变化率最大。当输入大于等于2时，会发现输出几乎一直都是1，代表激活。同理，当输出小于等于-2时，该函数的输出几乎一致为-1，代表抑制。<br>由于tanh函数呈现一个S的形状，故该函数属于S型函数，神经网络会使用S型函数来量化神经元的激活程度。<br>除了tanh激活函数之外，还有sigmod函数（这是用来做二分类的激活函数）、softmax函数。这两种其实都是输出层一个节点的输出/输出层所有节点的输出总和 来代表一个神经元相对于其他神经元激活的比例。这两个函数也叫做<strong>指数家族函数</strong>。那为什么要用这两种激活函数？ 因为指数家族函数比较平滑，另外，可以解决所有节点输出都为0的情况，这时候用简单除法的情况就是0/0，会出现除数不能为零的错误；而指数函数不会出问题,它的每个节点除数为所有节点相加然后取倒数的结果。<strong>记得把公式在这里补上</strong></p><blockquote><p>最近流行的激活函数为ReLU，因为它能比较好的解决“梯度消失”问题，其激活函数的图形如下。有关“梯度消失“,读者可以自行搜索，也可以查看参考文献中的有关Alexnet的部分。</p></blockquote><h4 id="开始进行前向传播的计算"><a href="#开始进行前向传播的计算" class="headerlink" title="开始进行前向传播的计算"></a>开始进行前向传播的计算</h4><p>在开始执行前向传播算法之前，我们必须从数据库中查询出节点与连接的信息，然后在内存中建立起与该特定搜索项匹配的网络。<br>第一步我们应该从隐藏层中找出与本地查询相关的所有节点——在本例中，隐藏节点必须关联于查询条件中的某个单词，或者关联于查询结果中的某个文档。<br>由于其他节点不会被用来判断结果或训练网络，故不需要获取它们。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getallhiddenids</span><span class="params">(self,wordids,urlids)</span>:</span></span><br><span class="line">  hiddens=&#123;&#125;</span><br><span class="line">  <span class="keyword">for</span> wordid <span class="keyword">in</span> wordids:</span><br><span class="line">    cur = self.con.execute(<span class="string">'select toid from wordhidden where fromid=%d'</span> % wordid)</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> cur:</span><br><span class="line">      hiddens[row[<span class="number">0</span>]]=<span class="number">1</span></span><br><span class="line">  <span class="keyword">for</span> urlid <span class="keyword">in</span> urlids:</span><br><span class="line">    cur = self.con.execute(<span class="string">'select fromid from hiddenurl where toid=%d'</span> % urlid)</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> cur:</span><br><span class="line">      hiddens[row[<span class="number">0</span>]]=<span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> hiddens.keys()</span><br></pre></td></tr></table></figure><p></p><p>上述的描述，隐藏节点与查询查询和相关文档都关联的节点很常见，我们来描述一下比较特别的情况。</p><ol><li>情况1 隐藏节点只与查询单词连接<br>{asset_img wordhidden.png 隐藏节点只与查询单词连接}<br>这种时候，由于该查询词无对应的搜索结果，故隐藏节点只与查询单词节点连接</li><li>情况2 隐藏层与文档连接，但是不与查询词连接<br>{asset_img hiddenurl.png 隐藏节点只与搜索结果的文档连接}<br>这种时候，由查询词搜索得到的文档，由于非查询词也能搜索到相同的一份文档，故需要获取上图中绿色的隐藏节点。但由于非查询词没有输入，会被认为是0。查询词的相关节点会被视为1.<br>第二步，我们得到了隐藏层的相关节点之后，就应该建立相应的神经网络。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setupnetwork</span><span class="params">(self,wordids,urlids)</span>:</span></span><br><span class="line">  <span class="comment"># 节点列表</span></span><br><span class="line">  self.wordids = wordids</span><br><span class="line">  self.hiddenids = self.getallhiddenids(wordids,urlids)</span><br><span class="line">  self.urlids = urlids</span><br><span class="line">  <span class="comment"># 节点输出</span></span><br><span class="line">  self.ai = [<span class="number">1.0</span>]*len(self.wordids)</span><br><span class="line">  self.ah = [<span class="number">1.0</span>]*len(self.hiddenids)</span><br><span class="line">  self.ao = [<span class="number">1.0</span>]*len(self.urlids)</span><br><span class="line">  <span class="comment"># 建立权重矩阵</span></span><br><span class="line">  self.wi = [[self.getstrength(wordid,hiddenid,<span class="number">0</span>) \</span><br><span class="line">  <span class="keyword">for</span> hiddenid <span class="keyword">in</span> self.hiddenids]<span class="keyword">for</span> wordid <span class="keyword">in</span> self.wordids]</span><br><span class="line">  self.wo = [[self.getstrength(hiddenid,urlid,<span class="number">1</span>) \</span><br><span class="line">  <span class="keyword">for</span> urlid <span class="keyword">in</span> self.urlids] <span class="keyword">for</span> hiddenid <span class="keyword">in</span> self.hiddenids]</span><br></pre></td></tr></table></figure></li></ol><p>最后，我们来构造前向传播算法。算法接受一组输入，将其带入神经网络进行计算，得到一组输出。与查询词相关的输入层节点应该设置为1，其他输入层节点设置为0，由于0乘上任何权重都为0，故可以忽略。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feedforward</span><span class="params">(self)</span>:</span></span><br><span class="line">  <span class="comment"># 查询单词是仅有的输入，将该查询词节点输出设置为1</span></span><br><span class="line">  <span class="comment"># 非查询单词输入为0（乘上任何权重后还是0，故忽略）</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.worids)):</span><br><span class="line">    self.ai[i]=<span class="number">1.0</span></span><br><span class="line">  <span class="comment"># 计算隐藏层节点的活跃程度</span></span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> range(len(self.hiddenids)):</span><br><span class="line">    sum = <span class="number">0.0</span> </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.wordids)):</span><br><span class="line">      sum = sum + self.ai[i]*self.wi[i][j]</span><br><span class="line">      self.ah[j] = tanh(sum)</span><br><span class="line">  <span class="comment"># 计算输出层节点的活跃程度</span></span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> range(len(self.urlids)):</span><br><span class="line">    sum = <span class="number">0.0</span> </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.hiddenids)):</span><br><span class="line">      sum = sum + self.ah[i]*self.wo[i][j]</span><br><span class="line">      self.ao[j] = tanh(sum)</span><br><span class="line">  <span class="keyword">return</span> self.ao[:] <span class="comment"># 进行拷贝</span></span><br></pre></td></tr></table></figure><p></p><p>前向传播算法的执行过程是循环遍历所有位于隐藏层中的节点，对于每个隐藏层节点，将所有来自输入层的每个节点的输出乘上相应连接强度之后累加起来，经过激活函数处理后，即为该隐藏节点的激活程度。<br>输出层的每个节点的输出值同样是前一层所有节点的输出乘上相应的连接强度之后累加，经过激活函数处理，即为输出层的激活程度。<br>对于任意层的节点的输出值计算过程都是类似的，都是前一层所有节点的输出乘上相应的连接强度，然后经过激活函数处理后的值。我们可以很容易的将网络扩展，让它包含更多的层。<br>现在，我们可以编写简单的函数，建立神经网络，运行前向传播：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getresult</span><span class="params">(self,wordids,urlids)</span>:</span></span><br><span class="line">  self.setupnetwork(wordids,urlids)</span><br><span class="line">  <span class="keyword">return</span> self.feedforward()</span><br></pre></td></tr></table></figure><p></p><p>试验一下神经网络：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mynet=nn.searchnet(<span class="string">'nn.db'</span>)</span><br><span class="line">mynet.getresult([wWorld,wBank],[uWorldBank,uRiver,uEarch])</span><br><span class="line"><span class="comment"># [0.76,0.76，0.76]</span></span><br></pre></td></tr></table></figure><p></p><p>返回列表中的数字代表文档的相关性，这里代表uWorldBank、uRiver、uEarch相对搜索词wWorld、wBank的相关性。<br>因为网络还没有进行训练，所以神经网络给每个文档的评分结果是一致的。</p><blockquote><p>从这里我们可以看到，如果没有激活函数，神经网络做的全部都是线性计算，故激活函数还有另外一种解释： 激活函数的引入是为了提高神经网络的非线性表达能力。没有激活函数，神经网络只能做线性计算，为什么只能做线性计算？ 我们举个例子：<br><img src="/2017/12/02/SearchAndRanking/linecompute.png" title="线性计算的例子"><br>我们假设B1、B2、B3三个隐藏层节点的输出分别为b1、b2、b3。这3个节点对C的连接权重分别为w1,w2,w3。节点C的值可以表示为<br>$$<br>\begin {pmatrix}<br>b_1 &amp; b_2 &amp; b_3<br>\end {pmatrix} \times<br>\begin {pmatrix}<br>w_1 \\<br>w_2 \\<br>w_3<br>\end {pmatrix} = b_1 \times w_1+b_2 \times w_2+b_3 \times w_3<br>$$<br>你会发现这就是线性代数中的矩阵乘法运算，完全是线性计算。而激活函数很明显不是线性函数，加上激活函数之后，神经网络就有表达非线性模型的能力了。<br>激活函数还可以解释为收敛效果，神经元的输出经过激活函数之后，值域变小了。假如神经元一会输出一个比期望值大很多的数字，经过反向传播学习，权重改变许多后，又得到一个比期望值小很多的数字，这回导致神经网络的学习效果一直在“震荡”。</p></blockquote><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>有趣之处就在于此。神经网络会接受输入并给出输出，但是我们还没有告诉它一个好的结果是什么样的，因而其返回的结果是毫无价值的。<br>现在，我们将通过为神经提供某些人实际搜索的例子和相应的返回结果，以及用户决定点击的情况，对网络展开训练。<br>为此，我们需要一个算法，来修改介于两节点间连接的权重值，以便更好的反映人们告知网络的正确答案。由于无法假设每个用户都会点击一个适合所有人的答案，因此权重值需逐步加以调整。我们将要使用的算法被称为<strong>反向传播算法</strong>，因为该算法在调整权重值时是沿着网络反向进行的。<br>因为在对网络进行训练时，我们始终都知道每个输出层节点的期望输出，所以在这种情况下，如果用户在某个查询词下点击了某个结果，那么代表这个结果的节点在应该朝着1的方向推进，否则就向0推进。修改某个节点输出结果的唯一方法，就是修改针对该节点的总输入。<br>为了确定我们应该如何改变总的输入，训练算法需要知道tanh函数在其当前输出级别上的<strong>斜率</strong>（也叫导数）。<br>当斜率大于0时，输入值增大，输出就会增大；同理如果斜率小于0，输入值增大，输出就会越小。我们的目标是使得神经网络预测的期望值和用户给出的真实值之间的误差最小，利用斜率的性质，我们可以沿着斜率的反方向行走，当斜率大于0，我们将输入变小，如果斜率小于0，我们将输入变大。这种做法也叫做<strong>梯度下降法</strong>，它的原理是下山的时候，沿着梯度的方向行走，海拔降低的最快（最快的方式应该是跳崖，嘿嘿）。<br>我们在计算一下tanh函数的的斜率，当输出为0.0时，斜率非常抖，因此输入改变一点点，输出就会获得很大的变化。<br>如果输出结果接近-1或1，则改变输入对输出构成的影响就比较小，输出还是接近是-1或1。<br>我们给出该函数在任何输出值情况下的斜率计算函数:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dtanh</span><span class="params">(y)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> <span class="number">1.0</span> - y*y</span><br></pre></td></tr></table></figure><p></p><p>在执行反向传播算法之前，有必要运行一下feedforward函数，这样便可以得到神经网络的当前的计算值，也叫做当前神经网络的<strong>预测值</strong>。运行正向传播后，我们便得到了输出层每个节点的输出值。然后，我们按照一下的步骤进行反向传播算法。<br>对于输出层中的每个节点：</p><ol><li>计算节点当前输出结果与期望结果之间的差距</li><li>利用dtanh函数确定节点的总输入需要如何改变</li><li>改变每个外部回指连接的强度值，其值与连接的当前强度及学习速率成一定比例</li></ol><p>对于每个隐藏层的节点：</p><ol><li>将每个输出连接的强度值乘以其目标节点所需的改变量，然后累加求和，从而改变节点的输出结果</li><li>使用dtanh函数确定节点的总输入所需的改变量</li><li>改变每个输入连接的强度值，其值与连接的当前当杜及学习速率成一定比例<br>由于全部计算都依赖于第当前权重的理解，而非对更新后权重的了解，该算法的逻辑实际上是预先对所有误差进行计算，然后再对权重加以调整。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backPropagate</span><span class="params">(self,targets,N=<span class="number">0.5</span>)</span></span></span><br><span class="line"><span class="function">  # 计算所有的误差</span></span><br><span class="line"><span class="function">  #   计算输出层的误差</span></span><br><span class="line">  output_deltas = [0.0]*len(self.urlids)</span><br><span class="line">  <span class="keyword">for</span> k <span class="keyword">in</span> range(len(self.urlids)):</span><br><span class="line">    error = targets[k]-self.ao[k]</span><br><span class="line">    <span class="comment"># 函数为y=tanh(x),x为总输入，y为输出</span></span><br><span class="line">    <span class="comment"># 将函数变形，得到 dtanh(y)=x </span></span><br><span class="line">    <span class="comment"># 总输入乘上误差量，就是总输入需要的改变量</span></span><br><span class="line">    output_deltas[k]=dtanh(self.ao[k])*error</span><br><span class="line">  <span class="comment"># 计算隐藏层的误差</span></span><br><span class="line">  hidden_deltas = [<span class="number">0.0</span>]*len(self.hiddenids)</span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> range(len(self.hiddenids)):</span><br><span class="line">    error = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(len(self.urlids)):</span><br><span class="line">      <span class="comment"># 算累计误差的时候，乘上权重是因为复合函数的求导法则</span></span><br><span class="line">      error = error + output_deltas[k]*self.wo[j][k]</span><br><span class="line">    hidden_deltas = dtanh(self.ah[j])*error</span><br><span class="line">  <span class="comment"># 更新隐藏层到输出层的权重</span></span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> range(self.hiddenids):</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(self.urlids):</span><br><span class="line">    <span class="comment"># 对于输出层来说，其输入y = w*ah</span></span><br><span class="line">    <span class="comment"># 我们视w为自变量，其导数为ah</span></span><br><span class="line">      change = output_deltas[k]*self.ah[j]</span><br><span class="line">      self.wo[j][k]=self.wo[j][k]+N*change</span><br><span class="line">  <span class="comment"># 更新输入层到隐藏层的权重</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(self.wordids):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(self.hiddenids):</span><br><span class="line">      change = hidden_deltas[k]*self.ai[i]</span><br><span class="line">      self.wi[i][j]=self.wi[i][j]+N*change</span><br></pre></td></tr></table></figure></li></ol><p>有关其背后的算术的详解：可以参照<br><a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" target="_blank" rel="noopener">手把手教你反向传播的例子</a>。<br>现在，我们所要做的全部工作就是编写一个简单的方法，建立神经网络，运行前向传播和反向传播算法。<br>该方法接受wordids列表、urlids列表，以及一个URL（代表用户的点击的文档）作为参数。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainquey</span><span class="params">(self,wordids,urlids,selectedurl)</span>:</span></span><br><span class="line">  <span class="comment">#如果有必要，生成隐藏节点</span></span><br><span class="line">  self.generatehiddennode(wordids,urlids)</span><br><span class="line">  self.setupnetwork(wordids,urlids)</span><br><span class="line">  self.feedforward()</span><br><span class="line">  targets=[<span class="number">0.0</span>]*len(urlids)</span><br><span class="line">  targets[urlids.index(selectedurl)]=<span class="number">1.0</span></span><br><span class="line">  self.backPropagate(targets)</span><br><span class="line">  self.updatedatabase()</span><br></pre></td></tr></table></figure><p></p><p>为了将结果保存起来，我们还需要一个方法来更新数据库中的权重值，权重信息位于实例变量wi和wo中。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updatedatabse</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="comment"># 将值存在数据库中</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.wordids)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(self.hiddedinds)):</span><br><span class="line">      self.setstrength(self.worids[i],self.hiddenids[j],<span class="number">0</span>,self.wi[i][j])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> range(len(self.hiddedinds)):</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(len(self.urlids)):</span><br><span class="line">      self.setstrength(self.hiddenids[j],self.urlids[k],<span class="number">0</span>,self.wi[j][k])</span><br></pre></td></tr></table></figure><p></p><p>现在，我们进行一次简单的训练：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mynet = searchnet(<span class="string">'nn.db'</span>)</span><br><span class="line">mynet.trainquery([wWorld,wBank],[uWorldBank,uRiver,uEarth],uWorldBank)</span><br><span class="line">mynet.getresult([wWorld,wBandk],[uWorldBank,uRiver,uEarth])</span><br><span class="line"><span class="comment">#输出为 [0.335,0.055,0.055]</span></span><br></pre></td></tr></table></figure><p></p><p>神经网络接收了一次训练之后，uWorldBank的评分明显比其他两个文档的评分要高。如果有很多次这样的输入，神经网络对uWorldBank文档的评分和对其他文档的给出的评分，两种评分的差距会越大。</p><h3 id="训练实验"><a href="#训练实验" class="headerlink" title="训练实验"></a>训练实验</h3><p>至此我们已经看到了，利用某个样例结果对网络加以训练，可以增加针对该结果导数输出。<br>尽管这是有价值的，但是它并没有展示出神经网络的真正威力——即，对以前未曾见过的输入情况进行合理推测。<br>我们现在做如下的尝试:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">allurls=[uWorldBank,uRiver,uEarth]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">30</span>):</span><br><span class="line">  mynet.trainquery([wWorld,wBank],allurls,uWorldBank)</span><br><span class="line">  mynet.trainquery([wRiver,wBank],allurls,uRiver)</span><br><span class="line">  mynet.trainquery([wWorld],allurls,uEarth)</span><br><span class="line">mynet.getresult([wWorld,wBank],allurls)</span><br><span class="line"><span class="comment">#输出 [0.861,0.011,0.016]</span></span><br><span class="line">mynet.getresult([wRiver,wBank],allurls)</span><br><span class="line"><span class="comment">#输出 [-0.030,0.883,0.006]</span></span><br><span class="line"><span class="comment"># 下面输入一个神经网络从未见到的情况</span></span><br><span class="line">mynet.getresult([wBank],allurls)</span><br><span class="line"><span class="comment">#输出 [0.865,0.001,-0.85]</span></span><br></pre></td></tr></table></figure><p></p><p>尽管神经网络从未见过有关“bank”的查询，但是它给出了一个合理的猜测。<br>bank（此时代表岸边的意思）除了经常与River（河）之间存在联系外，同时bank（此时代表银行）也经常与WorldBank（世界银行）之间存在联系，但神经网络对WorldBank文档的平均依然比River文档的评价更高。<br>神经网络不仅掌握了文档和查询词之间的联系，还了解到一次特点的查询中，<strong>哪些是重要词</strong>——简单的关联一下查询词和文档之间的联系是做不到的。</p><h3 id="与搜索引擎结合"><a href="#与搜索引擎结合" class="headerlink" title="与搜索引擎结合"></a>与搜索引擎结合</h3><p>先来一点作假的训练方法，我们的searcher类的query函数接收查询词并返回查询结果，我们可以将这些查询词和查询结果送入神经网络。<br>首选，我们改写searcher的query函数，让其可以返回查询词和根据查询词得到的文档列表。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">query</span>:</span></span><br><span class="line">    <span class="comment">#... 前面的省略</span></span><br><span class="line">    <span class="keyword">return</span> wordids,[r[<span class="number">1</span>] <span class="keyword">for</span> r <span class="keyword">in</span> rankedscores[<span class="number">0</span>:<span class="number">10</span>]]<span class="comment">#r[0]为评分,r[1]为文档的URL ID</span></span><br></pre></td></tr></table></figure><p></p><p>有了输出和输出之后，我们应该在获取到用户的点击信息作为反馈，神经网络才可以被训练。<br>搜集用户对结果的点击情况的信息的具体方法取决于我们对应用程序的设计。我们可以在网页中写JavaScript脚本，截获用户的点击行为，在浏览器调整到用户点击页面之前调用trainquery方法。<br>我们也可以让用户对网页的相关性进行投票的方法，获得反馈。<br>最后，我们应该把神经网络的评分也考虑进去，其做法与前面的评分函数类似。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mynet = nn.searchnet(<span class="string">'nn.db'</span>)</span><br><span class="line"><span class="comment"># 写出神经网络的评分函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nnscore</span><span class="params">(self,rows,wordids)</span>:</span> <span class="comment">#rows每一项格式为:url 第一个单词的位置 第二个单词的位置...</span></span><br><span class="line">  <span class="comment"># 用set去除重复的URL </span></span><br><span class="line">  urlids = [urlid <span class="keyword">for</span> urlid <span class="keyword">in</span> set([row[<span class="number">0</span>] <span class="keyword">for</span> row <span class="keyword">in</span> rows])]</span><br><span class="line">  nnres = mynet.getresult(wordids,urlids)</span><br><span class="line">  scores= dict([(urlids[i],nnres[i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(urlids))])</span><br><span class="line">  <span class="keyword">return</span> self.normalizescores(scores)</span><br></pre></td></tr></table></figure><p></p><p>我们可以将上述函数的评分给予一个权重，放到weights列表中进行试验。<br>事实上，更值得推荐的做法是，等网络经过大量不同样例的训练之后，再将其作为评价总分的一部分纳入到weights列表中。<br>本章介绍了开发一个搜索引擎所需要的许多知识，不够相比实际情况这依然是非常有限的，本章的习题将涉及某些进阶议题。<br>这里，我们并没有考虑性能问题——这可能我们要对数以百万的网页建立索引，不过本文所建立的搜索引擎，对于建立十万级别的网页，还是绰绰有余的，这对新闻网站或者公司内部站点来说，已经足够了。</p><h1 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h1><ol><li><strong>分词</strong> seperatewords 方法目前将任何非字母和非数字的字符都当做了分隔符，这意味着无法为“c++”、“$20”、“Ph.D”、或“617-555-1212“这样的词建立正确的索引。更好的分词方法是什么呢？使用空白的分隔符是否可以？对于中文呢？中文可不是依靠空格来进行分词的语言。请编写一个更好的分词。<br>对于中文分词，有个pyhton库，叫”Jieba“</li><li><strong>布尔操作符</strong> 许多搜索引擎都支持布尔查询，它允许用户构造诸如”python OR perl“这样的搜索条件。一个OR查询可以通过分别执行两次查询后，再对结果进行组合的方式来实现，但是对于”python AND (program OR code)“又该怎样处理呢？ 请修改查询方法以支持某些布尔操作。<br>略</li><li><strong>精确匹配</strong> 搜索引擎通常都支持”精确匹配“查询：网页中用双引号括起来的词，其顺序必须与查询条件中的单词顺序相同，而且中间不允许夹杂任何其他的单词。请编写一个新的getrows函数，只返回精确匹配的结果。（提示，你可以使用SQL中的减法运算，得到两个单词间的位置之差）。<br>略</li><li><strong>长文/短文检索</strong> 有时，用户可能会对查找有关疑难问题的长篇文章感兴趣，有时可能会对命令行工具的短篇文章感兴趣，请编写一个权重函数，该函数根据用户传入的参数，倾向于搜索出比较长或者比较短的文档。<br>略</li><li><strong>单词频度偏好</strong> ”单词计数“的度量方法对较长的文档有利，因为篇幅长的文档会拥有更多的单词，也更可能包含要搜索的单词。请编写一个新的度量方法，以文档中单词数量的百分比作为频度进行计算。<br>略</li><li><strong>外部回指链接搜索</strong> 目前的代码可以根据外部回指链接的文本对返回结果进行排序，但我们必须先利用基于内容的算法找到这些网页。而有时，相关度很大的网页并不包含任何查询文本；相反，有很多指向该网页的链接，这些指向该文档的链接描述符合搜索中的查询词——举个例子：指向图片的链接，图片内容不包含文字，而img标签的alt会有该图片的描述，不看这个信息，就无法从文字上了解图片内容是什么。请修改搜索代码，令其搜索结果中也包含哪些外部回指链接的描述也符合查询词的情况。<br>略</li><li><p><strong>不同的训练选项</strong> 对神经网络进行训练时，我们使用了一组0值代表用户没有点击的URL，用1代表用户点击的URL。请修改训练函数，令其能够允许用户对结果给予一个从1到5的评价。</p></li><li><p><strong>附加层</strong> 目前的神经网络只有一个隐藏层。请修改searchnet类，令其可以支持任意数量的隐藏层，关于层数，我们可以在初始化的时候加以指定。</p></li></ol><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" target="_blank" rel="noopener">手把手教你反向传播的例子</a></li><li>论文样本不均衡<br>假设你现在在做2分类，训练集中正负样本的比例为999：1，当用这样的数据集训练神经网络的时候，神经网络总是倾向于输出正，因为这样的误差最小，这是由于神经网络的原理造成的，使得期望值也神经网络的预测值相差最小，既然正样本那么多，那么一直输出正，便能得到99.9%的正确率。这种情况叫<strong>样本不均衡性</strong>。比较简单的解决方法是调整样本比例。<br>想象一种极端情况，训练集中全是正样本，这时候神经网络会一直输出为正，这种情况下，根本没有梯度来供给反向传播使用的。<br>再看一个特殊情况，假设同样是做2分类，假设样本是均衡的，为1：1，我们先将正样本全部输入神经网络进行训练，然后再输入负样本进行训练，这时候的学习效果也是非常差的，因为一开始输入全是正样本，神经网络趋向于不论输入什么，全部输出为正；后面全部输入负样本，神经网络会倾向于全部输出为负，来减少误差。这种情况，需要将<strong>数据打散</strong>（shuffle-洗牌）。</li><li>论文-alexnet<br>这篇论文可以搜索在百度学术或者google学术上搜索“ImageNet classification with deep convolutional neural networks” 来得到。<br>这篇论文使得深度学习受到广泛关注，原因是因为Alex作为学生，使用卷积神经网络的深度学习模型击败了业界神话Google（Google拥有海量的数据集、强悍的硬件集群）。<br>alexnet 这篇论文使用了两种方法来提示卷积神经网络的准确率：<ol><li>使用数据扩充<br>它的意思对训练样本进行随机的平移、旋转、放缩等处理，然后将这些数据加入到训练集中，起到<strong>扩充训练集</strong>的效果。</li><li>使用ReLU作为激活函数<br>使用ReLU主要是与之前使用sigmod激活函数做区别，sigmod做激活函数时，其导数的值域为[0,0.25],反向传播的梯度每经过一层神经网络，梯度就会变成原来的1/4甚至更小，导致梯度无法传到前面的层，即前面的层学习的参数完全是错的，而后面的参数又依赖前面的参数，后面的神经网络层学的再好也没用，这叫<strong>梯度消失问题</strong>；而ReLU导数在激活状态下一直为1，其他时候为0。 相对sigmod，ReLU可以很好的缓解梯度消失问题。</li></ol></li><li>论文手写字体识别<br>CNN是LeCun发明的，这篇论文是LeCun写的关于如何用卷积神经网络模型来识别0-9共10个数字，最初用于识别支票上的银行卡号。作者训练了一个分类器，可以对手写的0-9数字进行分类，然后使用了一个滑动窗口，通过不断的滑动窗口，对图片的不同部分进行分类，便可以识别图片上的所有手写的数字啦，由于滑动的窗口之间可能存在重叠，对于重叠的部分，一般会使用非极大值抑制(NMS)的方法只取出分值最大的结果。</li><li>为什么神经网络的参数不能全部初始化为0，如果全部初始化为1呢？<br>可以在<a href="playground.tensorflow.org">playground tensorflow</a>试着玩一下，将所有的连接的权重全部改为1。<img src="/2017/12/02/SearchAndRanking/allone.jpg" title="所有的权重全部初始化为1的学习效果"> 从图中我们看到神经网络学习的效果：x1与它下一层中的神经元之间的权重都是相同的值;x2与它下一层中的神经元之间的权重都是相同的值。<br>什么原因呢？ 由于权重都都初始化为一样的，当他们反向传播的时候，同一个输入节点的不同条链路上的梯度都是一样的（这点可以根据符合函数的链式求导法则得知)，他们的权重变化量也会一致，这回导致很多神经元效果都是一样的，学不到不同的斜率，效果会大打折扣的，故一般使用的随机数的方式来初始化权重。</li><li>小技巧<br><strong>混淆学习</strong> 有时候在训练多分类器的时候，其中有两个类很相似，在多类学习的时候很难将这两个类分开，这时候可以直接将这个两个当做一个类先训练着，然后对这两个非常容易混淆的类，再单独训练一个神经网络做2分类。</li></ol></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>如果您觉得读完本文有收获，不妨小额赞助我一下，让我有动力继续写出高质量的教程！</div><button id="rewardButton" disable="enable"><span>打赏</span></button><div id="QR" style="display:block"><div id="wechat" style="display:inline-block"><img id="wechat_qr" src="/images/smacker.jpg" alt="倔强的土豆 微信支付"><p>微信支付</p></div></div></div></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2017/11/12/cluster/" rel="next" title="发现群组"><i class="fa fa-chevron-left"></i> 发现群组</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2018/01/07/optimization/" rel="prev" title="优化">优化 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div><div class="comments" id="comments"><div id="gitment-container"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div id="sidebar-dimmer"></div><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">倔强的土豆</p><p class="site-description motion-element" itemprop="description">分享机器学习、深度学习的点滴</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">9</span> <span class="site-state-item-name">日志</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/laiqun" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:laiqun@msn.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#搜索引擎的定义与组成"><span class="nav-number">1.</span> <span class="nav-text">搜索引擎的定义与组成</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#建立搜索引擎的步骤"><span class="nav-number">2.</span> <span class="nav-text">建立搜索引擎的步骤</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据处理"><span class="nav-number">2.1.</span> <span class="nav-text">数据处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据检索"><span class="nav-number">2.2.</span> <span class="nav-text">数据检索</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#抓取数据"><span class="nav-number">3.</span> <span class="nav-text">抓取数据</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#使用urllib来下载网页"><span class="nav-number">3.1.</span> <span class="nav-text">使用urllib来下载网页</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#书写爬虫"><span class="nav-number">3.2.</span> <span class="nav-text">书写爬虫</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#建立索引"><span class="nav-number">4.</span> <span class="nav-text">建立索引</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#设计数据库中的数据表"><span class="nav-number">4.1.</span> <span class="nav-text">设计数据库中的数据表</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#在网页中查找单词"><span class="nav-number">4.2.</span> <span class="nav-text">在网页中查找单词</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#加入索引"><span class="nav-number">4.3.</span> <span class="nav-text">加入索引</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#将普通文本加入索引"><span class="nav-number">4.3.1.</span> <span class="nav-text">将普通文本加入索引</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#将链接处的文本加入索引"><span class="nav-number">4.3.2.</span> <span class="nav-text">将链接处的文本加入索引</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#测试加入索引功能"><span class="nav-number">4.3.3.</span> <span class="nav-text">测试加入索引功能</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#查询"><span class="nav-number">5.</span> <span class="nav-text">查询</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#基于内容的排名"><span class="nav-number">5.1.</span> <span class="nav-text">基于内容的排名</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#归一化的方法"><span class="nav-number">5.1.1.</span> <span class="nav-text">归一化的方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#单词频度"><span class="nav-number">5.1.2.</span> <span class="nav-text">单词频度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#单词位置"><span class="nav-number">5.1.3.</span> <span class="nav-text">单词位置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#单词距离"><span class="nav-number">5.1.4.</span> <span class="nav-text">单词距离</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基于外部回指链接的排名"><span class="nav-number">5.2.</span> <span class="nav-text">基于外部回指链接的排名</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#数量上–简单计数"><span class="nav-number">5.2.1.</span> <span class="nav-text">数量上–简单计数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#质量上–pagerank算法"><span class="nav-number">5.2.2.</span> <span class="nav-text">质量上–pagerank算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#利用衔接文本信息"><span class="nav-number">5.2.3.</span> <span class="nav-text">利用衔接文本信息</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#利用神经网络学习数据之间的关联"><span class="nav-number">5.3.</span> <span class="nav-text">利用神经网络学习数据之间的关联</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#一个点击跟踪神经网络的设计"><span class="nav-number">5.3.1.</span> <span class="nav-text">一个点击跟踪神经网络的设计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#设置数据库"><span class="nav-number">5.3.2.</span> <span class="nav-text">设置数据库</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#前向传播"><span class="nav-number">5.3.3.</span> <span class="nav-text">前向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#激活函数的来由"><span class="nav-number">5.3.3.1.</span> <span class="nav-text">激活函数的来由</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#开始进行前向传播的计算"><span class="nav-number">5.3.3.2.</span> <span class="nav-text">开始进行前向传播的计算</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#反向传播"><span class="nav-number">5.3.4.</span> <span class="nav-text">反向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#训练实验"><span class="nav-number">5.3.5.</span> <span class="nav-text">训练实验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#与搜索引擎结合"><span class="nav-number">5.3.6.</span> <span class="nav-text">与搜索引擎结合</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#练习"><span class="nav-number">6.</span> <span class="nav-text">练习</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考"><span class="nav-number">7.</span> <span class="nav-text">参考</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">倔强的土豆</span></div><div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div><span class="post-meta-divider">|</span><div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.3</div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script><link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css"><script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script><script type="text/javascript">function renderGitment(){var a=new Gitmint({id:window.location.pathname,owner:"laiqun",repo:"laiqun.github.io",lang:navigator.language||navigator.systemLanguage||navigator.userLanguage,oauth:{client_secret:"55aaeb736714431ea52109dd66461b1644ca6177",client_id:"c90dfa80285ea91b9120"}});a.render("gitment-container")}renderGitment()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html><!-- rebuild by neat -->