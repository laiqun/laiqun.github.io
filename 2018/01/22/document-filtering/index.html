<!-- build time:Wed Jan 24 2018 22:54:59 GMT+0800 (CST) --><!DOCTYPE html><html class="theme-next mist" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3"><link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222"><meta name="keywords" content="Hexo, NexT"><meta name="description" content="文档过滤本章将向大家演示如何依据内容来对文档进行分类。文档分类是机器智能(machine Intelligence)的一个应用，很有实用价值,而且现在越来越普及。关于文档过滤，最有价值也最为人们所熟知的应用，恐怕要数垃圾邮件过滤了。随着电子邮件的广泛普及与邮件发送的超低成本，人们面临的一大问题是任何人的邮件地址只要落入"><meta property="og:type" content="article"><meta property="og:title" content="文档过滤"><meta property="og:url" content="laiqun.github.io/2018/01/22/document-filtering/index.html"><meta property="og:site_name" content="广阔天地，大有作为"><meta property="og:description" content="文档过滤本章将向大家演示如何依据内容来对文档进行分类。文档分类是机器智能(machine Intelligence)的一个应用，很有实用价值,而且现在越来越普及。关于文档过滤，最有价值也最为人们所熟知的应用，恐怕要数垃圾邮件过滤了。随着电子邮件的广泛普及与邮件发送的超低成本，人们面临的一大问题是任何人的邮件地址只要落入不法者之手，便有可能会收到未经许可的商业邮件，致使我们无法阅读到真正感兴趣的邮件"><meta property="og:locale" content="zh-Hans"><meta property="og:updated_time" content="2018-01-24T14:54:27.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="文档过滤"><meta name="twitter:description" content="文档过滤本章将向大家演示如何依据内容来对文档进行分类。文档分类是机器智能(machine Intelligence)的一个应用，很有实用价值,而且现在越来越普及。关于文档过滤，最有价值也最为人们所熟知的应用，恐怕要数垃圾邮件过滤了。随着电子邮件的广泛普及与邮件发送的超低成本，人们面临的一大问题是任何人的邮件地址只要落入不法者之手，便有可能会收到未经许可的商业邮件，致使我们无法阅读到真正感兴趣的邮件"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",version:"5.1.3",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!0},fancybox:!0,tabs:!0,motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="laiqun.github.io/2018/01/22/document-filtering/"><title>文档过滤 | 广阔天地，大有作为</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">广阔天地，大有作为</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">你看到我的筋斗云了嘛？</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="laiqun.github.io/2018/01/22/document-filtering/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="倔强的土豆"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="广阔天地，大有作为"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">文档过滤</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-22T20:26:20+08:00">2018-01-22 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/01/22/document-filtering/#comments" itemprop="discussionUrl"><span class="post-comments-count gitment-comments-count" data-xid="/2018/01/22/document-filtering/" itemprop="commentsCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="文档过滤"><a href="#文档过滤" class="headerlink" title="文档过滤"></a>文档过滤</h1><p>本章将向大家演示如何依据内容来对文档进行分类。文档分类是机器智能(machine Intelligence)的一个应用，很有实用价值,而且现在越来越普及。关于文档过滤，最有价值也最为人们所熟知的应用，恐怕要数垃圾邮件过滤了。随着电子邮件的广泛普及与邮件发送的超低成本，人们面临的一大问题是任何人的邮件地址只要落入不法者之手，便有可能会收到未经许可的商业邮件，致使我们无法阅读到真正感兴趣的邮件。</p><p>当然，垃圾信息的问题并非仅限于电子邮件。随着时间的推移，Web网站已经越来越具有互动的特征,它们或向用户征求意见，或请求用户提供原创内容，这些行为都会伴以垃圾信息侵扰的问题。例如像Yahoo!Groups和Usenet这样的公共留言板，就长期遭受着垃圾帖的侵扰。这些帖子或与留言板主题毫不相干，或者就是以兜售可疑产品为目的。现在，博客和维基也遭遇到了同样的问题。每当我们在构建一个允许普通大众一起参与的应用系统时，就始终应该考虑应对垃圾信息的策略。</p><p>本章中介绍的算法不是专门针对垃圾信息的。由于这些算法可以解决更为一般性的问题，即学习井鉴别文档所属的分类，因此我们还可以将其应用于一些相比垃圾信息而言不那么令人生厌的问题。一种可能的应用是，根据邮件正文自动将收件箱中的邮件划分为社交类邮件或工作相关类邮件。还有一种可能的应用是，识别出要求回复的邮件，并将其自动转发给最适合的人员进行处理。本章的最后一个例子，会为大家示范如何将来自某一RSS订阅源的内容项自动过滤到不同的分类之中。</p><h1 id="过滤垃圾信息"><a href="#过滤垃圾信息" class="headerlink" title="过滤垃圾信息"></a>过滤垃圾信息</h1><p>早期尝试对垃圾信息进行过滤所用的都是基于规则的分类器(rule-based classitiem)，使用时会有人事先设计好一组规则，用以指明某条信息是否属干垃圾信息。典型的规则包括英文大写字母的过度使用，与医学药品相关的单词，或是过干花哨的HTML用色等。<br>基于规则的分类器，其间题很快就显现了出来:垃圾信息制造者在知道了所有规则以后，为了绕开过滤器，其行为就会变得更加隐蔽，而且人们会发现，如果他们的父母不知道关闭大写锁定键(Caps Lock)，一些正常的邮件也会被归类成垃圾邮件。<br>基于规则的过滤器还有另一个问题：是否被当做垃圾信息很大程度上因其所面对的读者和张贴位置的不同而不同。对于某一位特定用户、公告留言板或者维基百科而言，那些可以用来明确指示是否是垃圾信息的关键词，在其他场合下可能就会变得相当正常。为了解决这一问题，本章所要考查的程序会在开始阶段和逐渐收到更多消息之后，根据人们提供给它的有关哪些是垃圾邮件，哪些不是垃圾邮件的信息，不断地进行学习。通过这样的方式，我可以分别为不同的用户、群组或网站建立起各自的应用实例和数据集·它们对垃圾信息的界定将逐步形成自己的观点。</p><h1 id="文档和单词"><a href="#文档和单词" class="headerlink" title="文档和单词"></a>文档和单词</h1><p>即将构造的分类器需要利用某些特征来对不同的内容项进行分类。所谓特征，是指任何可以用来判断内容中具备或缺失的东西。当考虑对文档进行分类时，所谓的内容即是文档，而特征则是文档中的单词。将单词作为特征时，其假设是某些单词相对而言更有可能会出现于垃圾信息中。这一假设是大多数垃圾信息过滤器背后所依赖的基本前提。不过，特征未必是一个个单词，它们也可以是词组或短语，或者任何可以归为文档中缺失或存在的其他东西。</p><p>请新建一个文件，取名docclass.py，并在其中加人一个名为getwords的函数，用来从文本中提取特征:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getwords</span><span class="params">(doc)</span>:</span></span><br><span class="line">    splitter=re.compile (<span class="string">'\\W*'</span>)</span><br><span class="line">    <span class="comment">#根据非字母字符进行单词拆分</span></span><br><span class="line">    words=[s.lower() <span class="keyword">for</span> s <span class="keyword">in</span> splitter.split(doc) <span class="keyword">if</span> len(s)&gt;<span class="number">2</span> <span class="keyword">and</span> len(s)&lt;<span class="number">20</span>]</span><br><span class="line">    <span class="comment">#只返回一组不重复的单词</span></span><br><span class="line">    <span class="keyword">return</span> dict ( [ (w,<span class="number">1</span>) <span class="keyword">for</span> w <span class="keyword">in</span> words] )</span><br></pre></td></tr></table></figure><p></p><p>该函数以任何非字母类字符为分隔符对文本进行划分，将文本拆分成了一个个单词。这过程只留下了真正的单词，并将这些单词全都转换成了小写形式。</p><p>决定采用哪些特征颇具技巧性，也十分重要。特征必须具备足够的普遍性，即时常出现，但又不能普遍到每一篇文档里都能找到。理论上，整篇文档的文本都可以作为特征，但是除非我们一再收到内容完全相同的邮件，否则这样的特征几乎肯定是毫无价值的。在另一种极端情况下，特征也可以是单个字符。但是由于每一封电子邮件中都有可能会出现所有这些字符，因此要想利用这样的特征将希望看到和不希望看到的文档区分开来是很困难的。即便选择使用单词作为特征，也依然还是会带来一些问题，包括如何正确划分单词，哪些标点符号应该被纳入单词，以及是否应该包含头信息(header information)等。</p><p>在根据特征进行判断时还有一点需要考虑，那就是如何才能更好地利用特征将一组文档划归到目标分类中去。例如，前述getwords函数的代码通过将单词转换为小写形式，从而减少了特征的总数。这意味着，程序会将位于句首以大写字母开头的单词与位于句中全小写形式的单词视为相同——这样做非常好，因为具有不同大小写形式的同一单词往往代表的含义是相同的。然而，上述函数完全没有考虑到被用于许多垃圾信息中的“SHOUTING风格”（指许多垃圾邮件中存在大写单词使用过度的情况），而这一点可能对区分垃圾邮件和非垃圾邮件是至关重要的。除此以外，如果超过半数以上的单词都是大写时，那就说明必定会有其他的特征存在。</p><p>正如你所看到的，在选择特征集时需要做大量的权衡，而且还要不断地进行调整。不过眼下，可以暂且使用这个简单的getwords函数，在本章的后续部分，我们还将了解到有关特征提取的一些改进方法。</p><h1 id="对分类器进行训练"><a href="#对分类器进行训练" class="headerlink" title="对分类器进行训练"></a>对分类器进行训练</h1><p>本章中讨论的分类器可以通过接受训练的方式来学习如何对文档进行分类。本书中的许多其他算法，例如我们在搜索引擎那一章见到过的神经网络，都是通过读取正确答案的样本进行学习的。如果分类器掌握的文档及其正确分类的样本越多，其预测的效果也就越好。人们专门设计分类器，其目的也就在于此，即：从极为不确定的状态开始，随着分类器不断了解到哪些特征对于分类而言更为重要，其确定性也在逐渐地增加。</p><p>我们要做的第一件事情，是编写一个代表分类器的类。这个类将对分类器到目前为止所掌握的信息进行封装。以这样的方式构造Python模块的好处在于，我们可以针对不同的用户、群组或査询，建立起多个分类器实例，并分别对它们加以训练，以响应特定群组的需求。<br>请在docclass.py中新建一个名为classifier的类:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">classifier</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, getfeatures,filename=None)</span></span></span><br><span class="line"><span class="function">    #择统计特征/分类组合的数量</span></span><br><span class="line">    self.fc=&#123;&#125;</span><br><span class="line">    <span class="comment">#统计每个分类中的文档数量</span></span><br><span class="line">    self.cc=&#123;)</span><br><span class="line">    self.getfeatures=getfeatures</span><br></pre></td></tr></table></figure><p></p><p>该类中有3个实例变量，它们分别是fc、cc和getfeatures。变量fc将记录不同特征在不同类别中的计数。例如：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'python'</span>: &#123;<span class="string">'bad'</span>: <span class="number">0</span>, <span class="string">'good'</span>: <span class="number">6</span>&#125;, <span class="string">'the'</span>: &#123;<span class="string">'bad'</span>: <span class="number">3</span>, <span class="string">'good'</span>: <span class="number">3</span>&#125;&#125;</span><br></pre></td></tr></table></figure><p></p><p>上述示例表明，单词‘the’被划归为”bad“类3次，被划为good类也是3次。而单词”python”却只在good类中出现过。<br>变量cc记录各个分类出现的次数的字典。这一信息是我们稍后即将讨论的概率计算所需的。最后一个实例变量，getfeatures，对应一个函数，其作用是从即将被归类的内容项中提取特征，在本里中，即为前面定义的getwords函数。<br>我们新加入以下几个函数，以实现计数值的增加和获取。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加某个分类的总数量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">incc</span><span class="params">(self,cat)</span>:</span></span><br><span class="line">  self.cc.setdefault(cat,<span class="number">0</span>)</span><br><span class="line">  self.cc[cat]+=<span class="number">1</span></span><br><span class="line"><span class="comment"># 属于某一分类的总数量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">catcount</span><span class="params">(self,cat)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> cat <span class="keyword">in</span> self.cc:</span><br><span class="line">    <span class="keyword">return</span> float(self.cc[cat])</span><br><span class="line"> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"><span class="comment"># 所有分类的总数量相加</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">totalcount</span><span class="params">(self)</span>:</span></span><br><span class="line"> <span class="keyword">return</span> sum(self.cc.values())</span><br><span class="line"><span class="comment"># 有几个分类，获取分类名列表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">categories</span><span class="params">(self)</span>:</span></span><br><span class="line"> <span class="keyword">return</span> self.cc.keys()</span><br><span class="line"><span class="comment"># 增加对特征/分类的键值对的计数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">incf</span><span class="params">(self,f,cat)</span>:</span></span><br><span class="line">  self.fc.setdefault(f,&#123;&#125;)</span><br><span class="line">  self.fc[f].setdefault(cat,<span class="number">0</span>)<span class="comment">#键不存在时，设置默认值</span></span><br><span class="line">  self.fc[f][cat]+=<span class="number">1</span></span><br><span class="line"><span class="comment"># 某一特征出现在某一分类中的次数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fcount</span><span class="params">(self,f,cat)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> f <span class="keyword">in</span> self.fc <span class="keyword">and</span> cat <span class="keyword">in</span> self.fc[f]:</span><br><span class="line">    <span class="keyword">return</span> float(self.fc[f][cat])</span><br><span class="line"> <span class="keyword">return</span> <span class="number">0.0</span></span><br></pre></td></tr></table></figure><p></p><p>train方法接受一个文档和一个分类作为参数。它利用getfeatures函数，将内容项拆分为彼此独立的各个特征。然后调用incf函数，根据该文档所处分类增加该特征的分类计数值。最后，会调用incc来增加该分类的总计数值。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self,item,cat)</span>:</span></span><br><span class="line">    features=self.getfeatures(item)</span><br><span class="line">    <span class="comment">#增加特征对应的分类计数</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> features:</span><br><span class="line">        self.incf(f,cat)</span><br><span class="line">    <span class="comment">#增加该分类的总计数量</span></span><br><span class="line">    self.incc(cat)</span><br></pre></td></tr></table></figure><p></p><p>我们来写一段小程序，测试上面的train函数。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cl=classifier(docclass.getwords)</span><br><span class="line">cl.train(<span class="string">'the quick brown fox jumps over the lazy dog'</span>,<span class="string">'good'</span>)</span><br><span class="line">cl.train(<span class="string">'make quick money in the online casino'</span>,<span class="string">'bad'</span>)</span><br><span class="line">cl.fcount(<span class="string">'quick'</span>,<span class="string">'good'</span>)</span><br><span class="line"><span class="comment">#1.0</span></span><br><span class="line">cl.fcount(<span class="string">'quick'</span>,<span class="string">'bad'</span>)</span><br><span class="line"><span class="comment">#1.0</span></span><br></pre></td></tr></table></figure><p></p><p>此处，我们用一个函数将训练用的样本数据导数到分类器中是很有价值的，因为这样就无须每次创建分类器的时候对其进行手工训练了。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sampletrain</span><span class="params">(cl)</span>:</span></span><br><span class="line">      cl.train(<span class="string">'Nobody owns the water.'</span>,<span class="string">'good'</span>)</span><br><span class="line">      cl.train(<span class="string">'the quick rabbit jumps fences'</span>,<span class="string">'good'</span>)</span><br><span class="line">      cl.train(<span class="string">'buy pharmaceuticals now'</span>,<span class="string">'bad'</span>)</span><br><span class="line">      cl.train(<span class="string">'make quick money at the online casino'</span>,<span class="string">'bad'</span>)</span><br><span class="line">      cl.train(<span class="string">'the quick brown fox jumps'</span>,<span class="string">'good'</span>)</span><br></pre></td></tr></table></figure><p></p><p>经过一次上述训练后，fc变量的计数值为：</p><table><thead><tr><th>单词</th><th>good</th><th>bad</th></tr></thead><tbody><tr><td>water</td><td>1</td><td>0</td></tr><tr><td>the</td><td>3</td><td>1</td></tr><tr><td>rabbit</td><td>1</td><td>0</td></tr><tr><td>quick</td><td>2</td><td>1</td></tr><tr><td>pharmaceuticals</td><td>0</td><td>1</td></tr><tr><td>owns</td><td>1</td><td>0</td></tr><tr><td>online</td><td>0</td><td>1</td></tr><tr><td>now</td><td>0</td><td>1</td></tr><tr><td>nobady</td><td>1</td><td>0</td></tr><tr><td>money</td><td>0</td><td>1</td></tr><tr><td>make</td><td>0</td><td>1</td></tr><tr><td>jumps</td><td>2</td><td>0</td></tr><tr><td>fox</td><td>1</td><td>0</td></tr><tr><td>fences</td><td>1</td><td>0</td></tr><tr><td>casino</td><td>0</td><td>1</td></tr><tr><td>buy</td><td>0</td><td>1</td></tr><tr><td>brown</td><td>1</td><td>0</td></tr></tbody></table><p>变量cc的计数为:</p><table><thead><tr><th>分类</th><th>计数</th></tr></thead><tbody><tr><td>good</td><td>3</td></tr><tr><td>bad</td><td>2</td></tr></tbody></table><h1 id="计算概率"><a href="#计算概率" class="headerlink" title="计算概率"></a>计算概率</h1><p>既然我们已经对一封电子邮件在每个分类的出现次数进行了统计，那么接下来的工作就是要将其转换成概率了。所谓概率，是指一个介于0-1之间的数字，用以指示某一事件发生的可能性。在本例中，可以用一个单词在某个分类下的计数值除上该分类的总文档数，计算出该单词属于某个分类的概率。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fprob</span><span class="params">(self,f,cat)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> self.catcount(cat)==<span class="number">0</span>:<span class="comment">#该分类的文档计数为0</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span> </span><br><span class="line">    <span class="comment">#该单词在该分类的计数/该分类的总文档数</span></span><br><span class="line">    <span class="keyword">return</span> self.fcount(f,cat)/self.catcount(cat)</span><br></pre></td></tr></table></figure><p></p><p>我们称上述概率为条件概率，通常记为Pr(A|B)，读作”在给定B条件下A发生的概率“。<br>在本例中，目前我们所求得的值对应于Pr(word|classification),即：对于一个给定的分类，某个单词属于该分类的可能性。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cl=classifier(docclass.getwords) </span><br><span class="line">sampletrain(cl)</span><br><span class="line">cl.fprob(<span class="string">'quick'</span>,<span class="string">'good'</span>) </span><br><span class="line"><span class="comment">#计算过程：</span></span><br><span class="line"><span class="comment">#经过sampletrain训练后，fc变量中quick 属于good 2次，输入bad 1次</span></span><br><span class="line"><span class="comment">#good类的文档总共出现3次，bad类的文档总共出现2次</span></span><br><span class="line"><span class="comment"># 2/3=0.66666</span></span><br></pre></td></tr></table></figure><p></p><p>从执行结果中我们可以看到，在三篇被归类为”good“的文档中，有两篇文档出现了”quick“，即一篇”good“分类的文档中包含该单词的概率为:Pr(quick|good)=2/3=0.666。</p><h2 id="概率、先验概率、后验概率"><a href="#概率、先验概率、后验概率" class="headerlink" title="概率、先验概率、后验概率"></a>概率、先验概率、后验概率</h2><p>P(A|B)也叫做后验概率 ，此时P(A)叫做先验概率，P(B)为B事件发生的概率。这里解释一下先验概率和后验概率之间的关系。</p><h1 id="一个合理的推测的初始值"><a href="#一个合理的推测的初始值" class="headerlink" title="一个合理的推测的初始值"></a>一个合理的推测的初始值</h1><p>我们注意到前面的fc统计计数的表格，单词”money“只在一篇文档中出现过，并且这是一则涉及赌博的广告，因此文档被划归为”bad“类。由于单词”money“只在一篇”bad“类的文档中出现过，这时候利用fprob计算money属于good分类的概率为0.这样做有些偏激，因为”money“可能完全是一个中性词，出现这样的结果只是恰好训练集中有一个bad类别的文档含有单词”money“。从这里可以看出fprob方法针对目前为止见到的特征与分类，给出的结果在训练初级阶段，会对极少出现的单词变得异常敏感。后期随着训练的数据集越来越多，其概率值会被逐渐的纠正。<br>为了解决上述问题，在我们手头掌握的有关当前特征的信息极为有限时，我们还需要根据一个假设的概率来做出判断。一个推荐的初始值是0.5。我们还需要为假设的概率赋予一个权重——权重为1表示权重与一个单词相当。而根据训练集算出的估计概率的权重为：该特征在不同分类下总共出现的总次数。<br>最终概率为：</p><blockquote><p>(根据训练集估计的概率× 权重1 + 假设概率 × 权重2)/(权重1+权重2)</p></blockquote><p>以单词”money“为例，示范一下其在”bad“类下的概率计算过程。</p><blockquote><p>(count<em>fprob+weight</em>assumeprob)/(count+weight) #count是该特征在不同分类下出现的总次数<br>=(1<em>1+1</em>0.5)/(1+1)<br>=0.75</p></blockquote><p>现在我们写出考虑初始假设概率与权重的概率计算函数：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#prf为基础概率的计算函数，即根据训练集估计的概率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weightedprob</span><span class="params">(self,f,cat,prf,weight=<span class="number">1</span>,ap=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="comment">#计算当前的概率值</span></span><br><span class="line">    basicprob = prf(f,cat)</span><br><span class="line">    <span class="comment">#统计特征在不同分类下出现的总次数</span></span><br><span class="line">    totals = sum([self.fcount(f,c) <span class="keyword">for</span> c <span class="keyword">in</span> self.categories()])</span><br><span class="line">    <span class="comment">#计算加权平均</span></span><br><span class="line">    bp = (totals*basicprob + weight*ap)/(total+weight)</span><br><span class="line">    <span class="keyword">return</span> bp</span><br></pre></td></tr></table></figure><p></p><p>测试一下weightedprob函数：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cl=classifier(docclass.getwords) </span><br><span class="line">sampletrain(cl)</span><br><span class="line">cl.weightedprob(<span class="string">'money'</span>,<span class="string">'good'</span>,cl.fprob) </span><br><span class="line"><span class="comment">#（0*0+1*0.5)/(1+1)=0.25</span></span><br><span class="line">docclass.sampletrain(cl)<span class="comment">#又输入一次训练集</span></span><br><span class="line">cl.weightedprob(<span class="string">'money'</span>,<span class="string">'good'</span>,cl.fprob)</span><br><span class="line"><span class="comment">#(0*0+1*0.5)/(2+1)=0.16666666666666666</span></span><br></pre></td></tr></table></figure><p></p><p>正如我们所看到的：随着训练的进行，单词的概率从假设的初始值开始逐渐的被”拉动“，经过第二次运行sampletrain方法训练之后，classifier对各个单词的概率变得更加肯定了。</p><p>这里选择0.5作为初始值仅仅是因为它介于0-1的正中间。不过，也有可能我们已经掌握了更多的背景信息，从而做出更合理的初始值假设。例如：一个准备对垃圾信息过滤器进行训练的人，可以利用他人训练过的垃圾过滤器，将其所得的概率值作为此处的假设的概率初始值。不管怎样，这个过滤器要有能力处理极小会见到的单词。</p><h1 id="朴素分类器"><a href="#朴素分类器" class="headerlink" title="朴素分类器"></a>朴素分类器</h1><p>我们求出一个单词在指定分类下的概率后，就需要一种方法将各个单词的在该分类下的概率进行组合，从而得到整篇文档属于该分类的概率。<br>这里要介绍的朴素贝叶斯分类器。这种方法之所以被冠以”朴素“，是因为它假设将要被组合的各个概率是<strong>彼此独立</strong>的。即：一个单词在属于某个指定分类的概率，与其他单词出现在该分类的概率是不相关的。事实上这个假设是不成立的，因为你会发现，包含单词”Casino”（赌场）的文档更有可能包含单词”money”。<br>这意味着我们无法采用朴素贝叶斯分类器的计算结果用在做文档分类，因为不满足各个单词<strong>彼此独立</strong>的假设，尽管如此，我们还是可以对各个分类的结果进行比较，看看那个分类的可能性最大。在实际应用中，若不考虑假设的潜在缺陷，朴素贝叶斯分类器是一种非常有效的文档分类方法。</p><h1 id="整篇文档的概率"><a href="#整篇文档的概率" class="headerlink" title="整篇文档的概率"></a>整篇文档的概率</h1><p>为了使用朴素贝叶斯分类器，首先我们需要确定整篇文档属于给定分类的概率。正如此前讨论过的，我们需要假设概率的彼此独立性，即：可以通过将所有的概率相乘，计算出总的概率值。<br>例如：假设我们注意到有20%的“bad”类文档中出现了单词“Python”——Pr(Python|Bad)=0.2；同时有80%的“bad”类的文档出现了单词“casino”（赌场），即Pr(Casino|Bad)=0.8。两个单词同时出现在同一个“bad”类文档的概率为Pr(Python&amp;Casino|bad)=0.8*0.2=0.16。从中我们会发现，计算整篇文档的概率，只需将该篇文档中含有的不同的单词的概率相乘即可。<br>新建一个classifier的子类，取名naivebayes，并为其添加一个docprob方法，该方法的作用是提取特征（不同单词），并将所有单词的概率值相乘以求出整体概率:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">naivebayes</span><span class="params">(classifier)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">docprob</span><span class="params">(self,item,cat)</span>:</span></span><br><span class="line">    features = self.getfeatures(item)</span><br><span class="line">    <span class="comment">#将所有特征的概率相乘</span></span><br><span class="line">    p=<span class="number">1</span> </span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> features:</span><br><span class="line">      p*=self.weightedprob(f,cat,self.fprob)</span><br><span class="line">    <span class="keyword">return</span> p</span><br></pre></td></tr></table></figure><p></p><p>现在我们已经知道了如何计算Pr(Document|Category)，即一个给定的概率，文档中出现这些单词组合的可能性。<br>做到这一步还不够，为了对文档进行分类，我们真正需要的是Pr(Category|Document)。换言之，就是对于一篇给定的文档，它属于某个分类的概率是多少？ 如何进行换算呢？一个名叫Thomas Bayes的英国数学家早在大于250年前就已经找到了解决这一问题的办法。</p><h1 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h1><p>贝叶斯定理是一种对条件概率进行调换求解的方法，意思是通过P(B|A)来求P(A|B)，而A|B和B|A正好是A和B的位置对调了一下。</p><blockquote><p>Pr(A|B)=Pr(B|A)*Pr(A)/Pr(B)</p></blockquote><p>在本例中，即为：</p><blockquote><p>Pr(Category|Document) = Pr(Document|Category)*Pr(Category)/Pr(Document)</p></blockquote><p>Pr(Document|Category)的计算方法上一节已经介绍过了，但是等式中的Pr(Category)和Pr(Document)应该如何计算呢？<br>Pr(Category)是随机选择一篇文档属于该分类的概率，就是该分类的文档数除上文档的总数。<br>至于Pr(Document)，我们也可以计算它，但这将会是一下不必要的工作。请记住，我们不会将这一计算结果当做真实的概率值。相反，我们会计算每个分类的概率，然后对各个分类的计算结果进行比较，选最有可能的分类作为分类结果。<br>由于不论计算的是哪个分类，Pr(Document)的值都是一样的，其对结果所产生的影响也完全是一样的，因此我们完全可以忽略这一项。<br>prob方法用于计算属于某个分类的概率，并返回Pr(Document|Category)与Pr(Category)的乘积。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prob</span><span class="params">(self,item,cat)</span>:</span></span><br><span class="line">  catprob = self.catcount(cat)/self.totalcount()</span><br><span class="line">  docprob = self.docprob(item,cat)</span><br><span class="line">  <span class="keyword">return</span> docprob*catprob</span><br></pre></td></tr></table></figure><p></p><p>请在Python的执行环境下尝试一下该函数，看看针对不同的字符串和分类，概率值是如何计算的：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cl=naivebayes(docclass.getwords) </span><br><span class="line">sampletrain(cl)</span><br><span class="line">cl.prob(<span class="string">'quick rabbit'</span>,<span class="string">'good'</span>) </span><br><span class="line"><span class="comment">#0.15624999999999997</span></span><br><span class="line">cl.prob(<span class="string">'quick rabbit'</span>,<span class="string">'bad'</span>) </span><br><span class="line"><span class="comment">#0.050000000000000003</span></span><br></pre></td></tr></table></figure><p></p><p>计算过程如下:<br>任意一篇文档属于good的概率:3/5<br>quick单词出现在good类文档中的加权概率:(3<em>(2/3)+1</em>0.5)/(3+1)<br>rabiit单词出现在good分类文档的加权概率为：(1<em>(1/3)+1</em>0.5)/(1+1)<br>则出现quick rabbit字符串，属于good分类的概率为前面三个概率相乘=0.156</p><p>任意一篇文档属于bad的概率:2/5<br>quick单词出现在bad类文档中的加权概率:(3<em>(1/2)+1</em>0.5)/(3+1)<br>rabiit单词出现在bad分类文档的加权概率为：(1<em>(0)+1</em>0.5)/(1+1)<br>则出现quick rabbit字符串，属于bad分类的概率为前面三个概率相乘=0.05<br>根据不同分类值的比较，”quick rabbit”属于good的概率高于输入bad分类的可能性，故它应该被划归为good分类。</p><h1 id="分类结果的选择"><a href="#分类结果的选择" class="headerlink" title="分类结果的选择"></a>分类结果的选择</h1><p>构造朴素贝叶斯分类器的最后一个步骤是实际判定某个内容项属于哪个分类。此处最简单的方法，是计算被考査内容在每个不同分类中的概率，然后选择概率最大的分类作为分类结果。如果我们只是在试图判断”将内容放到哪里最合适”的问题，那么这不失为一种可行的策略，但是在许多应用中，我们无法将各个分类同等看待，而且在一些应用中，对于分类法而言，承认不知道答案，要好过把概率值最大值的那个分类作为结果的情况。</p><p>在垃圾信息过滤的例子中，避免将普通邮件错当成垃圾邮件要比截获每一封垃圾邮件更为重要。收件箱中偶尔收到几封垃圾邮件还是可以容忍的，但是一封重要的邮件则有可能会因为自动过滤到废件箱而被完全忽视。假如我们必须在废件箱中找回自己的重要邮件，那就真的没必要再使用垃圾信息过滤器了。</p><p>为了解决这一问题，我们可以为每个分类定义一个最小阈值。对于一封将要被划归到某个分类的新邮件而言，其概率与所有其他分类的概率相比，必须大于某个指定的数值才行。这一指定的数值就是阈值。以垃圾邮件过滤为例，假如过滤到”bad”分类的的阈值为3,则针对”bad”分类的概率就必须至少3倍于针对”good”分类的概率才行。假如针对”good”分类的阔值为1，则对于任何邮件，只要概率确实大于针对”bad”分类的概率，它就是属于”good”分类的。任何更有可能属于”bad”分类，但概率并没有超过其它分类概率3倍以上的邮件，都将被划归到“未知”分类中。<br>为了定义阈值，请修改初始化方法，在classifier中加人一个新的实例变量<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,getfeatures)</span>:</span></span><br><span class="line">  classifier.__init_ _(self,getfeatures)</span><br><span class="line">  self.thresholds=&#123;&#125;</span><br></pre></td></tr></table></figure><p></p><p>请加人几个用于设值和取值的简单方法，令其默认返回为1.0<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setthreshold</span><span class="params">(self,cat,t)</span>:</span></span><br><span class="line">  self.thresholds[cat]=t</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getthreshold</span><span class="params">(self,cat)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> cat <span class="keyword">not</span> <span class="keyword">in</span> self.thresholds: </span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span></span><br><span class="line">  <span class="keyword">return</span> self.thresholds[cat]</span><br></pre></td></tr></table></figure><p></p><p>现在，我们可以构建classify方法了。该方法将计算每个分类的概率，从中得出最大值，并将其与次大概率值进行对比，确定是否超过了规定的阈值。如果没有任何一个分类满足上述条件，方法就返回默认值。请将该方法加入classifier中:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(self,item,default=None)</span>:</span></span><br><span class="line">  probs=&#123;&#125;</span><br><span class="line">  <span class="comment">#寻找概率最大的分类</span></span><br><span class="line">  max=<span class="number">0.0</span></span><br><span class="line">  <span class="keyword">for</span> cat <span class="keyword">in</span> self.categories():</span><br><span class="line">    probs[cat]=self.prob(item,cat)</span><br><span class="line">    <span class="keyword">if</span> probs[cat]&gt;max:</span><br><span class="line">      max=probs[cat]</span><br><span class="line">      best=cat</span><br><span class="line">  <span class="comment"># 确保最大概率值超过次大概率值的指定倍数</span></span><br><span class="line">  <span class="keyword">for</span> cat <span class="keyword">in</span> probs:</span><br><span class="line">    <span class="keyword">if</span> cat==best:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">if</span> probs[cat]*self.getthreshold(best)&gt;probs[best]:</span><br><span class="line">      <span class="keyword">return</span> default</span><br><span class="line">  <span class="keyword">return</span> best</span><br></pre></td></tr></table></figure><p></p><p>大功告成！我们已经建立起一个完整的文档分类系统。通过构造不同的特征提取方法，我们还可以对系统进行扩展，以实现对任何其他内容的分类。<br>下面我们来验证一下这个分类器：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cl=naivebayes(getwords)</span><br><span class="line">sampletrain(cl)</span><br><span class="line">cl.classify(<span class="string">'quick rabbit'</span>,default=<span class="string">'unknown'</span>)</span><br><span class="line"><span class="comment"># 'good'</span></span><br><span class="line">cl.classify(<span class="string">'quick money'</span>,default=<span class="string">'unknown'</span>)</span><br><span class="line"><span class="comment"># 'bad'</span></span><br><span class="line">cl.setthreshold(<span class="string">'bad'</span>,<span class="number">3.0</span>)</span><br><span class="line">cl.classify(<span class="string">'quick money'</span>,default=<span class="string">'unknown'</span>)</span><br><span class="line"><span class="comment"># 'unknown'</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">  docclass.sampletrain(cl)</span><br><span class="line">cl.classify(<span class="string">'quick money'</span>,default=<span class="string">'unknown'</span>)</span><br><span class="line"><span class="comment"># 'bad'</span></span><br></pre></td></tr></table></figure><p></p><p>当然，我们还可以修改这一阈值，看看对结果有何影响。一些垃圾信息过滤插件允许用户自己控制阈值。<br>这一以来，只要当前阈值会出现太多的垃圾邮件进入收件箱的情况，或者大量正常的邮件被归为垃圾邮件，我们就可以对阈值进行调整。<br>当然，对于另一些涉及文档过滤的应用而言，阈值的定义也可能不同；有时候所有分类可能是平等的，有时候将内容过滤到“未知”分类是不可接受的。</p></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>如果您觉得读完本文有收获，不妨小额赞助我一下，让我有动力继续写出高质量的教程！</div><button id="rewardButton" disable="enable"><span>打赏</span></button><div id="QR" style="display:block"><div id="wechat" style="display:inline-block"><img id="wechat_qr" src="/images/smacker.jpg" alt="倔强的土豆 微信支付"><p>微信支付</p></div></div></div></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2018/01/07/optimization/" rel="next" title="优化"><i class="fa fa-chevron-left"></i> 优化</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"></div></div></footer></div></article><div class="post-spread"></div></div></div><div class="comments" id="comments"><div id="gitment-container"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div id="sidebar-dimmer"></div><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">倔强的土豆</p><p class="site-description motion-element" itemprop="description">分享机器学习、深度学习的点滴</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">7</span> <span class="site-state-item-name">日志</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/laiqun" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:laiqun@msn.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#文档过滤"><span class="nav-number">1.</span> <span class="nav-text">文档过滤</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#过滤垃圾信息"><span class="nav-number">2.</span> <span class="nav-text">过滤垃圾信息</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#文档和单词"><span class="nav-number">3.</span> <span class="nav-text">文档和单词</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#对分类器进行训练"><span class="nav-number">4.</span> <span class="nav-text">对分类器进行训练</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#计算概率"><span class="nav-number">5.</span> <span class="nav-text">计算概率</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#概率、先验概率、后验概率"><span class="nav-number">5.1.</span> <span class="nav-text">概率、先验概率、后验概率</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#一个合理的推测的初始值"><span class="nav-number">6.</span> <span class="nav-text">一个合理的推测的初始值</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#朴素分类器"><span class="nav-number">7.</span> <span class="nav-text">朴素分类器</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#整篇文档的概率"><span class="nav-number">8.</span> <span class="nav-text">整篇文档的概率</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#贝叶斯定理"><span class="nav-number">9.</span> <span class="nav-text">贝叶斯定理</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#分类结果的选择"><span class="nav-number">10.</span> <span class="nav-text">分类结果的选择</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">倔强的土豆</span></div><div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div><span class="post-meta-divider">|</span><div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.3</div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script><link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css"><script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script><script type="text/javascript">function renderGitment(){var a=new Gitmint({id:window.location.pathname,owner:"laiqun",repo:"laiqun.github.io",lang:navigator.language||navigator.systemLanguage||navigator.userLanguage,oauth:{client_secret:"55aaeb736714431ea52109dd66461b1644ca6177",client_id:"c90dfa80285ea91b9120"}});a.render("gitment-container")}renderGitment()</script></body></html><!-- rebuild by neat -->