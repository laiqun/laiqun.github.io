<!-- build time:Fri Feb 23 2018 23:31:07 GMT+0800 (CST) --><!DOCTYPE html><html class="theme-next mist" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3"><link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222"><meta name="keywords" content="Hexo, NexT"><meta name="description" content="文档过滤本章将向大家演示如何依据内容来对文档进行分类。文档分类是机器智能(machine Intelligence)的一个应用，很有实用价值,而且现在越来越普及。关于文档过滤，最有价值也最为人们所熟知的应用，恐怕要数垃圾邮件过滤了。随着电子邮件的广泛普及与邮件发送的超低成本，人们面临的一大问题是任何人的邮件地址只要落入"><meta property="og:type" content="article"><meta property="og:title" content="文档过滤"><meta property="og:url" content="laiqun.github.io/2018/01/22/document-filtering/index.html"><meta property="og:site_name" content="广阔天地，大有作为"><meta property="og:description" content="文档过滤本章将向大家演示如何依据内容来对文档进行分类。文档分类是机器智能(machine Intelligence)的一个应用，很有实用价值,而且现在越来越普及。关于文档过滤，最有价值也最为人们所熟知的应用，恐怕要数垃圾邮件过滤了。随着电子邮件的广泛普及与邮件发送的超低成本，人们面临的一大问题是任何人的邮件地址只要落入不法者之手，便有可能会收到未经许可的商业邮件，致使我们无法阅读到真正感兴趣的邮件"><meta property="og:locale" content="zh-Hans"><meta property="og:updated_time" content="2018-02-10T10:17:22.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="文档过滤"><meta name="twitter:description" content="文档过滤本章将向大家演示如何依据内容来对文档进行分类。文档分类是机器智能(machine Intelligence)的一个应用，很有实用价值,而且现在越来越普及。关于文档过滤，最有价值也最为人们所熟知的应用，恐怕要数垃圾邮件过滤了。随着电子邮件的广泛普及与邮件发送的超低成本，人们面临的一大问题是任何人的邮件地址只要落入不法者之手，便有可能会收到未经许可的商业邮件，致使我们无法阅读到真正感兴趣的邮件"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",version:"5.1.3",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!0},fancybox:!0,tabs:!0,motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="laiqun.github.io/2018/01/22/document-filtering/"><title>文档过滤 | 广阔天地，大有作为</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">广阔天地，大有作为</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">你看到我的筋斗云了嘛？</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="laiqun.github.io/2018/01/22/document-filtering/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="倔强的土豆"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="广阔天地，大有作为"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">文档过滤</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-22T20:26:20+08:00">2018-01-22 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/01/22/document-filtering/#comments" itemprop="discussionUrl"><span class="post-comments-count gitment-comments-count" data-xid="/2018/01/22/document-filtering/" itemprop="commentsCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="文档过滤"><a href="#文档过滤" class="headerlink" title="文档过滤"></a>文档过滤</h1><p>本章将向大家演示如何依据内容来对文档进行分类。文档分类是机器智能(machine Intelligence)的一个应用，很有实用价值,而且现在越来越普及。关于文档过滤，最有价值也最为人们所熟知的应用，恐怕要数垃圾邮件过滤了。随着电子邮件的广泛普及与邮件发送的超低成本，人们面临的一大问题是任何人的邮件地址只要落入不法者之手，便有可能会收到未经许可的商业邮件，致使我们无法阅读到真正感兴趣的邮件。</p><p>当然，垃圾信息的问题并非仅限于电子邮件。随着时间的推移，Web网站已经越来越具有互动的特征,它们或向用户征求意见，或请求用户提供原创内容，这些行为都会伴以垃圾信息侵扰的问题。例如像Yahoo!Groups和Usenet这样的公共留言板，就长期遭受着垃圾帖的侵扰。这些帖子或与留言板主题毫不相干，或者就是以兜售可疑产品为目的。现在，博客和维基也遭遇到了同样的问题。每当我们在构建一个允许普通大众一起参与的应用系统时，就始终应该考虑应对垃圾信息的策略。</p><p>本章中介绍的算法不是专门针对垃圾信息的。由于这些算法可以解决更为一般性的问题，即学习井鉴别文档所属的分类，因此我们还可以将其应用于一些相比垃圾信息而言不那么令人生厌的问题。一种可能的应用是，根据邮件正文自动将收件箱中的邮件划分为社交类邮件或工作相关类邮件。还有一种可能的应用是，识别出要求回复的邮件，并将其自动转发给最适合的人员进行处理。本章的最后一个例子，会为大家示范如何将来自某一RSS订阅源的内容项自动过滤到不同的分类之中。</p><h1 id="过滤垃圾信息——朴素贝叶斯方法"><a href="#过滤垃圾信息——朴素贝叶斯方法" class="headerlink" title="过滤垃圾信息——朴素贝叶斯方法"></a>过滤垃圾信息——朴素贝叶斯方法</h1><p>早期尝试对垃圾信息进行过滤所用的都是基于规则的分类器(rule-based classitiem)，使用时会有人事先设计好一组规则，用以指明某条信息是否属干垃圾信息。典型的规则包括英文大写字母的过度使用，与医学药品相关的单词，或是过干花哨的HTML用色等。<br>基于规则的分类器，其间题很快就显现了出来:垃圾信息制造者在知道了所有规则以后，为了绕开过滤器，其行为就会变得更加隐蔽，而且人们会发现，如果他们的父母不知道关闭大写锁定键(Caps Lock)，一些正常的邮件也会被归类成垃圾邮件。<br>基于规则的过滤器还有另一个问题：是否被当做垃圾信息很大程度上因其所面对的读者和张贴位置的不同而不同。对于某一位特定用户、公告留言板或者维基百科而言，那些可以用来明确指示是否是垃圾信息的关键词，在其他场合下可能就会变得相当正常。为了解决这一问题，本章所要考查的程序会在开始阶段和逐渐收到更多消息之后，根据人们提供给它的有关哪些是垃圾邮件，哪些不是垃圾邮件的信息，不断地进行学习。通过这样的方式，我可以分别为不同的用户、群组或网站建立起各自的应用实例和数据集。它们对垃圾信息的界定将逐步形成自己的观点。</p><h2 id="特征抽取-将文档拆成单词"><a href="#特征抽取-将文档拆成单词" class="headerlink" title="特征抽取-将文档拆成单词"></a>特征抽取-将文档拆成单词</h2><p>即将构造的分类器需要利用某些特征来对不同的内容项进行分类。所谓特征，是指任何可以用来判断内容中具备或缺失的东西。当考虑对文档进行分类时，所谓的内容即是文档，而特征则是文档中的单词。将单词作为特征时，其假设是某些单词相对而言更有可能会出现于垃圾信息中。这一假设是大多数垃圾信息过滤器背后所依赖的基本前提。不过，特征未必是一个个单词，它们也可以是词组或短语，或者任何可以归为文档中缺失或存在的其他东西。</p><p>请新建一个文件，取名docclass.py，并在其中加人一个名为getwords的函数，用来从文本中提取特征:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getwords</span><span class="params">(doc)</span>:</span></span><br><span class="line">    splitter=re.compile (<span class="string">'\\W*'</span>)</span><br><span class="line">    <span class="comment">#根据非字母字符进行单词拆分</span></span><br><span class="line">    words=[s.lower() <span class="keyword">for</span> s <span class="keyword">in</span> splitter.split(doc) <span class="keyword">if</span> len(s)&gt;<span class="number">2</span> <span class="keyword">and</span> len(s)&lt;<span class="number">20</span>]</span><br><span class="line">    <span class="comment">#只返回一组不重复的单词</span></span><br><span class="line">    <span class="keyword">return</span> dict ( [ (w,<span class="number">1</span>) <span class="keyword">for</span> w <span class="keyword">in</span> words] )</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">去重是很重要的，</span></span><br><span class="line"><span class="string">比如我们训练集只有一个good文档文档内容为"quick quick quick"</span></span><br><span class="line"><span class="string">那么计算P(qucik|good)=3/1=3，而概率是取0-1之间的数值，这个结果很明显是错误的</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><p></p><p>该函数以任何非字母类字符为分隔符对文本进行划分，将文本拆分成了一个个单词。这过程只留下了真正的单词，并将这些单词全都转换成了小写形式。</p><p>决定采用哪些特征颇具技巧性，也十分重要。特征必须具备足够的普遍性，即时常出现，但又不能普遍到每一篇文档里都能找到。理论上，整篇文档的文本都可以作为特征，但是除非我们一再收到内容完全相同的邮件，否则这样的特征几乎肯定是毫无价值的。在另一种极端情况下，特征也可以是单个字符。但是由于每一封电子邮件中都有可能会出现所有这些字符，因此要想利用这样的特征将希望看到和不希望看到的文档区分开来是很困难的。即便选择使用单词作为特征，也依然还是会带来一些问题，包括如何正确划分单词，哪些标点符号应该被纳入单词，以及是否应该包含头信息(header information)等。</p><p>在根据特征进行判断时还有一点需要考虑，那就是如何才能更好地利用特征将一组文档划归到目标分类中去。例如，前述getwords函数的代码通过将单词转换为小写形式，从而减少了特征的总数。这意味着，程序会将位于句首以大写字母开头的单词与位于句中全小写形式的单词视为相同——这样做非常好，因为具有不同大小写形式的同一单词往往代表的含义是相同的。然而，上述函数完全没有考虑到被用于许多垃圾信息中的“SHOUTING风格”（指许多垃圾邮件中存在大写单词使用过度的情况），而这一点可能对区分垃圾邮件和非垃圾邮件是至关重要的。除此以外，如果超过半数以上的单词都是大写时，那就说明必定会有其他的特征存在。</p><p>正如你所看到的，在选择特征集时需要做大量的权衡，而且还要不断地进行调整。不过眼下，可以暂且使用这个简单的getwords函数，在本章的后续部分，我们还将了解到有关特征提取的一些改进方法。</p><h2 id="对分类器进行训练"><a href="#对分类器进行训练" class="headerlink" title="对分类器进行训练"></a>对分类器进行训练</h2><p>本章中讨论的分类器可以通过接受训练的方式来学习如何对文档进行分类。本书中的许多其他算法，例如我们在搜索引擎那一章见到过的神经网络，都是通过读取正确答案的样本进行学习的。如果分类器掌握的文档及其正确分类的样本越多，其预测的效果也就越好。人们专门设计分类器，其目的也就在于此，即：从极为不确定的状态开始，随着分类器不断了解到哪些特征对于分类而言更为重要，其确定性也在逐渐地增加。</p><p>我们要做的第一件事情，是编写一个代表分类器的类。这个类将对分类器到目前为止所掌握的信息进行封装。以这样的方式构造Python模块的好处在于，我们可以针对不同的用户、群组或査询，建立起多个分类器实例，并分别对它们加以训练，以响应特定群组的需求。<br>请在docclass.py中新建一个名为classifier的类:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">classifier</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, getfeatures,filename=None)</span></span></span><br><span class="line"><span class="function">    #择统计特征/分类组合的数量</span></span><br><span class="line">    self.fc=&#123;&#125;</span><br><span class="line">    <span class="comment">#统计每个分类中的文档数量</span></span><br><span class="line">    self.cc=&#123;)</span><br><span class="line">    self.getfeatures=getfeatures</span><br></pre></td></tr></table></figure><p></p><p>该类中有3个实例变量，它们分别是fc、cc和getfeatures。变量fc将记录不同特征在不同类别中的计数。例如：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'python'</span>: &#123;<span class="string">'bad'</span>: <span class="number">0</span>, <span class="string">'good'</span>: <span class="number">6</span>&#125;, <span class="string">'the'</span>: &#123;<span class="string">'bad'</span>: <span class="number">3</span>, <span class="string">'good'</span>: <span class="number">3</span>&#125;&#125;</span><br></pre></td></tr></table></figure><p></p><p>上述示例表明，单词‘the’被划归为”bad“类3次，被划为good类也是3次。而单词”python”却只在good类中出现过。<br>变量cc记录各个分类出现的次数的字典。这一信息是我们稍后即将讨论的概率计算所需的。最后一个实例变量，getfeatures，对应一个函数，其作用是从即将被归类的内容项中提取特征，在本里中，即为前面定义的getwords函数。<br>我们新加入以下几个函数，以实现计数值的增加和获取。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加某个分类的总数量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">incc</span><span class="params">(self,cat)</span>:</span></span><br><span class="line">  self.cc.setdefault(cat,<span class="number">0</span>)</span><br><span class="line">  self.cc[cat]+=<span class="number">1</span></span><br><span class="line"><span class="comment"># 属于某一分类的总数量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">catcount</span><span class="params">(self,cat)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> cat <span class="keyword">in</span> self.cc:</span><br><span class="line">    <span class="keyword">return</span> float(self.cc[cat])</span><br><span class="line"> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"><span class="comment"># 所有分类的总数量相加</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">totalcount</span><span class="params">(self)</span>:</span></span><br><span class="line"> <span class="keyword">return</span> sum(self.cc.values())</span><br><span class="line"><span class="comment"># 有几个分类，获取分类名列表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">categories</span><span class="params">(self)</span>:</span></span><br><span class="line"> <span class="keyword">return</span> self.cc.keys()</span><br><span class="line"><span class="comment"># 增加对特征/分类的键值对的计数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">incf</span><span class="params">(self,f,cat)</span>:</span></span><br><span class="line">  self.fc.setdefault(f,&#123;&#125;)</span><br><span class="line">  self.fc[f].setdefault(cat,<span class="number">0</span>)<span class="comment">#键不存在时，设置默认值</span></span><br><span class="line">  self.fc[f][cat]+=<span class="number">1</span></span><br><span class="line"><span class="comment"># 某一特征出现在某一分类中的次数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fcount</span><span class="params">(self,f,cat)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> f <span class="keyword">in</span> self.fc <span class="keyword">and</span> cat <span class="keyword">in</span> self.fc[f]:</span><br><span class="line">    <span class="keyword">return</span> float(self.fc[f][cat])</span><br><span class="line"> <span class="keyword">return</span> <span class="number">0.0</span></span><br></pre></td></tr></table></figure><p></p><p>train方法接受一个文档和一个分类作为参数。它利用getfeatures函数，将内容项拆分为彼此独立的各个特征。然后调用incf函数，根据该文档所处分类增加该特征的分类计数值。最后，会调用incc来增加该分类的总计数值。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self,item,cat)</span>:</span></span><br><span class="line">    features=self.getfeatures(item)</span><br><span class="line">    <span class="comment">#增加特征对应的分类计数</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> features:</span><br><span class="line">        self.incf(f,cat)</span><br><span class="line">    <span class="comment">#增加该分类的总计数量</span></span><br><span class="line">    self.incc(cat)</span><br></pre></td></tr></table></figure><p></p><p>我们来写一段小程序，测试上面的train函数。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cl=classifier(docclass.getwords)</span><br><span class="line">cl.train(<span class="string">'the quick brown fox jumps over the lazy dog'</span>,<span class="string">'good'</span>)</span><br><span class="line">cl.train(<span class="string">'make quick money in the online casino'</span>,<span class="string">'bad'</span>)</span><br><span class="line">cl.fcount(<span class="string">'quick'</span>,<span class="string">'good'</span>)</span><br><span class="line"><span class="comment">#1.0</span></span><br><span class="line">cl.fcount(<span class="string">'quick'</span>,<span class="string">'bad'</span>)</span><br><span class="line"><span class="comment">#1.0</span></span><br></pre></td></tr></table></figure><p></p><p>此处，我们用一个函数将训练用的样本数据导数到分类器中是很有价值的，因为这样就无须每次创建分类器的时候对其进行手工训练了。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sampletrain</span><span class="params">(cl)</span>:</span></span><br><span class="line">      cl.train(<span class="string">'Nobody owns the water.'</span>,<span class="string">'good'</span>)</span><br><span class="line">      cl.train(<span class="string">'the quick rabbit jumps fences'</span>,<span class="string">'good'</span>)</span><br><span class="line">      cl.train(<span class="string">'buy pharmaceuticals now'</span>,<span class="string">'bad'</span>)</span><br><span class="line">      cl.train(<span class="string">'make quick money at the online casino'</span>,<span class="string">'bad'</span>)</span><br><span class="line">      cl.train(<span class="string">'the quick brown fox jumps'</span>,<span class="string">'good'</span>)</span><br></pre></td></tr></table></figure><p></p><p>经过一次上述训练后，fc变量的计数值为：</p><table><thead><tr><th>单词</th><th>good</th><th>bad</th></tr></thead><tbody><tr><td>water</td><td>1</td><td>0</td></tr><tr><td>the</td><td>3</td><td>1</td></tr><tr><td>rabbit</td><td>1</td><td>0</td></tr><tr><td>quick</td><td>2</td><td>1</td></tr><tr><td>pharmaceuticals</td><td>0</td><td>1</td></tr><tr><td>owns</td><td>1</td><td>0</td></tr><tr><td>online</td><td>0</td><td>1</td></tr><tr><td>now</td><td>0</td><td>1</td></tr><tr><td>nobady</td><td>1</td><td>0</td></tr><tr><td>money</td><td>0</td><td>1</td></tr><tr><td>make</td><td>0</td><td>1</td></tr><tr><td>jumps</td><td>2</td><td>0</td></tr><tr><td>fox</td><td>1</td><td>0</td></tr><tr><td>fences</td><td>1</td><td>0</td></tr><tr><td>casino</td><td>0</td><td>1</td></tr><tr><td>buy</td><td>0</td><td>1</td></tr><tr><td>brown</td><td>1</td><td>0</td></tr></tbody></table><p>变量cc的计数为:</p><table><thead><tr><th>分类</th><th>计数</th></tr></thead><tbody><tr><td>good</td><td>3</td></tr><tr><td>bad</td><td>2</td></tr></tbody></table><h2 id="计算指定类别后，出现该单词的概率"><a href="#计算指定类别后，出现该单词的概率" class="headerlink" title="计算指定类别后，出现该单词的概率"></a>计算指定类别后，出现该单词的概率</h2><p>既然我们已经对一封电子邮件在每个分类的出现次数进行了统计，那么接下来的工作就是要将其转换成概率了。所谓概率，是指一个介于0-1之间的数字，用以指示某一事件发生的可能性。在本例中，指定分类后，出现该单词的数量，除上该分类的文档总数，就是该分类下出现该单词的概率。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fprob</span><span class="params">(self,f,cat)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> self.catcount(cat)==<span class="number">0</span>:<span class="comment">#该分类的文档计数为0</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span> </span><br><span class="line">    <span class="comment">#该单词在该分类的计数/该分类的总文档数</span></span><br><span class="line">    <span class="keyword">return</span> self.fcount(f,cat)/self.catcount(cat)</span><br></pre></td></tr></table></figure><p></p><p>我们称上述概率为条件概率，通常记为Pr(A|B)，读作”在给定B条件下A发生的概率“。<br>在本例中，目前我们所求得的值对应于Pr(word|classification),即：对于一个给定的分类，某个单词属于该分类的可能性。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cl=classifier(docclass.getwords) </span><br><span class="line">sampletrain(cl)</span><br><span class="line">cl.fprob(<span class="string">'quick'</span>,<span class="string">'good'</span>) </span><br><span class="line"><span class="comment">#计算过程：</span></span><br><span class="line"><span class="comment">#经过sampletrain训练后，fc变量中quick 属于good 2次，输入bad 1次</span></span><br><span class="line"><span class="comment">#good类的文档总共出现3次，bad类的文档总共出现2次</span></span><br><span class="line"><span class="comment"># 2/3=0.66666</span></span><br></pre></td></tr></table></figure><p></p><p>从执行结果中我们可以看到，在三篇被归类为”good“的文档中，有两篇文档出现了”quick“，即一篇”good“分类的文档中包含该单词的概率为:Pr(quick|good)=2/3=0.666。</p><blockquote><p>概率、先验概率、后验概率<br>P(A|B)也叫做后验概率 ，此时P(A)叫做先验概率，P(B)为B事件发生的概率。这里解释一下先验概率和后验概率之间的关系。<br>对上帝来说，一切都是确定的，因此概率作为一门学问存在，正好证明了人类的无知。好在人类还是足够聪明的，我们并没有因为事物是随机的而束手无措，我们根据事物的可能性来决定我们的行为。比如，某个人抢银行之前，一定反反复复考虑过各种可能性。如果人们要等到一切都确定后再做，那么你可能什么都做不了，因为几乎一切都是随机的。<br>一个事情有N种发生的可能性，我们不能确信哪种会发生，是因为我们不能控制结果的发生，影响结果的许多因素不在我们的支配范围之内，这些因素影响结果的机理或者我们不知道，或者太复杂以至于超出了我们大脑或电脑的运算能力。比如：我们不确定掷硬币得到正面或反面，是因为我们的能力不足以用一些物理方程来求解这个结果。再比如：你不能断定你期末能考88分，因为出题、阅卷的不是你。<br>过去发生的事情虽然事实上是确定的，但因为我们的不知道过去这里发生了什么，它成了随机的。我们在某个地方挖出了一块瓷器的碎片，它可能是孔子的夜壶，可能是秦始皇的餐具，也可能是校长家的破茶壶从他家到垃圾站又被埋在了这个地方。<br>因此：概率在实质上就是无知，而不是说事物本身是随机的。<br>你拿着一把锄头在操场上乱挖，忽然发现一个暗室。里面是什么情景呢？应该说一切皆有可能。你根据你的大脑已储存的东西能做出一些可能性判断，有些可能性高，如“里面是黑的”。有些可能性低：如发现“本拉登在这里打麻将”。有无限的可能性，也可能藏着一个杀人犯，也可能有毒蛇，……。你对每种场景的可能性认识就是概率分布P(Ai)。这样的概率就是先验概率。<br>你是否能听到狗叫也是随机的，你对此的概率判断P(y), (y表示会听到狗叫）也是先验判断。<br>如果接下来你确实听见了狗叫，你对洞中情形虽然也不确定，但肯定会有新的判断：“本拉登边吃狗肉边打麻将”、“几个狗在打麻将”、“一只狗想念另一只狗，在这里放录音”……。这些场景先前当然你也想到过（是某个Ai之一），不过现在“听到狗叫”后，你的概率判断发生了变化，你现在的判断就叫后验概率P(Ai|y)。<br>先验概率是指根据以往经验和分析得到的概率,如全概率公式,它往往作为”由因求果”问题中的”因”出现。<br>例子:<br>你来到一个山洞,这个山洞里可能有熊也可能没有熊, 记你觉得山洞有熊的为事件Y。<br>然后,你也许听到山洞里传来熊的吼声, 记听到熊吼声为事件X。<br>你一开始认为山洞有熊的概率是P(Y); 听到熊的吼声之后,你认为有熊的概率是P(Y|X)。<br>很明显,在这个例子里面P(Y|X)&gt;P(Y), P(Y)就是先验概率,P(Y|X)是后验概率。<br>《猫和老鼠》乘猫之危 猫的两种猜想 后面猫通过高速摄影机，慢镜头播放，这时候有了新的认识。</p></blockquote><h2 id="一个合理的推测的初始值"><a href="#一个合理的推测的初始值" class="headerlink" title="一个合理的推测的初始值"></a>一个合理的推测的初始值</h2><p>我们注意到前面的fc统计计数的表格，单词”money“只在一篇文档中出现过，并且这是一则涉及赌博的广告，因此文档被划归为”bad“类。由于单词”money“只在一篇”bad“类的文档中出现过，这时候利用fprob计算money属于good分类的概率为0。这样做有些偏激，因为”money“可能完全是一个中性词，出现这样的结果只是恰好训练集中有一个bad类别的文档含有单词”money“。从这里可以看出fprob方法针对目前为止见到的特征与分类，给出的结果在训练初级阶段，会对极少出现的单词变得异常敏感。后期随着训练的数据集越来越多，其概率值会被逐渐的纠正。</p><blockquote><p>再举一个例子：假如我们有一篇文档包含我们训练集中没有的单词，比如“hello”，那么我们会发现，其属于good分类的概率为0，属于bad分类的概率也为0，这明显是不合理的。</p></blockquote><p>为了解决上述问题，在我们手头掌握的有关当前特征的信息极为有限时，我们还需要根据一个假设的概率来做出判断。一个推荐的初始值是0.5。我们还需要为假设的概率赋予一个权重——权重为1表示权重与一个单词相当。而根据训练集算出的估计概率的权重为：该特征在不同分类下总共出现的总次数。<br>最终概率为：</p><blockquote><p>(根据训练集估计的概率× 权重1 + 假设概率 × 权重2)/(权重1+权重2)</p></blockquote><p>以单词”money“为例，示范一下其在”bad“类下的概率计算过程。</p><blockquote><p>(count×fprob+weight×assumeprob)/(count+weight) #count是该特征在不同分类下出现的总次数<br>=(1×1+1×0.5)/(1+1)<br>=0.75</p></blockquote><p>现在我们写出考虑初始假设概率与权重的概率计算函数：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#prf为基础概率的计算函数，即根据训练集估计的概率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weightedprob</span><span class="params">(self,f,cat,prf,weight=<span class="number">1</span>,ap=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="comment">#计算当前的概率值</span></span><br><span class="line">    basicprob = prf(f,cat)</span><br><span class="line">    <span class="comment">#统计特征在不同分类下出现的总次数</span></span><br><span class="line">    totals = sum([self.fcount(f,c) <span class="keyword">for</span> c <span class="keyword">in</span> self.categories()])</span><br><span class="line">    <span class="comment">#计算加权平均</span></span><br><span class="line">    bp = (totals*basicprob + weight*ap)/(total+weight)</span><br><span class="line">    <span class="keyword">return</span> bp</span><br></pre></td></tr></table></figure><p></p><p>测试一下weightedprob函数：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cl=classifier(docclass.getwords) </span><br><span class="line">sampletrain(cl)</span><br><span class="line">cl.weightedprob(<span class="string">'money'</span>,<span class="string">'good'</span>,cl.fprob) </span><br><span class="line"><span class="comment">#（0×0+1×0.5)/(1+1)=0.25</span></span><br><span class="line">docclass.sampletrain(cl)<span class="comment">#又输入一次训练集</span></span><br><span class="line">cl.weightedprob(<span class="string">'money'</span>,<span class="string">'good'</span>,cl.fprob)</span><br><span class="line"><span class="comment">#(0×0+1×0.5)/(2+1)=0.16666666666666666</span></span><br></pre></td></tr></table></figure><p></p><p>正如我们所看到的：随着训练的进行，单词的概率从假设的初始值开始逐渐的被”拉动“，经过第二次运行sampletrain方法训练之后，classifier对各个单词的概率变得更加肯定了。</p><p>这里选择0.5作为初始值仅仅是因为它介于0-1的正中间。不过，也有可能我们已经掌握了更多的背景信息，从而做出更合理的初始值假设。例如：一个准备对垃圾信息过滤器进行训练的人，可以利用他人训练过的垃圾过滤器，将其所得的概率值作为此处的假设的概率初始值。不管怎样，这个过滤器要有能力处理极小会见到的单词。</p><h2 id="朴素贝叶斯分类器概念"><a href="#朴素贝叶斯分类器概念" class="headerlink" title="朴素贝叶斯分类器概念"></a>朴素贝叶斯分类器概念</h2><p>我们求出一个单词在指定分类下的概率后，就需要一种方法将各个单词的在该分类下的概率进行组合，从而得到整篇文档属于该分类的概率。<br>这里要介绍的“朴素贝叶斯”分类器。“朴素贝叶斯”要从两个概念上来理解，第一个是“朴素”，第二个是“贝叶斯”。下面我们依次介绍每个概念：</p><ol><li>先说”<strong>朴素</strong>“的含义，是因为它假设将要被组合的各个概率是<strong>彼此独立</strong>的。即：一个单词在属于某个指定分类的概率，与其他单词出现在该分类的概率是不相关的。事实上这个假设是不成立的，因为你会发现，包含单词”Casino”（赌场）的文档更有可能包含单词”money”。这意味着我们无法采用朴素贝叶斯分类器的计算结果用在做文档分类，因为不满足各个单词<strong>彼此独立</strong>的假设，尽管如此，我们还是可以对各个分类的结果进行比较，看看那个分类的可能性最大。在实际应用中，若不考虑假设的潜在缺陷，朴素贝叶斯分类器是一种非常有效的文档分类方法。</li><li>再说<strong>贝叶斯</strong>的含义，后续我们会用到贝叶斯公式:Pr(A|B)=Pr(B|A)×Pr(A)/Pr(B),这里了解即可，下一小节会详细说明这个公式的具体应用。</li></ol><h3 id="Step-I-利用朴素的方法，计算P-Document-category"><a href="#Step-I-利用朴素的方法，计算P-Document-category" class="headerlink" title="Step I 利用朴素的方法，计算P(Document|category)"></a>Step I 利用<strong>朴素</strong>的方法，计算P(Document|category)</h3><p>P(Document|category)即：指定分类后，出现该文档的概率。<br>为了使用朴素贝叶斯分类器，首先我们需要确定整篇文档属于给定分类的概率。正如此前讨论过的，我们需要假设<strong>概率的彼此独立性</strong>，即：可以通过将所有的概率相乘，计算出总的概率值。<br>例如：假设我们注意到有20%的“bad”类文档中出现了单词“Python”——Pr(Python|Bad)=0.2；同时有80%的“bad”类的文档出现了单词“casino”（赌场），即Pr(Casino|Bad)=0.8。两个单词同时出现在同一个“bad”类文档的概率为Pr(Python&amp;Casino|bad)=0.8×0.2=0.16。从中我们会发现，计算整篇文档的概率，只需将该篇文档中含有的不同的单词的概率相乘即可。<br>新建一个classifier的子类，取名naivebayes，并为其添加一个docprob方法，该方法的作用是提取特征（不同单词），并将所有单词的概率值相乘以求出整体概率:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">naivebayes</span><span class="params">(classifier)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">docprob</span><span class="params">(self,item,cat)</span>:</span></span><br><span class="line">    features = self.getfeatures(item)</span><br><span class="line">    <span class="comment">#将所有特征的概率相乘</span></span><br><span class="line">    p=<span class="number">1</span> </span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> features:</span><br><span class="line">      p*=self.weightedprob(f,cat,self.fprob)</span><br><span class="line">    <span class="keyword">return</span> p</span><br></pre></td></tr></table></figure><p></p><p>现在我们已经知道了如何计算Pr(Document|Category)，即一个给定的概率，文档中出现这些单词组合的可能性。<br>做到这一步还不够，为了对文档进行分类，我们真正需要的是Pr(Category|Document)。换言之，就是对于一篇给定的文档，它属于某个分类的概率是多少？ 如何进行换算呢？一个名叫Thomas Bayes的英国数学家早在大于250年前就已经找到了解决这一问题的办法。</p><h3 id="Step-II-利用贝叶斯定理求P-Category-Document"><a href="#Step-II-利用贝叶斯定理求P-Category-Document" class="headerlink" title="Step II 利用贝叶斯定理求P(Category|Document)"></a>Step II 利用贝叶斯定理求P(Category|Document)</h3><p>贝叶斯定理是一种对条件概率进行调换求解的方法，意思是通过P(B|A)来求P(A|B)，而A|B和B|A正好是A和B的位置对调了一下。</p><blockquote><p>Pr(A|B)=Pr(B|A)×Pr(A)/Pr(B)</p></blockquote><p>在本例中，即为：</p><blockquote><p>Pr(Category|Document) = Pr(Document|Category)×Pr(Category)/Pr(Document)</p></blockquote><p>Pr(Document|Category)的计算方法上一节已经介绍过了，但是等式中的Pr(Category)和Pr(Document)应该如何计算呢？<br>Pr(Category)是随机选择一篇文档属于该分类的概率，就是该分类的文档数除上文档的总数。<br>至于Pr(Document)，我们也可以计算它，但这将会是一下不必要的工作。请记住，我们不会将这一计算结果当做真实的概率值。相反，我们会计算每个分类的概率，然后对各个分类的计算结果进行比较，选最有可能的分类作为分类结果。<br>由于不论计算的是哪个分类，Pr(Document)的值都是一样的，其对结果所产生的影响也完全是一样的，因此我们完全可以忽略这一项。<br>prob方法用于计算属于某个分类的概率，并返回Pr(Document|Category)与Pr(Category)的乘积。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prob</span><span class="params">(self,item,cat)</span>:</span></span><br><span class="line">  catprob = self.catcount(cat)/self.totalcount()</span><br><span class="line">  docprob = self.docprob(item,cat)</span><br><span class="line">  <span class="keyword">return</span> docprob*catprob</span><br></pre></td></tr></table></figure><p></p><p>请在Python的执行环境下尝试一下该函数，看看针对不同的字符串和分类，概率值是如何计算的：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cl=naivebayes(docclass.getwords) </span><br><span class="line">sampletrain(cl)</span><br><span class="line">cl.prob(<span class="string">'quick rabbit'</span>,<span class="string">'good'</span>) </span><br><span class="line"><span class="comment">#0.15624999999999997</span></span><br><span class="line">cl.prob(<span class="string">'quick rabbit'</span>,<span class="string">'bad'</span>) </span><br><span class="line"><span class="comment">#0.050000000000000003</span></span><br></pre></td></tr></table></figure><p></p><p>计算过程如下:<br>任意一篇文档属于good的概率:3/5<br>quick单词出现在good类文档中的加权概率:(3×(2/3)+1×0.5)/(3+1)<br>rabiit单词出现在good分类文档的加权概率为：(1×(1/3)+1×0.5)/(1+1)<br>则出现quick rabbit字符串，属于good分类的概率为前面三个概率相乘=0.156</p><p>任意一篇文档属于bad的概率:2/5<br>quick单词出现在bad类文档中的加权概率:(3×(1/2)+1×0.5)/(3+1)<br>rabiit单词出现在bad分类文档的加权概率为：(1×(0)+1×0.5)/(1+1)<br>则出现quick rabbit字符串，属于bad分类的概率为前面三个概率相乘=0.05<br>根据不同分类值的比较，”quick rabbit”属于good的概率高于输入bad分类的可能性，故它应该被划归为good分类。</p><h2 id="分类结果的选择"><a href="#分类结果的选择" class="headerlink" title="分类结果的选择"></a>分类结果的选择</h2><p>构造朴素贝叶斯分类器的最后一个步骤是实际判定某个内容项属于哪个分类。此处最简单的方法，是计算被考査内容在每个不同分类中的概率，然后选择概率最大的分类作为分类结果。如果我们只是在试图判断”将内容放到哪里最合适”的问题，那么这不失为一种可行的策略，但是在许多应用中，我们无法将各个分类同等看待，而且在一些应用中，对于分类法而言，承认不知道答案，要好过把概率值最大值的那个分类作为结果的情况。</p><p>在垃圾信息过滤的例子中，避免将普通邮件错当成垃圾邮件要比截获每一封垃圾邮件更为重要。收件箱中偶尔收到几封垃圾邮件还是可以容忍的，但是一封重要的邮件则有可能会因为自动过滤到废件箱而被完全忽视。假如我们必须在废件箱中找回自己的重要邮件，那就真的没必要再使用垃圾信息过滤器了。</p><p>为了解决这一问题，我们可以为每个分类定义一个最小阈值。对于一封将要被划归到某个分类的新邮件而言，其概率与所有其他分类的概率相比，必须大于某个指定的数值才行。这一指定的数值就是阈值。以垃圾邮件过滤为例，假如过滤到”bad”分类的的阈值为3,则针对”bad”分类的概率就必须至少3倍于针对”good”分类的概率才行。假如针对”good”分类的阔值为1，则对于任何邮件，只要概率确实大于针对”bad”分类的概率，它就是属于”good”分类的。任何更有可能属于”bad”分类，但概率并没有超过其它分类概率3倍以上的邮件，都将被划归到“未知”分类中。<br>为了定义阈值，请修改初始化方法，在classifier中加人一个新的实例变量<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,getfeatures)</span>:</span></span><br><span class="line">  classifier.__init_ _(self,getfeatures)</span><br><span class="line">  self.thresholds=&#123;&#125;</span><br></pre></td></tr></table></figure><p></p><p>请加人几个用于设值和取值的简单方法，令其默认返回为1.0<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setthreshold</span><span class="params">(self,cat,t)</span>:</span></span><br><span class="line">  self.thresholds[cat]=t</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getthreshold</span><span class="params">(self,cat)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> cat <span class="keyword">not</span> <span class="keyword">in</span> self.thresholds: </span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span></span><br><span class="line">  <span class="keyword">return</span> self.thresholds[cat]</span><br></pre></td></tr></table></figure><p></p><p>现在，我们可以构建classify方法了。该方法将计算每个分类的概率，从中得出最大值，并将其与次大概率值进行对比，确定是否超过了规定的阈值。如果没有任何一个分类满足上述条件，方法就返回默认值。请将该方法加入classifier中:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(self,item,default=None)</span>:</span></span><br><span class="line">  probs=&#123;&#125;</span><br><span class="line">  <span class="comment">#寻找概率最大的分类</span></span><br><span class="line">  max=<span class="number">0.0</span></span><br><span class="line">  <span class="keyword">for</span> cat <span class="keyword">in</span> self.categories():</span><br><span class="line">    probs[cat]=self.prob(item,cat)</span><br><span class="line">    <span class="keyword">if</span> probs[cat]&gt;max:</span><br><span class="line">      max=probs[cat]</span><br><span class="line">      best=cat</span><br><span class="line">  <span class="comment"># 确保最大概率值超过次大概率值的指定倍数</span></span><br><span class="line">  <span class="keyword">for</span> cat <span class="keyword">in</span> probs:</span><br><span class="line">    <span class="keyword">if</span> cat==best:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">if</span> probs[cat]*self.getthreshold(best)&gt;probs[best]:</span><br><span class="line">      <span class="keyword">return</span> default</span><br><span class="line">  <span class="keyword">return</span> best</span><br></pre></td></tr></table></figure><p></p><p>大功告成！我们已经建立起一个完整的文档分类系统。通过构造不同的特征提取方法，我们还可以对系统进行扩展，以实现对任何其他内容的分类。<br>下面我们来验证一下这个分类器：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cl=naivebayes(getwords)</span><br><span class="line">sampletrain(cl)</span><br><span class="line">cl.classify(<span class="string">'quick rabbit'</span>,default=<span class="string">'unknown'</span>)</span><br><span class="line"><span class="comment"># 'good'</span></span><br><span class="line">cl.classify(<span class="string">'quick money'</span>,default=<span class="string">'unknown'</span>)</span><br><span class="line"><span class="comment"># 'bad'</span></span><br><span class="line">cl.setthreshold(<span class="string">'bad'</span>,<span class="number">3.0</span>)</span><br><span class="line">cl.classify(<span class="string">'quick money'</span>,default=<span class="string">'unknown'</span>)</span><br><span class="line"><span class="comment"># 'unknown'</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">  docclass.sampletrain(cl)</span><br><span class="line">cl.classify(<span class="string">'quick money'</span>,default=<span class="string">'unknown'</span>)</span><br><span class="line"><span class="comment"># 'bad'</span></span><br></pre></td></tr></table></figure><p></p><p>当然，我们还可以修改这一阈值，看看对结果有何影响。一些垃圾信息过滤插件允许用户自己控制阈值。<br>这一以来，只要当前阈值会出现太多的垃圾邮件进入收件箱的情况，或者大量正常的邮件被归为垃圾邮件，我们就可以对阈值进行调整。<br>当然，对于另一些涉及文档过滤的应用而言，阈值的定义也可能不同；有时候所有分类可能是平等的，有时候将内容过滤到“未知”分类是不可接受的。<br>以R.A.Fisher的名字命名的<strong>费舍尔方法</strong>，是前面介绍的朴素贝叶斯方法的一种替代方案，它可以给出非常精确的结果，尤其适合垃圾信息过滤。</p><blockquote><p>SpamBayes，一个用python编写成的outlook插件，便采用了这一方法。</p></blockquote><p>与朴素贝叶斯计算Pr(Category|Document)的方法不同，费舍尔方法为文档的每个特征都计算了其属于不同分类的概率，即直接计算Pr(Category|feature)。然后又将这些概率组合起来，并判断是否有可能构成一个随机集合。该方法还可以得到该文档属于不同分类的概率，并将这些概率进行比较，得出最适合的分类结果。 尽管这种方法更为复杂，但是因为它在为分类选择临界值时允许更大的灵活性，还是值得一学的。</p><h2 id="案例补充：如何用朴素贝叶斯解决配对问题："><a href="#案例补充：如何用朴素贝叶斯解决配对问题：" class="headerlink" title="案例补充：如何用朴素贝叶斯解决配对问题："></a>案例补充：如何用朴素贝叶斯解决配对问题：</h2><p>假设有2个训练集：一份正常的，文档为”start end”,一份异常的”start”。<br>现在给一个字符串’start run’，计算其异常还是正常的概率？<br>现在给一个字符串’start ooo end’，计算其异常还是正常的概率？</p><h3 id="不考虑初始值优化的情况"><a href="#不考虑初始值优化的情况" class="headerlink" title="不考虑初始值优化的情况"></a>不考虑初始值优化的情况</h3><p>由于单词run，训练集中没有出现过，故属于正常和属于异常的概率都是0。如果将其初始概率视为0.5，概率才会变得可以计算。此处的用意是为了说明初始化概率的重要性。</p><h3 id="考虑初始值优化的情况"><a href="#考虑初始值优化的情况" class="headerlink" title="考虑初始值优化的情况"></a>考虑初始值优化的情况</h3><p>我们规定，没出现的单词，默认初始化概率为0.5。如果是出现过的单词，不考虑初始化概率。我们只是为了验证思路才做出这样的规定。<br>由于所有的分类都要乘上这个没出现过单词的概率，故其实不乘也是可以的。<br>对于start run，属于good类的概率为(1/2)×0.5=(1/4)，属于bad类的概率为(1/2)×0.5=(1/4)。两者的概率大小相等，应该选哪个？ 根据我们前面的例子中的参数——“一个分类的概率要大于另外一个分类概率的3倍才可以确定分类结果”。我们应该把它划归到异常分类中。<br>对于start ooo end，属于正常分类的概率为(1/2)×(1/2)<em>(1)=(1/4),属于异常分类的概率为(1/2)×(1/2)</em>(0)=0,正常分类的概率远远大于异常分类的概率，此时可以确定，“start ooo end”属于正常概率。</p><h1 id="费舍尔方法——针对特征的分类概率"><a href="#费舍尔方法——针对特征的分类概率" class="headerlink" title="费舍尔方法——针对特征的分类概率"></a>费舍尔方法——针对特征的分类概率</h1><p>前面讨论过的朴素贝叶斯过滤器，将所有Pr(feature|category)的计算结果组合起来得到整篇文档的概率，然后再对其进行对调求解来得到Pr(category|Document)。在本节中，我们将直接计算当一篇文档中出现某个特征时，该文档属于某个分类的可能性，即Pr(Category|feature)。</p><blockquote><p>比如说单词”Casino”出现在500篇文档中，其中有499篇属于”bad”分类，那么”casino”属于”bad”分类的概率将非常接近于1。</p></blockquote><p>计算Pr(Category|feature)的常见方法是：</p><blockquote><p>(具有指定特征的属于某分类的文档数)/(具有指定特征的文档总数)</p></blockquote><p>但是这个计算方法存在缺点：上述计算公式并没有考虑我们收到属于某一分类的文档可能比其他分类更多的情况。假如我们有很多”good”分类的文档，而”bad”分类的文档则很少，那么一个出现于所有”bad”类文档的单词，即使邮件的邮件内容看上去可能没有问题，该单词属于”bad”分类的概率应该会更大一些。如果我们假设”未来将会收到的文档在各个分类中的数量是接近等量的”，那么上述公式会有就会有更好的表现，因为这使得它能更有效的利用特征来识别分类。这个假设一般是不会成立的，但是我们可以考虑不同分类的文档总数，对数据做归一化处理，来改进这个公式。</p><p>考虑到不同分类样本数不均衡的情况，对数据做归一化，改进的计算公式为：</p><ul><li>属于某分类的概率clf = Pr(feature|category)</li><li>属于所有不同分类的概率freqsum=Pr(feature|category)之和</li><li>cprob = clf/(freqsum)</li></ul><p>请在docclass.py中为classifier新建一个子类，取名为fisherclassifier，并加入如下方法:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">fisherclassifier</span>:</span>(classifier):</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">cprob</span><span class="params">(self,f,cat)</span>:</span></span><br><span class="line">    <span class="comment">#特征在该分类中出现的概率</span></span><br><span class="line">    clf = self.fprob(self)</span><br><span class="line">    <span class="keyword">if</span> clf ==<span class="number">0</span>:</span><br><span class="line">      <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="comment">#特征在所有分类中出现的概率</span></span><br><span class="line">    freqsum = sum([self.fprob(f,c) <span class="keyword">for</span> c <span class="keyword">in</span> self.categories()])</span><br><span class="line">    p=clf/freqsum</span><br><span class="line">    <span class="keyword">return</span> p</span><br></pre></td></tr></table></figure><p></p><p>通过归一化来满足’各分类所包含的内容项相当的假设’，该函数返回的概率值，代表了具备指定特征的内容项属于指定分类的可能性。<br>我们来测试一下这个函数<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">c1 = fisherclassifier(getwords)</span><br><span class="line">sampletrain(c1)</span><br><span class="line">c1.cprob(<span class="string">'quick'</span>,<span class="string">'good'</span>)</span><br><span class="line"><span class="comment">#(2/3)/( (2/3)+(1/2) )=0.571</span></span><br><span class="line">c1.cprob(<span class="string">'money'</span>,<span class="string">'bad'</span>)</span><br><span class="line"><span class="comment">#(1/3)/(1/3+0/2)=1</span></span><br></pre></td></tr></table></figure><p></p><p>上述方法告诉我们，出现单词’money’时，是垃圾邮件的概率为1.0。这与训练数据是相符的，不过这种方法同样也会遇到前面小节提到的问题——因为算法接触的单词太少，所以它有可能对概率值估计过于偏激。因此，不妨和前面小节采用相同的处理方式，用一个假设的初始值概率和权重，对概率进行加权平均，即：所有的概率值均以0.5作为初始值，而随后不断的训练过程，使得概率值随着训练慢慢接近真实概率。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">c1.weightedprob(<span class="string">'money'</span>,<span class="string">'bad'</span>,c1.cprob)</span><br></pre></td></tr></table></figure><p></p><h2 id="将各个概率组合起来"><a href="#将各个概率组合起来" class="headerlink" title="将各个概率组合起来"></a>将各个概率组合起来</h2><p>现在，我们需要将对应各个特征的概率值组合起来，形成一个总的概率值。理论上，我们可以将它们连乘起来，利用相乘的结果在不同分类间进行比较。当然，由于特征并不是相互独立的，因为它们并不代表真实的概率，不过这已经比我们在前一节中构造的贝叶斯分类器要好。由于费舍尔方法返回的结果是对概率的一种更好的估计，这对于结果报告或临界判断而言是非常有价值的。<br>费舍尔方法的计算</p><h2 id="将各概率值组合起来"><a href="#将各概率值组合起来" class="headerlink" title="将各概率值组合起来"></a>将各概率值组合起来</h2><p>现在，我们需要将对应各个特征的概率值组合起来，形成一个总的概率值。理论上，我们可以将它们连乘起来，利用相乘的结果在不同的分类间进行比较。当然，由于特征不是相互独立的，因此它们并不代表真实的概率，不过这已经比我们前一节构造的贝叶斯分类器要好很多。由于费舍尔方法返回的结果是对概率的一种更佳的估计方式，这对于结果报告或临界值判断而言是非常有价值的。<br>费舍尔方法的计算过程是将所有概率相乘起来，然后取自然对数，再乘上-2。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fisherprob</span><span class="params">(self,item,cat)</span>:</span></span><br><span class="line">  <span class="comment">#将所有的概率值相乘</span></span><br><span class="line">  p = <span class="number">1</span></span><br><span class="line">  features = self.getfeatures(item)</span><br><span class="line"> <span class="keyword">for</span> i <span class="keyword">in</span> features:</span><br><span class="line">  p*=(self.weightedprob(f,cat,self.cprob))</span><br><span class="line">  <span class="comment">#取自然对数，并乘上-2</span></span><br><span class="line">  fscore = <span class="number">-2</span>*math.log(p)</span><br><span class="line">  <span class="comment">#利用倒置对数卡方函数求得概率</span></span><br><span class="line">  <span class="keyword">return</span> self.invchi2(fscore,len(features))</span><br></pre></td></tr></table></figure><p></p><p>费舍尔方法告诉我们，如果概率彼此独立且随机分布，则这一计算结果将满足卡方分布。卡方分布其实可以叫做平方和-概率密度分布。也许我们会预料到，不属于某个分类的文档，可能会包含该分类的大量概率性不相同的特征，而属于该分类的会看到属于这个分类的大量概率性的很高的特征。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">invchi2</span><span class="params">(self,chi,df)</span>:</span>   <span class="comment">#df为采样次数,这里为特征数</span></span><br><span class="line">  m = chi/<span class="number">2.0</span></span><br><span class="line">  sum = term = math.exp(-m)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,df):</span><br><span class="line">    term *= m/i</span><br><span class="line">    sum+=term</span><br><span class="line">   <span class="keyword">return</span> min(sum,<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure><p></p><p>我们来测试一下这个函数：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cl=fisherclassifier(docclass.getwords)</span><br><span class="line">sampletrain(cl)</span><br><span class="line">cl.cprob(<span class="string">'quick'</span>,<span class="string">'good'</span>)</span><br><span class="line"><span class="comment">#0.57142857142857151</span></span><br><span class="line">cl.fisherprob(<span class="string">'quick rabbit'</span>,<span class="string">'good'</span>)</span><br><span class="line"><span class="comment">#0.78013986588957995</span></span><br><span class="line">cl.fisherprob(<span class="string">'quick rabbit'</span>,<span class="string">'bad'</span>)</span><br><span class="line"><span class="comment">#0.35633596283335256</span></span><br></pre></td></tr></table></figure><p></p><p>正如我们看到的，结果总是介于0-1之间。这个结果是该文档属于某个分类的概率值。</p><h2 id="对文档进行分类"><a href="#对文档进行分类" class="headerlink" title="对文档进行分类"></a>对文档进行分类</h2><p>我们可以利用fisherprob的返回值来决定如何分类。贝叶斯过滤器的做法是某个分类的概率乘上阈值与另一个分类的概率进行比较，费舍尔方法和贝叶斯过滤器的阈值处理方面不同，此处我们可以为每个分类指定下限。<br>举个例子：我们希望“good”分类的分值小于0.2，“bad”分类的分值小于0.6的文档要被划分到“未知”分类中，我们可以将“bad”分类的阈值设置为0.6，该分类的概率值小于0.6的表示不确定；将“good”分类的阈值设置为0.2，该分类的概率值小于0.2的被视为不确定。“bad”分类的阈值比较高，”good”分类的阈值比较低，这样做可以使得正常邮件被划分到“bad”分类的概率性很小，同时也会允许少量的垃圾邮件进入到收件箱。<br>我们在fisherclassifier类中新建一个init方法，再增加一个保存临界值的变量：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,getfeatures)</span>:</span></span><br><span class="line">  classifier.__init__(self,getfeatures)</span><br><span class="line">  self.minimums=&#123;&#125;</span><br></pre></td></tr></table></figure><p></p><p>请将下属两个设置阈值和获取阈值的方法加入类中，默认值取0：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setminimum</span><span class="params">(self,cat,min)</span>:</span></span><br><span class="line">  self.minimums[cat]=min</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getminimum</span><span class="params">(self,cat)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> cat <span class="keyword">not</span> <span class="keyword">in</span> self.minimums:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span> </span><br><span class="line">  <span class="keyword">return</span> self.minimums[cat]</span><br></pre></td></tr></table></figure><p></p><p>最后，再添加一个方法，用来计算每个分类的概率，并找到超过指定下限值的最佳结果：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(self,item,default=None)</span>:</span></span><br><span class="line">  <span class="comment"># 循环遍历并寻找最佳结果</span></span><br><span class="line">  best = default</span><br><span class="line">  max=<span class="number">0.0</span></span><br><span class="line">  <span class="keyword">for</span> c <span class="keyword">in</span> self.categories():</span><br><span class="line">  <span class="comment">#确保超过阈值，并找到大于阈值的所有分类概率值中的最大值</span></span><br><span class="line">    <span class="keyword">if</span> p &gt; self.getminimum(c) <span class="keyword">and</span> p &gt;max:</span><br><span class="line">      best = c</span><br><span class="line">      max = p</span><br><span class="line">  <span class="keyword">return</span> best</span><br></pre></td></tr></table></figure><p></p><p>现在我们可以针对测试数据，利用费舍尔评价方法验证一下分类器。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sampletrain(cl)</span><br><span class="line">cl.classify(<span class="string">'quick rabbit'</span>)</span><br><span class="line"><span class="comment">#'good'</span></span><br><span class="line">cl.classify(<span class="string">'quick money'</span>)</span><br><span class="line"><span class="comment">#'bad'</span></span><br><span class="line">cl.setminimum(<span class="string">'bad'</span>,<span class="number">0.8</span>)</span><br><span class="line">cl.classify(<span class="string">'quick money'</span>)</span><br><span class="line"><span class="comment">#'good'</span></span><br></pre></td></tr></table></figure><p></p><p>此处的执行结果与朴素贝叶斯分类器的结果类似。人们相信实践中费舍尔分类器对垃圾信息的过滤效果会更好；只不过对于这样的以小组的训练数据而言，过滤效果可能不太明显。应该使用何种分类器取决于你的应用，没有一种简单的方法可以预测出什么样的分类器会更好，也没有办法预测出我们应该使用多大的临界值。我们可以多次测试这两种分类的效果、测试各种不同的设置项，根据测试结果来决定选择哪种分类器以及各种不太设置项参数的设置。</p><h1 id="将经过训练的分类器持久化"><a href="#将经过训练的分类器持久化" class="headerlink" title="将经过训练的分类器持久化"></a>将经过训练的分类器持久化</h1><p>在任何真实世界的应用中，所有的训练和分类工作都不可能完全在一次会话中完成。我们需要将训练的数据保存起来，以便下次训练的时候可以在上次训练的基础上继续进行。<br><strong>使用SQLite</strong><br>我们将使用数据库将分类器的训练信息进行持久化。python3内置了sqlite3模块来操作sqlite数据库。<br>本节中的代码将当期classifier类中所用的字典结构都替换为持久化的数据存储。我们在classifier中添加一个方法，为该分类器打开数据库，并在必要的时候执行建表操作。这些数据表和使用字典时的结构是相匹配的：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sqlite3</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setdb</span><span class="params">(self,dbfile)</span>:</span></span><br><span class="line">  self.con = sqlite3.connect(dbfile)</span><br><span class="line">  self.con.execute(<span class="string">'create table if not exists fc(feature,category,count)'</span>)</span><br><span class="line">  self.con.execute(<span class="string">'create table if not exists cc(category,count)'</span>)</span><br></pre></td></tr></table></figure><p></p><p>如果我们在打算将分类器一直到另一个数据库上，为了能够在所使用的目标系统上你能够正常运行，有可能需要修改响应的建表语句。<br>我们还需要修改用于获取和累加计数值的辅助函数：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">incf</span><span class="params">(self,f,cat)</span>:</span></span><br><span class="line">    count=self.fcount(f,cat)</span><br><span class="line">    <span class="keyword">if</span> count==<span class="number">0</span>:</span><br><span class="line">      self.con.execute(<span class="string">"insert into fc values ('%s','%s',1)"</span> </span><br><span class="line">                       % (f,cat))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      self.con.execute(</span><br><span class="line">        <span class="string">"update fc set count=%d where feature='%s' and category='%s'"</span> </span><br><span class="line">        % (count+<span class="number">1</span>,f,cat)) </span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fcount</span><span class="params">(self,f,cat)</span>:</span></span><br><span class="line">    res=self.con.execute(</span><br><span class="line">      <span class="string">'select count from fc where feature="%s" and category="%s"'</span></span><br><span class="line">      %(f,cat)).fetchone()</span><br><span class="line">    <span class="keyword">if</span> res==<span class="keyword">None</span>: <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>: <span class="keyword">return</span> float(res[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">incc</span><span class="params">(self,cat)</span>:</span></span><br><span class="line">    count=self.catcount(cat)</span><br><span class="line">    <span class="keyword">if</span> count==<span class="number">0</span>:</span><br><span class="line">      self.con.execute(<span class="string">"insert into cc values ('%s',1)"</span> % (cat))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      self.con.execute(<span class="string">"update cc set count=%d where category='%s'"</span> </span><br><span class="line">                       % (count+<span class="number">1</span>,cat))    </span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">catcount</span><span class="params">(self,cat)</span>:</span></span><br><span class="line">    res=self.con.execute(<span class="string">'select count from cc where category="%s"'</span></span><br><span class="line">                         %(cat)).fetchone()</span><br><span class="line">    <span class="keyword">if</span> res==<span class="keyword">None</span>: <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>: <span class="keyword">return</span> float(res[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p></p><p>获取所有分类列表的方法以及文档总数的方法也应该修改：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">categories</span><span class="params">(self)</span>:</span></span><br><span class="line">  cur=self.con.execute(<span class="string">'select category from cc'</span>);</span><br><span class="line">  <span class="keyword">return</span> [d[<span class="number">0</span>] <span class="keyword">for</span> d <span class="keyword">in</span> cur]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">totalcount</span><span class="params">(self)</span>:</span></span><br><span class="line">  res=self.con.execute(<span class="string">'select sum(count) from cc'</span>).fetchone();</span><br><span class="line">  <span class="keyword">if</span> res==<span class="keyword">None</span>: <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  <span class="keyword">return</span> res[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p></p><p>最后，我们需要在训练结束后天就一条提交语句，以便在所有计数值被更新之后程序能将数据存入数据库。请将下面的代码加入classifier中train方法的末尾处：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.con.commit()</span><br></pre></td></tr></table></figure><p></p><p>大功告成！在对classifier初始化之后，我们需要调用setdb方法，并传入数据库文件的名称。所有训练数据都将被自动存入数据库中。我们可以使用上次训练的数据作为基础，来进一步训练分类器。我们还可以将该分类器的训练数据用于另外一种类型的分类器：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cl=fisherclassifier(docclass.getwords)</span><br><span class="line">cl.setdb(<span class="string">'test1.db'</span>)</span><br><span class="line">sampletrain(cl)</span><br><span class="line">cl2=naivebayes(docclass.getwords)</span><br><span class="line">cl2.setdb(<span class="string">'test1.db'</span>)</span><br><span class="line">cl2.classify(<span class="string">'quick money'</span>)</span><br><span class="line"><span class="comment">#'bad'</span></span><br></pre></td></tr></table></figure><p></p><h1 id="对特征检测的改进"><a href="#对特征检测的改进" class="headerlink" title="对特征检测的改进"></a>对特征检测的改进</h1><p>目前为止所有的例子中，建立特征列表的函数只是简单的使用了非字母非数字类字符作为分隔符对单词进行拆分。函数还将所有的单词都转换成了小写形式，因此我们没有办法检测大写单词的过度使用问题。有几种不同的方法可以对其加以改进。</p><ul><li>不真正的区分大写和小写的单词，而是将“含有很多大写单词”这样的现象作为一种特征。</li><li>除了单个单词之外，还可以使用词组作为特征。</li><li>捕获更多的元信息（信息的相关信息），如：是谁发送了电子邮件，或者一篇博客被提交到了哪个分类下，可以用这样的信息标示为元信息。</li><li>保持URL和数字原封不动，不对其进行拆分。</li></ul><p>请记住，这不仅是让特征更有针对性这么简单。特征必须出现于多篇文档之后，因为它们对分类器而言起了很大的作用。</p><p>classifier类可以接受一个特征提取函数作为参数，然后调用传入的函数对内容进行特征抽取。<br>我们可以将特征抽取编写的更好一些。例如：我们可以接收更多的信息作为特征，相对于只考虑内容，我们还可以考虑标题、作者、摘要。我们还可以对文章的标题和文章内容区别对待。另外，对于作者区域的单词拆分是毫无意义的，如”John Smith”。<br>我们来编写这个新的特征提取函数。请注意，它需要一个订阅源中的一个项作为参赛，而非字符串：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">entryfeatures</span><span class="params">(entry)</span>:</span></span><br><span class="line">  splitter = re.compile(<span class="string">'\\W*'</span>)</span><br><span class="line">  f=&#123;&#125;</span><br><span class="line">  <span class="comment">#提取标题中的单词并进行标示</span></span><br><span class="line">  titlewords = [s.lower() <span class="keyword">for</span> s <span class="keyword">in</span> splitter.split(entry[<span class="string">'title'</span>]) <span class="keyword">if</span> len(s)&gt;<span class="number">2</span> <span class="keyword">and</span> len(s)&lt;<span class="number">20</span>]</span><br><span class="line">  <span class="keyword">for</span> w <span class="keyword">in</span> titlewords:</span><br><span class="line">    f[<span class="string">'Title:'</span>+w]=<span class="number">1</span></span><br><span class="line">  <span class="comment">#提取摘要中的单词</span></span><br><span class="line">  summarywords= [s.lower() <span class="keyword">for</span> s <span class="keyword">in</span> splitter.split(entry[<span class="string">'summary'</span>]) <span class="keyword">if</span> len(s)&gt;<span class="number">2</span> <span class="keyword">and</span> len(s)&lt;<span class="number">20</span>]</span><br><span class="line">  <span class="comment">#统计大写单词</span></span><br><span class="line">  uc=<span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(summarywords)):</span><br><span class="line">    w = summarywords[i]</span><br><span class="line">    f[w]=<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> w.issupper():</span><br><span class="line">      uc+=<span class="number">1</span></span><br><span class="line">    <span class="comment">#从摘要中获得词组作为特征</span></span><br><span class="line">    <span class="keyword">if</span> i&lt;len(summarywords)<span class="number">-1</span>:</span><br><span class="line">      twowords= <span class="string">' '</span>.join(summarywords[i:i+<span class="number">1</span>])</span><br><span class="line">      f[twowords]=<span class="number">1</span></span><br><span class="line">  <span class="comment">#保持文章发布者名字的完整性</span></span><br><span class="line">  f[<span class="string">'Publisher:'</span>+entry[<span class="string">'publisher'</span>]]=<span class="number">1</span></span><br><span class="line">  <span class="comment"># UPPERCASE是一个“虚拟”的单词，用来指示存在过多的大写内容</span></span><br><span class="line">  <span class="keyword">if</span> float(uc)/len(summarywords)&gt;<span class="number">0.3</span>:</span><br><span class="line">    f[<span class="string">'UPPERCASE'</span>]=<span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> f</span><br></pre></td></tr></table></figure><p></p><p>上述函数从文档和摘要中提取单词，就像之前的getwords函数那样。它会对不同位置的文本做不同的处理。</p><ul><li>对于标题中，只提取单词</li><li>对于摘要，考虑单词和词组</li><li>对于文档发布者，不进行拆分</li></ul><p>该函数还统计摘要中大写单词出现的比例，如果有30%的单词为大写，我们便会加入一个叫做”UPPERCASE”的虚拟单词作为特征。我们并不会直接指定大写单词这个规则，而是将这一情况当做了一个附加的特征(单词)，分类器可以利用该特征来进行训练——在某些场合下，分类器也可能会认为这一特征对文档分类而言是没有用处的。</p><blockquote><p>在实践中的思考：对于软件的日志信息，其通常是以行为单位，即一次写一行，这时候，使用一行字符串作为特征是比较好的，因为这一行的单词之间，有很强的关联性，拆成单词反而会打破这种关联性。</p></blockquote><h1 id="使用Akismet"><a href="#使用Akismet" class="headerlink" title="使用Akismet"></a>使用Akismet</h1><p>Akismet 与本章介绍的有关文本分类算法的研究稍微有些偏离，不过对于特定类型的应用而言，使用Akismet可以花费最小的代价满足你对垃圾信息过滤的需要，同时也免去了自己构造分类器的需要。<br>Akismet是作为WordPress的一个插件发展而来的，它允许人们向其报告自己博客上的垃圾评论，并与他人报告的垃圾评论进行相似度对比，对新提交的评论进行过滤。目前这些API是开方的，因此我们可以向Akismet发起任何的字符串请求，并获知Akismet是否认为该字符串属于垃圾信息。<br>我们要做的第一件事情是获得一个Akismet 的API密匙，可以从<a href="http://akismet.com获取到该密钥。这些密钥对于个人而言是免费的，此外还有一些针对商业用途的密钥可供选择。" target="_blank" rel="noopener">http://akismet.com获取到该密钥。这些密钥对于个人而言是免费的，此外还有一些针对商业用途的密钥可供选择。</a><br>Akismet API 是通过HTTP请求进行调用的，相应的函数库已经被写成了各种不通的语言版本。<br>我们可以通过pip install akismet 来安装这个软件包。<br>使用示例：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> akismet</span><br><span class="line">defaultkey = <span class="string">"YOURKEYHERE"</span></span><br><span class="line">pageurl=<span class="string">"http://yoururlhere.com"</span></span><br><span class="line">defaultagent=<span class="string">"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.0.7) "</span></span><br><span class="line">defaultagent+=<span class="string">"Gecko/20060909 Firefox/1.5.0.7"</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isspam</span><span class="params">(comment,author,ipaddress,agent=defaultagent,apikey=defaultkey)</span>:</span></span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    valid = akismet.verify_key(apikey,pageurl)</span><br><span class="line">    <span class="keyword">if</span> valid:</span><br><span class="line">    <span class="keyword">return</span> akismet.comment_check(apikey,pageurl,ipaddress,agent,comment_content=comment,comment_author_email=author,comment_type=<span class="string">"comment"</span>)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'Invalid key'</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"><span class="keyword">except</span> akismet.AkismetError, e:</span><br><span class="line">  <span class="keyword">print</span> (e.response, e.statuscode)</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">False</span></span><br></pre></td></tr></table></figure><p></p><p>现在，我们已经拥有了一个可以接收任何字符串的可供调用的方法，我们可以调用该函数来判断传入的字符串是否与博客评论中的内容类似。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">msg=<span class="string">'Make money fast! Online Casino!'</span></span><br><span class="line">isspam(msg,<span class="string">'spammer@spam.com'</span>,<span class="string">'127.0.0.1'</span>)</span><br><span class="line"><span class="comment"># True</span></span><br></pre></td></tr></table></figure><p></p><p>请以不同的用户名、代理、IP地址进行试验，观察结果如何变化。<br>由于Akismet的主要用途是对提交到博客上的垃圾评论进行判断，因此它也许并不适合处理其他类型的文档，如电子邮件。而且，与前面介绍的分类器不同，它不允许你对传入的参数做任何的调整，我们无法得知其内部的具体计算过程。不过，Akismet对于垃圾评论的过滤还是非常准确的，而且加入我们的应用正在不断的遭受到相似种类的垃圾信息的骚扰，那么Akismet是非常值得一试的，因为与我们用来分类器的数据集数量相比，Akismet拥有一个相当巨大的、用来作对比的文档集。</p><h1 id="替代方法"><a href="#替代方法" class="headerlink" title="替代方法"></a>替代方法</h1><p>本章中介绍的两个分类器都是<strong>监督型学习方法</strong>的例子，这是一种利用标注数据进行训练，使得分类器能更准确的进行预测的方法。前面讲搜索引擎时用到的人工神经网络也是一个<strong>监督型学习</strong>的例子。通过将特征作为输入，另每一个输出节点代表一种可能的分类，我们也可以用神经网络用于本章中的相同问题。同样的，后面介绍的<strong>支持向量机</strong>，也可以用来解决本章中的问题。<br>贝叶斯分类器之所以经常被用于文档分类是原因是，与其他方法相比它所要求的计算资源更少。一封电子邮件可能包含数百甚至数千个单词，与训练相应规模大小的神经网络相比，简单的更新一下计算值所占用的内存资源和处理器时钟周期会更少。而且正如你所看到的，这些工作完全可以在一个数据库中完成。神经网络是否会成为一种可行的替代方案，取决于训练和查询所要求的速度，以及实际运行的环境。神经网络的复杂性导致了其在理解上的困难。在本章中，我们可以清楚地看到概率，以及它们对最终分值的实际贡献有多大，而对于神经网络中两个神经元之间的连接强度而言，并不存在同样简单的解释。<br>另一方面，与本章中所介绍的分类器相比，神经网络和支持向量机有一个很大的优势：它们可以捕捉到输入特征之间更为复杂的关系。在贝叶斯分类器中，每个特征都有一个针对各个分类的概率值，将这些概率组合起来之后就得到了一个整体上的概率值。在神经网络中，某个特征的概率可能会依据其它特征的存在或缺失而改变，比如：你正在试图阻止赌博(casino)的垃圾信息，但是有很喜欢跑马(horse)，这种情况下，只有电子邮件中没有出现“horse”时，”casino”才会被认为是”bad”。朴素贝叶斯分类器无法捕获这样的相互依赖性，而神经网络却是可以的。</p><h1 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h1><ol><li>改变假设概率 请修改classifier类，使其能够支持针对不同的特征可以设置不同的假设概率。修改init方法，使其能够接受其它分类器的不同特征的概率值作为初始化参数，从一个更加合理的假设概率推测值开始，而不是0.5。<br>略</li><li>计算Pr(Document) 在朴素贝叶斯分类器中，Pr(Document)已经被略算过了，因为它对比较概率值而言并不是必需的。在特征彼此独立的前提下，事实上利用Pr(Document)来计算整体概率值是可行的。应该如何计算Pr(Document)呢？<br>即为这个温度中的各个单词出现的概率相乘，一个单词的出现概率计算方法为：该单词的出现次数除上所有的单词出现次数。</li><li>POP-3 电子邮件过滤器 Python有一个用于下载电子邮件的库，叫poplib。请编写一段脚本，从服务器下载电子邮件，并尝试对其进行分类。一封电子邮件包含有哪些不同的属性？你将如何利用这些属性来构建特征提取函数呢?<br>略</li><li>任意长度的短语 本章为你示范了提取词组和各个单词的方法。请修改代码令特征提取过程变得可配置，使其能够一次提取出一组拥有指定数量的单词，并将之作为一个独立的特征。<br>略</li><li>保留IP 地址 IP地址、电话号码、以及其他数字信息可能有助于对垃圾信息的识别。请修改特征提取函数，使其将这些信息作为特征加以返回(IP 地址中包含点号作为间隔符，点号不可以被剔除，但是文本中的点号需要去除)<br>略</li><li>其他虚拟特征 有许多像UPPERCASE那样的虚拟特征，这些特征可能对文档分类很有帮助。篇幅过长的文档或者长单词占有优势的情况也有可能是一种线索。请将这些情况也作为特征。你还能想到其他情况吗？<br>略</li><li>神经网络分类器 请修改搜索引擎那一章的神经网络代码，利用它对文档进行分类。神经网络的输出结果直接应该如何做比较？ 请编写一个程序对文档进行分类，并对其进行上千次的训练。计算每一种算法的执行所需要的时间。如何对这些算法做出对比呢？<br>略<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><a href="http://www.ruanyifeng.com/blog/2013/12/naive_bayes_classifier.html?bsh_bid=1598724030" target="_blank" rel="noopener">朴素贝叶斯分类器的应用-阮一峰</a></li></ol></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>如果您觉得读完本文有收获，不妨小额赞助我一下，让我有动力继续写出高质量的教程！</div><button id="rewardButton" disable="enable"><span>打赏</span></button><div id="QR" style="display:block"><div id="wechat" style="display:inline-block"><img id="wechat_qr" src="/images/smacker.jpg" alt="倔强的土豆 微信支付"><p>微信支付</p></div></div></div></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2018/01/07/optimization/" rel="next" title="优化"><i class="fa fa-chevron-left"></i> 优化</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2018/02/02/decision-tree/" rel="prev" title="决策树建模">决策树建模 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div><div class="comments" id="comments"><div id="gitment-container"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div id="sidebar-dimmer"></div><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">倔强的土豆</p><p class="site-description motion-element" itemprop="description">分享机器学习、深度学习的点滴</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">9</span> <span class="site-state-item-name">日志</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/laiqun" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:laiqun@msn.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#文档过滤"><span class="nav-number">1.</span> <span class="nav-text">文档过滤</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#过滤垃圾信息——朴素贝叶斯方法"><span class="nav-number">2.</span> <span class="nav-text">过滤垃圾信息——朴素贝叶斯方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#特征抽取-将文档拆成单词"><span class="nav-number">2.1.</span> <span class="nav-text">特征抽取-将文档拆成单词</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#对分类器进行训练"><span class="nav-number">2.2.</span> <span class="nav-text">对分类器进行训练</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#计算指定类别后，出现该单词的概率"><span class="nav-number">2.3.</span> <span class="nav-text">计算指定类别后，出现该单词的概率</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#一个合理的推测的初始值"><span class="nav-number">2.4.</span> <span class="nav-text">一个合理的推测的初始值</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#朴素贝叶斯分类器概念"><span class="nav-number">2.5.</span> <span class="nav-text">朴素贝叶斯分类器概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Step-I-利用朴素的方法，计算P-Document-category"><span class="nav-number">2.5.1.</span> <span class="nav-text">Step I 利用朴素的方法，计算P(Document|category)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Step-II-利用贝叶斯定理求P-Category-Document"><span class="nav-number">2.5.2.</span> <span class="nav-text">Step II 利用贝叶斯定理求P(Category|Document)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#分类结果的选择"><span class="nav-number">2.6.</span> <span class="nav-text">分类结果的选择</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#案例补充：如何用朴素贝叶斯解决配对问题："><span class="nav-number">2.7.</span> <span class="nav-text">案例补充：如何用朴素贝叶斯解决配对问题：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#不考虑初始值优化的情况"><span class="nav-number">2.7.1.</span> <span class="nav-text">不考虑初始值优化的情况</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#考虑初始值优化的情况"><span class="nav-number">2.7.2.</span> <span class="nav-text">考虑初始值优化的情况</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#费舍尔方法——针对特征的分类概率"><span class="nav-number">3.</span> <span class="nav-text">费舍尔方法——针对特征的分类概率</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#将各个概率组合起来"><span class="nav-number">3.1.</span> <span class="nav-text">将各个概率组合起来</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#将各概率值组合起来"><span class="nav-number">3.2.</span> <span class="nav-text">将各概率值组合起来</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#对文档进行分类"><span class="nav-number">3.3.</span> <span class="nav-text">对文档进行分类</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#将经过训练的分类器持久化"><span class="nav-number">4.</span> <span class="nav-text">将经过训练的分类器持久化</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#对特征检测的改进"><span class="nav-number">5.</span> <span class="nav-text">对特征检测的改进</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#使用Akismet"><span class="nav-number">6.</span> <span class="nav-text">使用Akismet</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#替代方法"><span class="nav-number">7.</span> <span class="nav-text">替代方法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#练习"><span class="nav-number">8.</span> <span class="nav-text">练习</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考文献"><span class="nav-number">9.</span> <span class="nav-text">参考文献</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">倔强的土豆</span></div><div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div><span class="post-meta-divider">|</span><div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.3</div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script><link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css"><script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script><script type="text/javascript">function renderGitment(){var a=new Gitmint({id:window.location.pathname,owner:"laiqun",repo:"laiqun.github.io",lang:navigator.language||navigator.systemLanguage||navigator.userLanguage,oauth:{client_secret:"55aaeb736714431ea52109dd66461b1644ca6177",client_id:"c90dfa80285ea91b9120"}});a.render("gitment-container")}renderGitment()</script></body></html><!-- rebuild by neat -->