<!-- build time:Sun Feb 25 2018 20:37:16 GMT+0800 (CST) --><!DOCTYPE html><html class="theme-next mist" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3"><link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222"><meta name="keywords" content="Hexo, NexT"><meta name="description" content="到目前为止，我们已经考察过了一部分分类器，其中大多数都非常适合对未知数据的所属分类进行预测。但是，在利用多种不同的属性（比如价格、大小）对数值型数据进行预测时，贝叶斯分类器、决策树、以及支持向量机都不是最佳的算法。本章我们将对一系列的算法进行考查：这些算法可以接受训练，根据之前见过的样本数据做出数值类的预测。而且它们还"><meta property="og:type" content="article"><meta property="og:title" content="构建价格模型"><meta property="og:url" content="laiqun.github.io/2018/02/23/buildingPriceModels/index.html"><meta property="og:site_name" content="广阔天地，大有作为"><meta property="og:description" content="到目前为止，我们已经考察过了一部分分类器，其中大多数都非常适合对未知数据的所属分类进行预测。但是，在利用多种不同的属性（比如价格、大小）对数值型数据进行预测时，贝叶斯分类器、决策树、以及支持向量机都不是最佳的算法。本章我们将对一系列的算法进行考查：这些算法可以接受训练，根据之前见过的样本数据做出数值类的预测。而且它们还可以显示出预测的概率分布情况，以帮助用户对预测过程加以解释。在本章中，我们将考察"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="/2018/02/23/buildingPriceModels/few.jpg"><meta property="og:image" content="/2018/02/23/buildingPriceModels/more.jpg"><meta property="og:image" content="/2018/02/23/buildingPriceModels/inverse.jpg"><meta property="og:image" content="/2018/02/23/buildingPriceModels/subtract.jpg"><meta property="og:image" content="/2018/02/23/buildingPriceModels/gaussian.jpg"><meta property="og:image" content="/2018/02/23/buildingPriceModels/diff.jpg"><meta property="og:image" content="/2018/02/23/buildingPriceModels/scale1.jpg"><meta property="og:image" content="/2018/02/23/buildingPriceModels/scale1.jpg"><meta property="og:updated_time" content="2018-02-25T12:36:46.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="构建价格模型"><meta name="twitter:description" content="到目前为止，我们已经考察过了一部分分类器，其中大多数都非常适合对未知数据的所属分类进行预测。但是，在利用多种不同的属性（比如价格、大小）对数值型数据进行预测时，贝叶斯分类器、决策树、以及支持向量机都不是最佳的算法。本章我们将对一系列的算法进行考查：这些算法可以接受训练，根据之前见过的样本数据做出数值类的预测。而且它们还可以显示出预测的概率分布情况，以帮助用户对预测过程加以解释。在本章中，我们将考察"><meta name="twitter:image" content="/2018/02/23/buildingPriceModels/few.jpg"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",version:"5.1.3",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!0},fancybox:!0,tabs:!0,motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="laiqun.github.io/2018/02/23/buildingPriceModels/"><title>构建价格模型 | 广阔天地，大有作为</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">广阔天地，大有作为</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">你看到我的筋斗云了嘛？</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="laiqun.github.io/2018/02/23/buildingPriceModels/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="倔强的土豆"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="广阔天地，大有作为"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">构建价格模型</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-23T22:07:14+08:00">2018-02-23 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/02/23/buildingPriceModels/#comments" itemprop="discussionUrl"><span class="post-comments-count gitment-comments-count" data-xid="/2018/02/23/buildingPriceModels/" itemprop="commentsCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><p>到目前为止，我们已经考察过了一部分分类器，其中大多数都非常适合对未知数据的所属<strong>分类</strong>进行预测。但是，在利用多种不同的属性（比如价格、大小）对<strong>数值型数据</strong>进行预测时，贝叶斯分类器、决策树、以及支持向量机都不是最佳的算法。本章我们将对一系列的算法进行考查：这些算法可以接受训练，根据之前见过的样本数据做出<strong>数值类</strong>的预测。而且它们还可以显示出预测的概率分布情况，以帮助用户对预测过程加以解释。<br>在本章中，我们将考察如何利用这些算法来构造价格预测模型。经济学家认为，价格（尤其是拍卖价格）是一种利用集体智慧来决定商品真实价值的非常好的方法；在一个拥有众多买家和卖家的大型市场中，通常对于交易双方而言，商品的价格最终将会达到一个最优值。与此同时，对价格进行预测也是测试此类算法的一种很好的手段，因此在确定价格时，通常有许多不同的因素需要考虑。例如,当我们打算竞价拍卖一台笔记本电脑时，需要考虑处理器的速度、RAM容量、硬盘大小、屏幕分辨率、以及骐达许多因素。<br>进行数值型预测的一项关键工作是确定哪些变量是重要的，以及如何将它们组合在一起。<br>在笔记本电脑的例子中，可能有一些变量即使对价格会产生一定的影响，其影响也几乎是微乎其微的，例如，赠送品、捆绑销售的软件。而且与硬盘大小相比，屏幕尺寸对最终价格所产生的影响可能更大一些。我们可以利用前面介绍的优化技术，自动确定各个变量的最佳权重。</p><h1 id="构造一个样本数据集"><a href="#构造一个样本数据集" class="headerlink" title="构造一个样本数据集"></a>构造一个样本数据集</h1><p>一个极富挑战的测验数值型预测算法的数据集应该具备某些特征，这些特征的存在会使得算法难以对数据做出预测。举例来说，如果你正打算购买电视机，那么很容易得到尺寸越大越好的结论，而这样的问题利用传统的统计技术来解决会更容易一些。因此，我们去考察那些价格并非简单的按照商品尺寸或特征数量的增长而价格成比例增长的数据集，这会更有挑战性。<br>在本节中，我们将根据一个假设的简单模型来构造一个有关葡萄酒价格的数据集。酒的价格是由酒的等级和储藏的年代共同决定的。该模型假设葡萄酒有“峰值年(peak age)”的现象，即相对于峰值年而言，年代稍早的品质会好一些，但品质不如峰值年；年代晚于峰值年的品质会急剧下降。一瓶高等级的葡萄酒从高价位开始，价格逐渐升高直到“峰值年”；而一瓶低等级的葡萄酒则会从一个低价开始，价格一路走低，即峰值年为生产出该酒的年。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> random,randint</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wineprice</span><span class="params">(rating,age)</span>:</span></span><br><span class="line">    peak_age = rating<span class="number">-50</span> <span class="comment">#峰值年与酒的品质有关</span></span><br><span class="line">    <span class="comment"># 根据等级和年代来计算价格</span></span><br><span class="line">    price = rating/<span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> age &gt; peak_age:</span><br><span class="line">        <span class="comment"># 经过‘峰值年’，后继5年里其品质将会变差</span></span><br><span class="line">        price = price*(<span class="number">5</span>-(age-peak_age))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 价格在接近‘峰值年’时，价格为增加到原值的5倍</span></span><br><span class="line">        price = price*(<span class="number">5</span>*((age+<span class="number">1</span>)/peak_age))</span><br><span class="line">    <span class="keyword">if</span> price &lt;<span class="number">0</span>:</span><br><span class="line">        price =<span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> price</span><br></pre></td></tr></table></figure><p></p><p>我们还需要一个函数来批量生产数据集。下列函数生产了300瓶葡萄酒，根据模型求出这些葡萄酒的价格，然后在原有价格的基础上随机的加减了20%，以此来表现诸如税收和价格局部变动的情况，这同时也是为了增加数值预测的难度。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">winset1</span><span class="params">()</span>:</span></span><br><span class="line">    rows = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">300</span>):</span><br><span class="line">        <span class="comment">#随机生成年代和等级</span></span><br><span class="line">        rating = random()*<span class="number">50</span>+<span class="number">50</span></span><br><span class="line">        age = random()*<span class="number">50</span></span><br><span class="line">        <span class="comment">#得到一个参考价格</span></span><br><span class="line">        price = wineprice(rating,age)</span><br><span class="line">        <span class="comment">#增加“噪声”</span></span><br><span class="line">        price*=(random()*<span class="number">0.4</span>+<span class="number">0.8</span>)</span><br><span class="line">        <span class="comment">#加入数据集</span></span><br><span class="line">        rows.append(&#123;<span class="string">'input'</span>:(rating,age),<span class="string">'result'</span>:price)</span><br><span class="line">    <span class="keyword">return</span> rows</span><br></pre></td></tr></table></figure><p></p><p>我们来测试一下前面编写的两个函数:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">wineprice(<span class="number">95.0</span>,<span class="number">3.0</span>)<span class="comment">#品质为95，储藏了3年</span></span><br><span class="line"><span class="comment">#21.111</span></span><br><span class="line">wineprice(<span class="number">95.0</span>,<span class="number">8.0</span>)<span class="comment">#品质为95，储藏了8年</span></span><br><span class="line"><span class="comment">#47.5</span></span><br><span class="line">winprice(<span class="number">99.0</span>,<span class="number">1.0</span>)<span class="comment">#品质为99,储藏了1年</span></span><br><span class="line"><span class="comment">#10.10</span></span><br><span class="line">data = wineset1()</span><br><span class="line"><span class="comment">#data[0]为&#123;'input':&#123;63.6,21.5&#125;,'result':34.5&#125;</span></span><br><span class="line"><span class="comment">#data[1]为&#123;'input':&#123;74.9,48.0&#125;,'result':0.0&#125; </span></span><br><span class="line"><span class="comment">#data[1]储藏时间太久，过期了</span></span><br></pre></td></tr></table></figure><p></p><p>变量的相互作用，使得这一个数据集很适合与对算法的测试。</p><h1 id="k-近邻算法"><a href="#k-近邻算法" class="headerlink" title="k-近邻算法"></a>k-近邻算法</h1><p>对于我们的葡萄酒定价问题而言，最简单的做法与人们人们尝试手动进行定价时所采用的做法是一样的——即，找到几瓶情况最为相似的酒，并假设其价格大体相同。算法通过寻找与当前所关注的商品情况相似的一组商品，对这些商品的价格求均值，进而做出价格预测。这种方法被称为k-最近邻算法(k-nearest neighbors,kNN)。</p><h2 id="近邻数的选择"><a href="#近邻数的选择" class="headerlink" title="近邻数的选择"></a>近邻数的选择</h2><p>kNN 中k代表的，是为了求得最终结果而参与求平均运算的商品数量。对于理想情况下的数据集，我们可以令k=1，这意味着我们仅仅考虑距离最近的邻居作为参考，并将其价格视为最终答案。不过在现实世界中，总是没有那样理想化。在本例中，我们故意引入了“<strong>噪声</strong>”，来模拟这样的情况（随机的加减了20%）。由于有了这些噪声，一部分顾客可能会因此大赚一笔；而也有消息闭塞的客户，可能会为此支付更多的钱。基于这样的原因，我们最好多<strong>选取一些近邻</strong>，然后对他们<strong>取平均，以此来减少噪声</strong>。</p><h3 id="近邻数过少"><a href="#近邻数过少" class="headerlink" title="近邻数过少"></a>近邻数过少</h3><p>为了形象的说明选择过少近邻的问题，我们以存储时间为例来考虑一下只有一个描述性变量的情况。下图是一副反映价格(y轴)和存储时间(x轴)之间的关系的图。图上还标注了当只使用一个最近邻时所得到的曲线。<br><img src="/2018/02/23/buildingPriceModels/few.jpg" title="k过小-选择的近邻数过少"><br>请注意次数所预测的价格是怎样过度依赖于曲线的随机变化的。如果我们打算利用图中的曲线进行预测，那么当我们真的只关注一瓶15年的葡萄酒和一瓶16年的葡萄酒价格上的差异时，我们就会得出结论，认为这两瓶葡萄酒在价格上会存在一个大的跳跃。</p><h3 id="近邻数过多"><a href="#近邻数过多" class="headerlink" title="近邻数过多"></a>近邻数过多</h3><p>选择过多的近邻同样会降低准确性，因为算法会对那些<strong>与被查询的商品根本没有任何相似性的商品求平均</strong>。下图为取20个近邻求平均后得到的曲线。<br><img src="/2018/02/23/buildingPriceModels/more.jpg" title="k过大-选择的近邻数过多"><br>很显然，对太多的葡萄酒价格取平均，就会极大地低估25年左右的葡萄酒对预估的价格造成的影响。<br>为了选择合适的近邻数，我们可以针对不同的数据集加以手工选择，或者采用一些优化措施。</p><h2 id="定义相似度的度量方法"><a href="#定义相似度的度量方法" class="headerlink" title="定义相似度的度量方法"></a>定义相似度的度量方法</h2><p>对于kNN算法，我们首先要做的一件事情是，寻找一种衡量两件物品之间相似度的方法。<br>我们已经在本书中学到了各种不同的度量方法。此处，我们将选用欧几里得距离作为度量算法。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">euclidean</span><span class="params">(v1,v2)</span>:</span></span><br><span class="line">    d=<span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(v1)):</span><br><span class="line">        d+=(v1[i]-v2[i])**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> math.sqrt(d)</span><br></pre></td></tr></table></figure><p></p><p>我们来测试一下这个函数:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#data[0]['input']=&#123;82.7,49.2&#125;</span></span><br><span class="line"><span class="comment">#data[1]['input']=&#123;98.9,25.7&#125;</span></span><br><span class="line">euclidean(data[<span class="number">0</span>][<span class="string">'input'</span>],data[<span class="number">1</span>][<span class="string">'input'</span>])</span><br><span class="line"><span class="comment">#28.5</span></span><br></pre></td></tr></table></figure><p></p><p>你会注意到，该函数在计算距离时，对年代和品质是<strong>同等对待</strong>的，但现实的情况是，某些变量对最终价格所产生的影响往往会比其他变量更大。这是kNN众所周知的一个缺点，而解决这一问题的方法会在后续部分<strong>处理不同类型的变量</strong>中。</p><h2 id="k最近邻算法的代码"><a href="#k最近邻算法的代码" class="headerlink" title="k最近邻算法的代码"></a>k最近邻算法的代码</h2><p>kNN是一种实现起来相对简单的算法，虽然这种算法的计算量很大，其优点在于其没有<strong>训练</strong>的过程，即有新数据加入数据集时，不需要训练。<br>我们来编写一个函数，利用该函数来计算给定商品与数据集中的所有商品之间的距离：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getdistances</span><span class="params">(data,vec1)</span>:</span><span class="comment"># vec1为给定商品,data为数据集</span></span><br><span class="line">    distancelist=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data)):</span><br><span class="line">        vec2= data[i][<span class="string">'input'</span>]<span class="comment">#从数据集中取出一项，命名为vec2</span></span><br><span class="line">        distacelist.append((euclidean(vec2,vec1),i))</span><br><span class="line">    distancelist.sort()</span><br><span class="line">    <span class="keyword">return</span> distancelist</span><br></pre></td></tr></table></figure><p></p><p>该函数针对指定向量，与数据集中的任何一个向量计算距离，并将结果放到了一个大列表中。为了让距离最近者位于最前端，我们对列表进行了排序。<br>kNN函数利用了上述距离列表，并对其中的前k项求平均值。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">knnestimate</span><span class="params">(data,vec1,k=<span class="number">5</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 得到经过排序的距离值</span></span><br><span class="line">    dlist = getdistances(data,vec1)</span><br><span class="line">    avg = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># 对前k项结果求平均</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        index = dlist[i][<span class="number">1</span>]</span><br><span class="line">        avg+=data[index][<span class="string">'result'</span>]</span><br><span class="line">    avg=avg/k</span><br><span class="line">    <span class="keyword">return</span> avg</span><br></pre></td></tr></table></figure><p></p><p>现在我们队新商品进行估价来测试一下上述函数:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">knnestimate(data,(<span class="number">95.0</span>,<span class="number">3.0</span>))</span><br><span class="line"><span class="comment">#29.17</span></span><br><span class="line">knnestimate(data,(<span class="number">99.0</span>,<span class="number">3.0</span>))</span><br><span class="line"><span class="comment">#22.35</span></span><br><span class="line">knnestimate(data,(<span class="number">99.0</span>,<span class="number">5.0</span>))</span><br><span class="line"><span class="comment">#37.6</span></span><br><span class="line">wineprice(data,(<span class="number">99.0</span>,<span class="number">5.0</span>))<span class="comment">#得到实际价格,注意与上述预测的价格做比较</span></span><br><span class="line"><span class="comment">#30.30 </span></span><br><span class="line">knnestimate(data,(<span class="number">99.0</span>,<span class="number">5.0</span>),k=<span class="number">1</span>)<span class="comment">#尝试使用更少的近邻</span></span><br><span class="line"><span class="comment"># 38.07</span></span><br></pre></td></tr></table></figure><p></p><p>请尝试不同的输入参数和不同的k值，看一看它们对结果产生的影响。</p><h2 id="加权kNN"><a href="#加权kNN" class="headerlink" title="加权kNN"></a>加权kNN</h2><p>目前我们所使用的算法有可能会选择距离过远的近邻，对于这样的情况，一种补偿的办法是根据距离的远近来确定每个近邻的权重。这种思想与前面所讲述的推荐算法相同，在那里我们计算一个寻求推荐的用户与其他人在偏好上的相似程度，并将相似程度作为了权重。与你相似的人权重会多一些，与你相似度较低的人，权重会少一些。</p><h3 id="为近邻分配权重"><a href="#为近邻分配权重" class="headerlink" title="为近邻分配权重"></a>为近邻分配权重</h3><p>商品越是相近，彼此间的距离就越小，我们需要一种方法来讲距离转换为权重。这里我们将介绍3种方法。<br><strong>1.倒数函数</strong><br>该函数最为简单的一种形式是返回距离的倒数。当完全一样或者非常接近的商品，会使得权重变得非常大，甚至无穷大。基于这一的原因，我们有必要在对距离求倒数的分子和分母上加上一个小小的常量。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inverserweight</span><span class="params">(dist,num=<span class="number">1.0</span>,const=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> num/(dist+const)</span><br></pre></td></tr></table></figure><p></p><p>该函数执行速度快，也易于实现，我们还可以尝试一下不同的num值，看看怎样的值效果更好一些。<br>该方法的主要缺陷在于：它会为特别相似的近邻分配很大的权重，而稍远一点的近邻，其权重“衰减”的很快。这些情况也许正是我们所期望的，但有的时候，这也会是算法对噪声变得更加敏感。<br>以下为倒数函数的示意图：<br><img src="/2018/02/23/buildingPriceModels/inverse.jpg" title="倒数函数示意图"><br><strong>2.减法函数</strong><br>这是一个简单的函数，它用一个常量值减去距离。如果相减的结果大于0，则权重为相减的结果；否则为0。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">substractweight</span><span class="params">(dist,const=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> dist&gt;const:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> const-dist</span><br></pre></td></tr></table></figure><p></p><p>该函数客户了前述对特别相似的近邻项分配过大权重的问题，但是它也有自身的局限。<br>由于权重值最终会跌至0，因此我们有可能找不到距离足够近的项，将其视为近邻，对于某些待预测的数据，可能会出现找不到“足够近似”的项而预测结果为0的情况。<br>以下为减法函数的示意图：<br><img src="/2018/02/23/buildingPriceModels/subtract.jpg" title="减法函数示意图"><br><strong>3.高斯函数</strong><br>我们来看一下高斯函数，有时候也叫它“钟型曲线”。该方法比前面提到的函数要复杂一些，不过该方法克服了前述函数的局限。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian</span><span class="params">(dist,sigma=<span class="number">10.0</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> math.e**(-dist**<span class="number">2</span>/(<span class="number">2</span>*sigma**<span class="number">2</span>))</span><br></pre></td></tr></table></figure><p></p><p>该函数在距离为0时权重为1，并且权重值会随着距离的增加而减少。与减法函数不同的是，这里的权重值永远不会跌至0，因此该方法总是可以做出预测的。<br>以下为高斯函数的示意图：<br><img src="/2018/02/23/buildingPriceModels/gaussian.jpg" title="高斯函数示意图"></p><p>我们来测试前述的3种函数，传入不同的参数值，看一下这些方法彼此间的差异如何：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">subtractweight(<span class="number">0.1</span>)</span><br><span class="line"><span class="comment"># 0.9</span></span><br><span class="line">inverseweight(<span class="number">0.1</span>)</span><br><span class="line"><span class="comment"># 5.0</span></span><br><span class="line">gaussian(<span class="number">0.1</span>)</span><br><span class="line"><span class="comment"># 0.99501247919268232</span></span><br><span class="line">gaussian(<span class="number">1.0</span>)</span><br><span class="line"><span class="comment"># 0.60653065971263342</span></span><br><span class="line">subtractweight(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 0.0</span></span><br><span class="line">inverseweight(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 0.90909090909090906</span></span><br><span class="line">gaussian(<span class="number">3.0</span>)</span><br><span class="line"><span class="comment"># 0.01110899653824231</span></span><br></pre></td></tr></table></figure><p></p><p>我们可以看到，所有的函数都是在距离为0处取得最大值，然后随着距离增加，其函数值越来越小。</p><h3 id="加权kNN的代码实现"><a href="#加权kNN的代码实现" class="headerlink" title="加权kNN的代码实现"></a>加权kNN的代码实现</h3><p>实现加权kNN算法的代码与普通的kNN函数在执行过程上是相似的，函数首先获得经过排序的距离值，选择距离最近的k个元素。<br>与普通的kNN算法不同的是，加权kNN函数算法的最重要的区别在于，它并不是对这些元素简单的求平均，它求的是加权平均。<br>加权平均的结果是通过将每一项的值乘上对应权重，然后将结果累加；求得总和后，我们再将其初上所有的权重值之和。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weightedknn</span><span class="params">(data,vec1,k=<span class="number">5</span>,weightf=gaussian)</span>:</span></span><br><span class="line">    <span class="comment"># 得到经过排序的距离值</span></span><br><span class="line">    dlist = getdistances(data,vec1)</span><br><span class="line">    avg = <span class="number">0.0</span></span><br><span class="line">    total_weight=<span class="number">0.0</span></span><br><span class="line">    <span class="comment"># 对前k项结果求平均</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        index = dlist[i][<span class="number">1</span>]</span><br><span class="line">        dist = dlist[i][<span class="number">0</span>]</span><br><span class="line">        weight = weightf(dist)</span><br><span class="line">        avg+=data[index][<span class="string">'result'</span>]*weight</span><br><span class="line">        total_weight += weight</span><br><span class="line">    avg=avg/total_weight</span><br><span class="line">    <span class="keyword">return</span> avg</span><br></pre></td></tr></table></figure><p></p><p>上述函数循环遍历距离最近的k个近邻，并将各个距离值传入预先定义好的权重函数。变量avg的值是通过将权重乘上对应数据项的数值而求得的。变量total_weight是所有权重值的总和。最后，我们将avg除上总的权重和。<br>我们来测试一下这个函数:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">weightedknn(data,(<span class="number">99.0</span>,<span class="number">5.0</span>))</span><br><span class="line"><span class="comment"># 32.6</span></span><br></pre></td></tr></table></figure><p></p><p>在本例中，通过计算所得的结果我们可以看出，weightedknn比knnestmate更接近正确答案。不过，这只是针对两组样例而言的。更加严格的测验过程需要涉及数据集中大量的数据项，测试出最佳的函数与最佳的参数。这回在<strong>交叉验证</strong>小节进行讲述。</p><h2 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h2><p>交叉验证是将数据拆分成训练集和测试机的一些列技术的统称。我们将训练集传入算法，这些训练集伴随着正确的答案（此处为价格）被输入模型，这个过程叫“训练”；训练后，我们就得到了一组可以用来进行预测的模型。随后，我们要求算法对测试集中的每一项数据做出预测，将模型给出的答案与正确的答案进行比较。将测试集比较完后，我们便可以得到评估模型准确程度的统计结果。<br>通常交叉验证会进行若干次，每次对数据的拆分都不相同。典型的情况下，测试集只会包含小部分数据，大概是所有数据的5%，剩下的95%都被用来做训练集。我们来编写函数实现对数据集的拆分。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dividedata</span><span class="params">(data,test=<span class="number">0.05</span>)</span>:</span></span><br><span class="line">    trainset=[]</span><br><span class="line">    testset=[]</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">if</span> random()&lt;test:</span><br><span class="line">            testset.append(row)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            trainset.append(row)</span><br><span class="line">    <span class="keyword">return</span> trainset,testset</span><br></pre></td></tr></table></figure><p></p><p>我们还需要一个运行模型并统计模型预测与实际值误差的函数。我们要为测试集的每一项都调用算法，得到一个预测值，将这个预测值与实际值进行比较得到一个差值结果。我们需要将测试集中每一项的差值累加起来，以此来评估预测结果与正确结果在整体上的差距。<br>我们这里使用差值的平方来表示模型预测值与实际值的偏离程度。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testalgorithm</span><span class="params">(algf,trainset,testset)</span>:</span></span><br><span class="line">    error = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> testset:</span><br><span class="line">        guess = algf(trainset,row[<span class="string">'input'</span>])</span><br><span class="line">        error += (row[<span class="string">'result'</span>]-guess)**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> error/len(testset)</span><br></pre></td></tr></table></figure><p></p><p>testalgotithm的algf接受一个算法函数作为参数，而该算法函数接受一个数据集和一个查询项作为参数。testalgorithm会循环遍历测试集中的每一行，并将algf的预测结果与实际结果相减后的平方视为误差。<br>对数字求<strong>平方</strong>法是一个常见的做法，因为它会凸显较大的误差，当误差较大时，平方后会使得误差显得更大。这意味着，一个在大多数时候都非常接近正确值，但偶尔存在较大偏离的算法，比起那些“始终都比较接近正确值的算法”要差一些。一般而言，这种情况是我们所期望的，不过也有例外，举例来说：如果一个算法在大多数情况下都非常接近正确值，偶尔犯一个大错误还是可以接受的情况，这时候我们将差的<strong>绝对值</strong>累加起来即可。</p><p>最后，我们需要一个函数来多次调用前述的拆分数据集函数与算法误差统计函数。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crossvalidate</span><span class="params">(algf,data,trials=<span class="number">100</span>,test=<span class="number">0.05</span>)</span>:</span></span><br><span class="line">    error = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(trials):</span><br><span class="line">        trainset,testset = dividedata(data,test)</span><br><span class="line">        error +=testalgorithm(algf,trainset,testset)</span><br><span class="line">    <span class="keyword">return</span> error/trials</span><br></pre></td></tr></table></figure><p></p><p>尝试使用不同的参数来测试一下这个函数,比如不同的k值：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">crossvalidate(numpredict.knnestimate,data)<span class="comment"># k=5</span></span><br><span class="line"><span class="comment"># 254.06864176819553</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">knn3</span><span class="params">(d,v)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> numpredict.knnestimate(d,v,k=<span class="number">3</span>) <span class="comment"># k=3</span></span><br><span class="line">crossvalidate(knn3,data)</span><br><span class="line"><span class="comment"># 166.97339783733005</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">knn1</span><span class="params">(d,v)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> numpredict.knnestimate(d,v,k=<span class="number">1</span>) <span class="comment"># k=1 </span></span><br><span class="line">crossvalidate(knn1,data)</span><br><span class="line"><span class="comment"># 209.54500183486215</span></span><br></pre></td></tr></table></figure><p></p><p>正如前面讨论的那样，使用过多和过少的近邻都会导致效果不好。在本例中，k=3的效果要比k=5或k=1的效果好。<br>我们也可以测试一下不同的权重函数，看看前述讨论的3种权重函数哪一个效果好一些。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">crossvalidate(numpredict.weightedknn,data)<span class="comment">#使用高斯函数</span></span><br><span class="line"><span class="comment"># 200.34187674254176</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">knninverse</span><span class="params">(d,v)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> numpredict.weightedknn(d,v,weightf=inverseweight)<span class="comment">#使用倒数函数</span></span><br><span class="line">crossvalidate(knninverse,data)</span><br><span class="line"><span class="comment"># 148.85947702660616</span></span><br></pre></td></tr></table></figure><p></p><p>待我们正确设置好参数之后，权重kNN算法似乎能够针对上述训练集给出更好的结果。选择正确的参数也许会费一些时间，但是对于一个特定的训练集而言，这样的工作只需要做一次即可。随着训练集内容的增长，偶尔我们还需要对参数进行更新。后续的<strong>对缩放结果进行优化</strong>小节中，我们将考察自动确定部分参数的方法。</p><h2 id="处理不同属性"><a href="#处理不同属性" class="headerlink" title="处理不同属性"></a>处理不同属性</h2><h3 id="增加两个新属性——酒瓶容量与生产流水线编号"><a href="#增加两个新属性——酒瓶容量与生产流水线编号" class="headerlink" title="增加两个新属性——酒瓶容量与生产流水线编号"></a>增加两个新属性——酒瓶容量与生产流水线编号</h3><p>前文所述的数据集是特意做了简化的，用来预测价格的所有属性大致上是可以比较的，而且这些属性对最终价格而言都是有意义的。<br>现在我们可以引入一个与价格完全不相关的属性，生产葡萄酒时的流水线编号，那么这一个变量也会被纳入距离计算之中。如果这样，对于其他各个属性都是完全一样的，仅仅是生产流水线编号不一样，算法给出的价格也是不同的，这种情况会使得准确度大大降低。</p><p>因为所有的变量都位于同一值域范围内，因此利用这些变量算出的距离值是有意义的。现在，我们加入一个新的属性，比如酒瓶的容量，该属性以毫升为单位。与之间所述的其他属性不同（那些属性的值域范围为0-100），这个属性的值域范围可能达到1500。很明显，新加入的酒瓶容量属性对距离计算所产生的影响将更加显著，其影响超过任何其他属性对距离计算产生的影响。这意味着，在计算距离的过程中，其它属性产生的作用可能微乎其微。如下图所示：</p><img src="/2018/02/23/buildingPriceModels/diff.jpg" title="不同的属性对距离计算的影响"><p>上图中横轴为酒瓶容量，纵轴为储藏年数。<br>我们来修改前述的数据集生成代码，增加酒瓶容量和生成流水线编号。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wineset2</span><span class="params">()</span>:</span></span><br><span class="line">    rows = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">300</span>):</span><br><span class="line">        rating = random()*<span class="number">50</span>+<span class="number">50</span></span><br><span class="line">        age = random()*<span class="number">50</span></span><br><span class="line">        aisle = float(randint(<span class="number">1</span>,<span class="number">20</span>))<span class="comment">#新加</span></span><br><span class="line">        bottlesize = [<span class="number">375.0</span>,<span class="number">750.0</span>,<span class="number">1500.0</span>,<span class="number">3000.0</span>] <span class="comment">#新加，四种酒瓶容量，随机算一个</span></span><br><span class="line">        price = wineprice(rating,age)</span><br><span class="line">        price *= (bottlesize/<span class="number">750</span>)</span><br><span class="line">        price *= (randm()*<span class="number">0.9</span>+<span class="number">0.2</span>)</span><br><span class="line">        rows.append(&#123;<span class="string">'input'</span>:(rating,age,aisle,bottlesize),<span class="string">'result'</span>:price&#125;)</span><br><span class="line">    <span class="keyword">return</span> rows</span><br></pre></td></tr></table></figure><p></p><p>测试该函数产生的数据集在现有算法上的效果：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data = wineset2()</span><br><span class="line">crossvalidate(knn3,data)</span><br><span class="line"><span class="comment"># 1427</span></span><br><span class="line">crossvalidate(weightedknn,data)</span><br><span class="line"><span class="comment"># 1195</span></span><br></pre></td></tr></table></figure><p></p><p>我们会发现，即使数据集包含了更多的信息，而且噪声也更少了，理论上，这应该得到更好的预测结果；但实际上预测效果比之前的结果更糟糕。其原因在于，算法还不知道如何对不同的属性加以区别对待。</p><h3 id="将不同值域的属性缩放到同一值域"><a href="#将不同值域的属性缩放到同一值域" class="headerlink" title="将不同值域的属性缩放到同一值域"></a>将不同值域的属性缩放到同一值域</h3><p>此处，我们不是需要一种根据属性的值来计算距离的方法，而是需要一种对数值进行<strong>归一化</strong>处理的方法，从而使所有属性都位于相同的值域范围内。做这样做也有利于查找那些不必要的属性（生成流水线编号），或者至少能降低其对距离计算的影响。一种可行的办法就是在进行任何计算之间先对数据按比例进行缩放。<br>按比例对数据进行缩放的简单形式是每个维度上的数值乘上该一个常量，如下图所示：<br><img src="/2018/02/23/buildingPriceModels/scale1.jpg" title="酒瓶容量的属性乘上(1/10)"><br>上图横轴为酒瓶容量，纵轴为储藏年数。我们可以看到，酒瓶容量被值为(1/10)的比例因子缩小了，相应的，近邻的距离计算值也会发生变化。这种做法解决了一些属性天生比其他属性更加<strong>强势</strong>问题。<br>那对于重要程度不高的属性，应该如何处置呢？ 如果我们将其乘上比例因子0，会发生怎样的变化呢，如下图所示：<br><img src="/2018/02/23/buildingPriceModels/scale1.jpg" title="通道容量属性乘上(0)"><br>上图横轴为生产流水线编号，纵轴为储藏年数。<br>我们可以发现进行距离计算时，生成流水线编号这个属性完全被忽略了，只按照存储年数来计算距离。如果所有无关紧要的变量都被缩放到了0，那么算法将会更加准确。<br>我们来编写一个函数，传入数据集和每个属性的缩放程度列表，该函数对不同的属性按照相应的缩放比例因子进行缩放。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用实例 rescale(data,[10,10,0,0.5]) </span></span><br><span class="line"><span class="comment"># [10,10,0,0.5] 分别是评级、年代、通道号、酒瓶容量的缩放比例</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rescale</span><span class="params">(data,scale)</span>:</span></span><br><span class="line">    scaleddata=[]</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> data:</span><br><span class="line">        scaled=[scale(i)*row[<span class="string">'input'</span>][i] <span class="keyword">for</span> i <span class="keyword">in</span> range(len(scale))]</span><br><span class="line">        scaleddata.append(&#123;<span class="string">'input'</span>:scaled,<span class="string">'result'</span>:row[<span class="string">'result'</span>]&#125;)</span><br><span class="line">    <span class="keyword">return</span> scaleddata</span><br></pre></td></tr></table></figure><p></p><p>我们精心挑选了一组缩放比例因子，对数据集进行按比例缩放，看看是否能得到满意的预测效果。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sdata=rescale(data,[<span class="number">10</span>,<span class="number">10</span>,<span class="number">0</span>,<span class="number">0.5</span>])</span><br><span class="line">crossvalidate(knn3,sdata)</span><br><span class="line"><span class="comment"># 660.9964024835578</span></span><br><span class="line">crossvalidate(numpredict.weightedknn,sdata)</span><br><span class="line"><span class="comment"># 852.32254222973802</span></span><br></pre></td></tr></table></figure><p></p><p>对于上述少量样例数据而言，这样的结果比之前的效果好，我们可以尝试修改一下scale列表中的参数值，看看能否取得更好的结果。</p><h3 id="对缩放因子进行优化"><a href="#对缩放因子进行优化" class="headerlink" title="对缩放因子进行优化"></a>对缩放因子进行优化</h3><p>考察自动确定部分参数的方法</p><h2 id="不对称分布"><a href="#不对称分布" class="headerlink" title="不对称分布"></a>不对称分布</h2><h1 id="何时使用k-最近邻算法"><a href="#何时使用k-最近邻算法" class="headerlink" title="何时使用k-最近邻算法"></a>何时使用k-最近邻算法</h1><h1 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h1></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>如果您觉得读完本文有收获，不妨小额赞助我一下，让我有动力继续写出高质量的教程！</div><button id="rewardButton" disable="enable"><span>打赏</span></button><div id="QR" style="display:block"><div id="wechat" style="display:inline-block"><img id="wechat_qr" src="/images/smacker.jpg" alt="倔强的土豆 微信支付"><p>微信支付</p></div></div></div></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2018/02/02/decision-tree/" rel="next" title="决策树建模"><i class="fa fa-chevron-left"></i> 决策树建模</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"></div></div></footer></div></article><div class="post-spread"></div></div></div><div class="comments" id="comments"><div id="gitment-container"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div id="sidebar-dimmer"></div><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">倔强的土豆</p><p class="site-description motion-element" itemprop="description">分享机器学习、深度学习的点滴</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">9</span> <span class="site-state-item-name">日志</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/laiqun" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:laiqun@msn.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#构造一个样本数据集"><span class="nav-number">1.</span> <span class="nav-text">构造一个样本数据集</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#k-近邻算法"><span class="nav-number">2.</span> <span class="nav-text">k-近邻算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#近邻数的选择"><span class="nav-number">2.1.</span> <span class="nav-text">近邻数的选择</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#近邻数过少"><span class="nav-number">2.1.1.</span> <span class="nav-text">近邻数过少</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#近邻数过多"><span class="nav-number">2.1.2.</span> <span class="nav-text">近邻数过多</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#定义相似度的度量方法"><span class="nav-number">2.2.</span> <span class="nav-text">定义相似度的度量方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#k最近邻算法的代码"><span class="nav-number">2.3.</span> <span class="nav-text">k最近邻算法的代码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#加权kNN"><span class="nav-number">2.4.</span> <span class="nav-text">加权kNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#为近邻分配权重"><span class="nav-number">2.4.1.</span> <span class="nav-text">为近邻分配权重</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#加权kNN的代码实现"><span class="nav-number">2.4.2.</span> <span class="nav-text">加权kNN的代码实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#交叉验证"><span class="nav-number">2.5.</span> <span class="nav-text">交叉验证</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#处理不同属性"><span class="nav-number">2.6.</span> <span class="nav-text">处理不同属性</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#增加两个新属性——酒瓶容量与生产流水线编号"><span class="nav-number">2.6.1.</span> <span class="nav-text">增加两个新属性——酒瓶容量与生产流水线编号</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#将不同值域的属性缩放到同一值域"><span class="nav-number">2.6.2.</span> <span class="nav-text">将不同值域的属性缩放到同一值域</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对缩放因子进行优化"><span class="nav-number">2.6.3.</span> <span class="nav-text">对缩放因子进行优化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#不对称分布"><span class="nav-number">2.7.</span> <span class="nav-text">不对称分布</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#何时使用k-最近邻算法"><span class="nav-number">3.</span> <span class="nav-text">何时使用k-最近邻算法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#练习"><span class="nav-number">4.</span> <span class="nav-text">练习</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">倔强的土豆</span></div><div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div><span class="post-meta-divider">|</span><div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.3</div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script><link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css"><script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script><script type="text/javascript">function renderGitment(){var a=new Gitmint({id:window.location.pathname,owner:"laiqun",repo:"laiqun.github.io",lang:navigator.language||navigator.systemLanguage||navigator.userLanguage,oauth:{client_secret:"55aaeb736714431ea52109dd66461b1644ca6177",client_id:"c90dfa80285ea91b9120"}});a.render("gitment-container")}renderGitment()</script></body></html><!-- rebuild by neat -->