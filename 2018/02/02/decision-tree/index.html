<!-- build time:Sat Feb 10 2018 18:17:34 GMT+0800 (CST) --><!DOCTYPE html><html class="theme-next mist" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3"><link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222"><meta name="keywords" content="Hexo, NexT"><meta name="description" content="到目前为止，我们已经掌握了几种不同的自动分类器算法，本章我们将对此做进一步的延伸，介绍一种非常有用的算法，叫做决策树学习。不同于其他大多数分类器，由决策树产生的模型具有易于解释的特点——贝叶斯分类器中数字列表会告诉我们每个单词的重要程序，但是你必须经过计算才能得知整篇文档的结果如何。理解神经网络的难度更大，因为位于两个"><meta property="og:type" content="article"><meta property="og:title" content="决策树建模"><meta property="og:url" content="laiqun.github.io/2018/02/02/decision-tree/index.html"><meta property="og:site_name" content="广阔天地，大有作为"><meta property="og:description" content="到目前为止，我们已经掌握了几种不同的自动分类器算法，本章我们将对此做进一步的延伸，介绍一种非常有用的算法，叫做决策树学习。不同于其他大多数分类器，由决策树产生的模型具有易于解释的特点——贝叶斯分类器中数字列表会告诉我们每个单词的重要程序，但是你必须经过计算才能得知整篇文档的结果如何。理解神经网络的难度更大，因为位于两个神经元之间的连接上的权重值本身没有什么实际意义。而对于决策树，我们只需要通过观察"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="/2018/02/02/decision-tree/tree_example.jpg"><meta property="og:image" content="/2018/02/02/decision-tree/tree_step1.jpg"><meta property="og:image" content="/2018/02/02/decision-tree/tree_step2.jpg"><meta property="og:image" content="/2018/02/02/decision-tree/visual_tree.jpg"><meta property="og:updated_time" content="2018-02-04T14:17:35.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="决策树建模"><meta name="twitter:description" content="到目前为止，我们已经掌握了几种不同的自动分类器算法，本章我们将对此做进一步的延伸，介绍一种非常有用的算法，叫做决策树学习。不同于其他大多数分类器，由决策树产生的模型具有易于解释的特点——贝叶斯分类器中数字列表会告诉我们每个单词的重要程序，但是你必须经过计算才能得知整篇文档的结果如何。理解神经网络的难度更大，因为位于两个神经元之间的连接上的权重值本身没有什么实际意义。而对于决策树，我们只需要通过观察"><meta name="twitter:image" content="/2018/02/02/decision-tree/tree_example.jpg"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",version:"5.1.3",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!0},fancybox:!0,tabs:!0,motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="laiqun.github.io/2018/02/02/decision-tree/"><title>决策树建模 | 广阔天地，大有作为</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">广阔天地，大有作为</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">你看到我的筋斗云了嘛？</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="laiqun.github.io/2018/02/02/decision-tree/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="倔强的土豆"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="广阔天地，大有作为"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">决策树建模</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-02T22:50:08+08:00">2018-02-02 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/02/02/decision-tree/#comments" itemprop="discussionUrl"><span class="post-comments-count gitment-comments-count" data-xid="/2018/02/02/decision-tree/" itemprop="commentsCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><p>到目前为止，我们已经掌握了几种不同的自动分类器算法，本章我们将对此做进一步的延伸，介绍一种非常有用的算法，叫做<strong>决策树学习</strong>。不同于其他大多数分类器，由决策树产生的模型具有易于解释的特点——贝叶斯分类器中数字列表会告诉我们每个单词的重要程序，但是你必须经过计算才能得知整篇文档的结果如何。理解神经网络的难度更大，因为位于两个神经元之间的连接上的权重值本身没有什么实际意义。而对于决策树，我们只需要通过观察就可以理解其推导的过程，我们甚至还可以将其转换成一系列简单的if-then语句。<br>本章会举一个例子，向大家示范了如何预测一个网站上有多少用户有可能会愿意为某些高级功能而付费。许多在线应用都提供一种付费之前先试用的方式。网站通常会提供有时间限制的试用版本或者提供功能受限的免费版本，用户体验之后再决定是否购买。</p><h1 id="预测用户的购买行为"><a href="#预测用户的购买行为" class="headerlink" title="预测用户的购买行为"></a>预测用户的购买行为</h1><p>有时，当拥有很大访问量的站点退出新应用时，会提供免费试用的机会。这些试用的人群多是受好奇新的驱使，而非针对性的寻找某款应用，因此他们成为付费用户的可能性是非常小的。这种情况令我们很难识别和跟踪潜在用户，于是许多站点就采取了向所有用户群发邮件的方式，而没有采用更有针对性的方法。<br>为了解决上述问题，假如我们能够预测出一位用户成为付费顾客的可能性有多大，那将是一项非常有价值的工作。到目前为止，我们已经知道可以利用贝叶斯分类器和神经网络分类器来完成这一功能。然而在这里，我们强调的是算法的清晰直观，如果我们知道哪些因素可以表明用户将成为付费顾客，那么就可以利用这些信息来指导我们的广告策略制定工作，让网站的某些功能具有更好的可用性，或者采取其他能够增加付费客户数量的策略。<br>此处，假设我们有一个提供免费试用的在线应用。用户为了获得试用的机会而注册了账号，待试用了若干天之后，他们可以选择购买基本服务或者高级服务。因为用户为了免费试用的需要注册账号，所以我们也可以收集用户的注册相关信息，并在试用结束的时候，记录那些用户成为了付费用户。<br>为了尽量减少用户的工作量，使其能够尽快的注册账号，网站不会过多的询问用户的个人信息；相反，它会从服务器的日志中收集这些信息，比如：用户来自哪个网站、所在的地理位置、他们在注册之前曾经浏览过多少网页等等。假设我们收集了这些数据、并且将它们填入到一张表格中，如下表所示：</p><table><thead><tr><th>来源网站（Referrer）</th><th>位置</th><th>是否阅读过FAQ</th><th>浏览网页数</th><th>购买的服务类型</th></tr></thead><tbody><tr><td>Slashdot</td><td>USA</td><td>Yes</td><td>18</td><td>None</td></tr><tr><td>Google</td><td>France</td><td>Yes</td><td>23</td><td>Premium</td></tr><tr><td>Digg</td><td>USA</td><td>Yes</td><td>24</td><td>Basic</td></tr><tr><td>Kiwitobes</td><td>France</td><td>Yes</td><td>23</td><td>Basic</td></tr><tr><td>Google</td><td>UK</td><td>No</td><td>21</td><td>Premium</td></tr><tr><td>(direct)</td><td>New Zealand</td><td>No</td><td>12</td><td>None</td></tr><tr><td>(direct)</td><td>UK</td><td>No</td><td>21</td><td>Basic</td></tr><tr><td>Google</td><td>USA</td><td>No</td><td>24</td><td>Premium</td></tr><tr><td>Slashdot</td><td>France</td><td>Yes</td><td>19</td><td>None</td></tr><tr><td>Digg</td><td>USA</td><td>No</td><td>18</td><td>None</td></tr><tr><td>Google</td><td>UK</td><td>No</td><td>18</td><td>None</td></tr><tr><td>Kiwitobes</td><td>UK</td><td>No</td><td>19</td><td>None</td></tr><tr><td>Digg</td><td>New Zealand</td><td>Yes</td><td>12</td><td>Basic</td></tr><tr><td>Google</td><td>UK</td><td>Yes</td><td>18</td><td>Basic</td></tr><tr><td>Kiwitobes</td><td>France</td><td>Yes</td><td>19</td><td>Basic</td></tr></tbody></table><p>上述的表格最后一列代表用户购买的服务类型，这正是我们要测量的信息。我们将上述信息整理到一个由一行行数据组成的列表里，列表中的每一行都是由上述表格中一条记录。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">my_data=[[<span class="string">'slashdot'</span>,<span class="string">'USA'</span>,<span class="string">'yes'</span>,<span class="number">18</span>,<span class="string">'None'</span>],</span><br><span class="line">        [<span class="string">'google'</span>,<span class="string">'France'</span>,<span class="string">'yes'</span>,<span class="number">23</span>,<span class="string">'Premium'</span>],</span><br><span class="line">        [<span class="string">'digg'</span>,<span class="string">'USA'</span>,<span class="string">'yes'</span>,<span class="number">24</span>,<span class="string">'Basic'</span>],</span><br><span class="line">        [<span class="string">'kiwitobes'</span>,<span class="string">'France'</span>,<span class="string">'yes'</span>,<span class="number">23</span>,<span class="string">'Basic'</span>],</span><br><span class="line">        [<span class="string">'google'</span>,<span class="string">'UK'</span>,<span class="string">'no'</span>,<span class="number">21</span>,<span class="string">'Premium'</span>],</span><br><span class="line">        [<span class="string">'(direct)'</span>,<span class="string">'New Zealand'</span>,<span class="string">'no'</span>,<span class="number">12</span>,<span class="string">'None'</span>],</span><br><span class="line">        [<span class="string">'(direct)'</span>,<span class="string">'UK'</span>,<span class="string">'no'</span>,<span class="number">21</span>,<span class="string">'Basic'</span>],</span><br><span class="line">        [<span class="string">'google'</span>,<span class="string">'USA'</span>,<span class="string">'no'</span>,<span class="number">24</span>,<span class="string">'Premium'</span>],</span><br><span class="line">        [<span class="string">'slashdot'</span>,<span class="string">'France'</span>,<span class="string">'yes'</span>,<span class="number">19</span>,<span class="string">'None'</span>],</span><br><span class="line">        [<span class="string">'digg'</span>,<span class="string">'USA'</span>,<span class="string">'no'</span>,<span class="number">18</span>,<span class="string">'None'</span>],</span><br><span class="line">        [<span class="string">'google'</span>,<span class="string">'UK'</span>,<span class="string">'no'</span>,<span class="number">18</span>,<span class="string">'None'</span>],</span><br><span class="line">        [<span class="string">'kiwitobes'</span>,<span class="string">'UK'</span>,<span class="string">'no'</span>,<span class="number">19</span>,<span class="string">'None'</span>],</span><br><span class="line">        [<span class="string">'digg'</span>,<span class="string">'New Zealand'</span>,<span class="string">'yes'</span>,<span class="number">12</span>,<span class="string">'Basic'</span>],</span><br><span class="line">        [<span class="string">'slashdot'</span>,<span class="string">'UK'</span>,<span class="string">'no'</span>,<span class="number">21</span>,<span class="string">'None'</span>],</span><br><span class="line">        [<span class="string">'google'</span>,<span class="string">'UK'</span>,<span class="string">'yes'</span>,<span class="number">18</span>,<span class="string">'Basic'</span>],</span><br><span class="line">        [<span class="string">'kiwitobes'</span>,<span class="string">'France'</span>,<span class="string">'yes'</span>,<span class="number">19</span>,<span class="string">'Basic'</span>]]</span><br></pre></td></tr></table></figure><p></p><p>除了之间将数据写入代码之外，我们还可以从文件中获取数据。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">my_data = [line.split(<span class="string">'\t'</span>) <span class="keyword">for</span> line <span class="keyword">in</span> file(<span class="string">'data.txt'</span>)]</span><br></pre></td></tr></table></figure><p></p><p>现在，我们已经掌握了用户相关的信息，包括：他们是通过哪些网站访问到这里的、用户所在的位置、在访问这个网站之前浏览了多少网站。我们只需要一种方法，能够将合理的推测值填入”购买的服务类型”一栏即可。</p><h1 id="引入决策树"><a href="#引入决策树" class="headerlink" title="引入决策树"></a>引入决策树</h1><p>相比于其它方法，决策树是一种更为简单的机器学习方法。它是对被观测数据进行分类的一种相当直观的方法，决策树在经过训练之后，看起来就像是以树状形式排列的一系列if-then语句。下图展示了一个利用决策树对水果进行分类的例子。<br><img src="/2018/02/02/decision-tree/tree_example.jpg" title="决策树示例"><br>一旦我们有了决策树，根据它进行决策的过程就变得相当容易理解了。只要沿着树的路径一直向下，正确回答每一个问题，最终就会得到答案。沿着最终的叶节点向上回溯，我们就会得到一个有关最终分类结果的推理过程。</p><p>本章我们将着手查看一种决策树的表示法，我们会编写代码，利用真实数据来构造决策树，并对新遇到的观测数据进行分类。首先，我们来构造决策树的表达方式。我们创建一个类表示树上的每一个节点：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">decisionnode</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,col=<span class="number">-1</span>,value=None,results=None,tb=None,fb=None)</span>:</span></span><br><span class="line">    self.col=col</span><br><span class="line">    self.value=value</span><br><span class="line">    self.results=results</span><br><span class="line">    self.tb=tb</span><br><span class="line">    self.fb=fb</span><br></pre></td></tr></table></figure><p></p><p>每一个节点都有5个实例变量，这5个变量都是在initializer中设置的。</p><ul><li>col 代表用来判断的属性，具体的说：待检验的判断条件所对应的列索引值。对于此例子，来源网站信息的索引值为0，位置信息的索引值为1，以此类推。</li><li>value 给定了用来判断的属性之后，为了使得判断结果为true，该记录的该属性必须满足的值。</li><li>tb和fb也是decisionnode ，它们对应于结果分别为true或false时，相对于当前节点的左右两个分支节点。</li><li>results 保存的是针对当前分支的结果，它是一个字典。除了叶子节点之外，其他节点上该值都是None。</li></ul><p>构造决策树的函数将会返回一个根节点，我们可以沿着它的True或者False分值一直遍历下去，直到达到最终结果为止。</p><h1 id="对树进行训练"><a href="#对树进行训练" class="headerlink" title="对树进行训练"></a>对树进行训练</h1><p>本章我们将使用一种叫做CART（Classification and Regression Trees的缩写，即分类回归树）的算法。<br>为了构造决策树，算法首先创建一个根节点。然后通过评估表中的所有观测变量，从中选出最合适的变量对数据进行拆分。为此，算法考察了所有不同的变量，然后从中选择出一个条件(比如：用户是否读过了FAQ?) 对结果数据进行分解，使得我们了解这一条件对用户做出决定的影响如何。</p><h2 id="对数据进行拆分"><a href="#对数据进行拆分" class="headerlink" title="对数据进行拆分"></a>对数据进行拆分</h2><p>函数divideset的作用是根据列表中的某一列的数据将列表拆分成两个数据集。该函数接受一个列表、一个代表列索引值的序号、对列进行拆分的参考值作为参数。以”是否阅读过FAQ”为例，它的可能取值为Yes或No，而对于“来源网站”而言，则会有很多可能的取值。函数会返回两个列表：第一个列表所包含的数据行，表示这些数据行的该列数据与我们指定的参考值相匹配；第二个列表，表示这些数据行的该列与我们指定的参考值不匹配。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在某一列上对数据集合进行拆分；该函数能够处理名词或者数值类型的数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">divideset</span><span class="params">(rows,column,value)</span>:</span></span><br><span class="line">  <span class="comment">#定义一个函数，其返回true时代表该行数据属于第一组；返回false时属于第二组</span></span><br><span class="line">  split_function = <span class="keyword">None</span></span><br><span class="line">  <span class="keyword">if</span> isinstance(value,int) <span class="keyword">or</span> isinstance(value,float):</span><br><span class="line">    split_function = <span class="keyword">lambda</span> row: row[column]&gt;=value</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    split_function = <span class="keyword">lambda</span> row: row[column]==value</span><br><span class="line">  <span class="comment">#将数据集拆分成两个集合，并返回</span></span><br><span class="line">  set1=[row <span class="keyword">for</span> row <span class="keyword">in</span> rows <span class="keyword">if</span> split_function(row)]</span><br><span class="line">  set2=[row <span class="keyword">for</span> row <span class="keyword">in</span> rows <span class="keyword">if</span> <span class="keyword">not</span> split_function(row)]</span><br><span class="line">  <span class="keyword">return</span> (set1,set2)</span><br></pre></td></tr></table></figure><p></p><p>上述代码创建了一个名为spit_fuction的函数，该函数根据数据集的类型（是否是数值型），对其进行拆分。</p><ul><li>如果数据是数值型的，split_function函数就会根据判断列中的数值是否大于参考值</li><li>如果不是数值型的，则函数智慧判断指定列是否与参考值相等。</li></ul><p>我们利用该函数将数据拆分成了两个集合，其中一个是split_function函数返回true的集合，另一个是返回false的集合。<br>我们来尝试运行下该函数:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#根据列索引号2，代表是否阅读过FAQ，选择yes的分为一类，其他情况为另一类</span></span><br><span class="line">divideset(my_data,<span class="number">2</span>,<span class="string">'yes'</span>)</span><br><span class="line"><span class="comment">#([['slashdot', 'USA', 'yes', 18, 'None'], ['google', 'France', 'yes', 23, 'Premium'],...]]</span></span><br><span class="line"><span class="comment">#[['google', 'UK', 'no', 21, 'Premium'], ['(direct)', 'New Zealand', 'no', 12, 'None'],...])</span></span><br></pre></td></tr></table></figure><p></p><p>下表给出了拆分结果：</p><table><thead><tr><th>True</th><th>Flase</th></tr></thead><tbody><tr><td>None</td><td>Premium</td></tr><tr><td>Premium</td><td>None</td></tr><tr><td>Basic</td><td>Basic</td></tr><tr><td>Basic</td><td>Premium</td></tr><tr><td>None</td><td>None</td></tr><tr><td>Basic</td><td>None</td></tr><tr><td>Basic</td><td>None</td></tr></tbody></table><p>目前来看，拆分所用的列并不是很理想，因为两边都混杂了各种情况。我们需要一种办法来选择最合适的列进行拆分。</p><h2 id="选择最合适的拆分方案"><a href="#选择最合适的拆分方案" class="headerlink" title="选择最合适的拆分方案"></a>选择最合适的拆分方案</h2><p>对于前述的观测数据而言，我们所选择的列并能很好的进行拆分。为了选择合适的列进行拆分，我们需要一种方法来衡量数据集合拆分后的混合情况。我们所要做的，就是找出合适的列，使得拆分的两个数据集合在混杂程度上尽可能的小。我们需要一个函数对数据集合的每一类结果进行计数。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对各种可能的结果进行计数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">uniquecounts</span><span class="params">(rows)</span>:</span></span><br><span class="line">  results=&#123;&#125;</span><br><span class="line">  <span class="keyword">for</span> row <span class="keyword">in</span> rows:</span><br><span class="line">    <span class="comment">#只统计最后一列的混合程度，该列代表用户购买结果</span></span><br><span class="line">    r = row[len(row)<span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">if</span> r <span class="keyword">not</span> <span class="keyword">in</span> results:</span><br><span class="line">      results[r]=<span class="number">0</span></span><br><span class="line">    results[r]+=<span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure><p></p><p>函数 uniquecounts 的作用是对不同类型的结果按类进行计数。其他函数可以利用该函数的计数结果来计算数据集合的混杂程度。对于混杂程度的测量，有几种不同的度量方式可供选择，此处我们只考察两种：基尼不纯度(Gini impurity)和熵(entropy)。</p><h2 id="基尼不纯度"><a href="#基尼不纯度" class="headerlink" title="基尼不纯度"></a>基尼不纯度</h2><p>基尼不纯度，是一种预期的误差率，它指的预计随机选一种结果在总体结果中出错的可能性。举例来说：如果集合中的每个数据项都属于同一分类，那么推测结果总是正确的，因此误差率为0。如果有4种可能的结果均匀的分布，随机选一种结果，则有75%的概率出错，因此误差率为0.75。<br>这里补上基尼不纯度的公式。<br>根据公式计算基尼不纯度的例子：<br>情况1. 硬币 正面和反面的概率为0.5。 基尼不纯度=0.5<em>0.5+0.5</em>0.5=0.5<br>情况2. 不均匀的硬币 正面概率为0.1 反面概率为0.9。 基尼不纯度=0.1<em>0.9+0.9</em>0.1=0.18<br>很明显，情况1比情况2更加混乱。</p><p>基尼不纯度的计算函数如下所示：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">giniimpurity</span><span class="params">(rows)</span>:</span></span><br><span class="line">  total = len(rows)</span><br><span class="line">  counts = uniquecounts(rows)</span><br><span class="line">  imp = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> k1 <span class="keyword">in</span> counts:</span><br><span class="line">    p1 = float(counts[k1])/total</span><br><span class="line">    <span class="keyword">for</span> k2 <span class="keyword">in</span> counts:</span><br><span class="line">      <span class="keyword">if</span> k1 == k2:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">      p2 = float(counts[k2])/total</span><br><span class="line">      imp+=p1*p2</span><br><span class="line">  <span class="keyword">return</span> imp</span><br></pre></td></tr></table></figure><p></p><p>该函数利用集合中的某一类结果出现的次数除上集合的总行数来计算相应的概率，然后将这些概率值的乘积累加起来。这样就能得到某一个类数据被随机分配到错误的类别中的概率。<br>基尼不纯度的概率值越大，说明混乱程度越高，也就说明拆分越不理想。概率值为0则代表拆分的结果非常理想，因为这说明每一行数据都被分配到了正确的类别中。</p><h2 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h2><p>在信息论中，熵代表是集合的无序程度——基本上就相当于我们在此处所说的集合的混杂程度。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 熵是遍历所有可能的结果之后得到的P(x)log(P(x))之和</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">entropy</span><span class="params">(rows)</span>:</span></span><br><span class="line">  <span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line">  log2 = <span class="keyword">lambda</span> x :log(x)/log(<span class="number">2</span>)</span><br><span class="line">  results = uniquecounts(rows)</span><br><span class="line">  <span class="comment"># 此处开机计算熵的值</span></span><br><span class="line">  ent = <span class="number">0.0</span></span><br><span class="line">  <span class="keyword">for</span> r <span class="keyword">in</span> results.keys():</span><br><span class="line">    p = float(results[r])/len(rows)</span><br><span class="line">    ent = ent - p*log2(p)</span><br><span class="line">  <span class="keyword">return</span> ent</span><br></pre></td></tr></table></figure><p></p><p>函数entropy计算了每一类数据出现的概率（即数据项出现的次数除上集合的总行数），并使用了如下公式:</p><blockquote><p>p(i)=frequncy(outcome) = count(outcome)/count(total rows)<br>Entropy = 针对所有结果的P(i)×log(P(i))之和</p></blockquote><p>这是一种衡量结果之间差异程度的测度方法。如果所有结果都相同（比如说，如果我们够幸运的话，所有人最终都成了付费用户），则熵为0。群组(groups)越是混乱，相应的熵就越高。我们之所以要将数据拆分成两个新组，其目的就是为了降低熵。<br>我们测试一下熵和基尼不纯度这两种度量方法：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">giniimpurity(my_data)</span><br><span class="line"><span class="comment">#0.6328125</span></span><br><span class="line">entropy(my_data)</span><br><span class="line"><span class="comment">#1.5052408149441479</span></span><br><span class="line">set1,set2=divideset(my_data,<span class="number">2</span>,<span class="string">'yes'</span>) </span><br><span class="line">entropy(set1)</span><br><span class="line"><span class="comment">#1.2987949406953985</span></span><br><span class="line">giniimpurity(set1)</span><br><span class="line"><span class="comment">#0.53125</span></span><br></pre></td></tr></table></figure><p></p><p>基尼不纯度和熵的主要区别在于，熵达到峰值的过程要相对慢一些。因此，熵对于混乱集合的“判罚”往往要更重一些。由于人们对熵的使用更加普遍，因此本章的后续部分将选择熵作为度量标准，不过我们如何想切换到基尼不纯度也是非常容易的。</p><h2 id="以递归的方式构造树"><a href="#以递归的方式构造树" class="headerlink" title="以递归的方式构造树"></a>以递归的方式构造树</h2><p>为了弄明白一个属性的好坏程度，我们的算法首先求出整个群组的熵，然后尝试利用每个属性的可能取值对群组进行拆分，并求出两个新群组的熵。<br>为了确定哪个属性最适合用来拆分，算法会计算相应的<strong>信息增益</strong>(Inforamtion gain)。所谓信息增益，是指当前熵与两个新群组经过加权平均后的熵之间的差值。算法会针对每个属性计算相应的信息增益，然后从中选择<strong>信息增益最大的属性</strong>。<br>待根节点处的判断条件确定之后，算法会根据该条件返回的True或False，分别建立两个分支。如下图所示：<br><img src="/2018/02/02/decision-tree/tree_step1.jpg" title="经过第一次拆分后的决策树"><br>算法将观测数据拆分成了两组，其中一组符合判断条件，另一组不符合判断条件。对于每个分支，算法随后会判断是否要对其做进一步的拆分，或者我们已经获得了一个明确的结论而无须再进行拆分了。如果新分支可以继续拆分，算法就会使用与上面同样的方法确定接下来到底应该选用哪一个变量，第二次拆分的情况如下图所示：<br><img src="/2018/02/02/decision-tree/tree_step2.jpg" title="经过第二次拆分后的决策树"><br>通过计算每个新生节点的最佳拆分属性，对分支的拆分过程和树的构造过程会不断的持续下去。当拆分某个节点所得的信息增益不大于0的时候，对分支的拆分才会停止。<br>我们新建一个函数buildtree，它是一个递归函数，它通过为当前数据集选择最合适的拆分条件来实现决策树的构造过程：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">buildtree</span><span class="params">(rows,scoref=entropy)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> len(rows)==<span class="number">0</span>:</span><br><span class="line">    <span class="keyword">return</span> decisionnode()</span><br><span class="line">  current_score = scoref(rows)</span><br><span class="line">  <span class="comment">#定义一些变量以记录最佳拆分条件</span></span><br><span class="line">  best_gain = <span class="number">0.0</span> <span class="comment">#指示拆分后的最佳增益量</span></span><br><span class="line">  best_criteria = <span class="keyword">None</span> <span class="comment">#表示判断条件的  包括选择的列属性和值</span></span><br><span class="line">  best_sets = <span class="keyword">None</span>     <span class="comment">#存放拆分后得到的两个集合</span></span><br><span class="line">  column_count = len(rows[<span class="number">0</span>])<span class="number">-1</span>  <span class="comment">#一行中的列数,最后一列除外，因为它是要预测的值</span></span><br><span class="line">  <span class="keyword">for</span> col <span class="keyword">in</span> range(<span class="number">0</span>,column_count):</span><br><span class="line">    <span class="comment"># 记下每一行记录的第col列的值，每个值都当做拆分条件试一下</span></span><br><span class="line">    column_values=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> rows:</span><br><span class="line">      <span class="comment"># row代表一行数据，row[col]为当前行数据在col列下的值</span></span><br><span class="line">      column_values[row[col]]=<span class="number">1</span> </span><br><span class="line">    <span class="comment"># 该列下每一行记录的值都试一下</span></span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> column_values.keys():</span><br><span class="line">      (set1,set2) = divideset(rows,col,value)</span><br><span class="line">      <span class="comment">#计算信息增益</span></span><br><span class="line">      p = float(len(set1))/len(rows)</span><br><span class="line">      gain = current_score-p*scoref(set1)-(<span class="number">1</span>-p)*scoref(set2)</span><br><span class="line">      <span class="keyword">if</span> gain&gt;best_gain <span class="keyword">and</span> len(set1)&gt;<span class="number">0</span> <span class="keyword">and</span> len(set2)&gt;<span class="number">0</span>:</span><br><span class="line">        best_gain = gain</span><br><span class="line">        best_criteria=(col,value)</span><br><span class="line">        best_sets=(set1,set2)</span><br><span class="line">  <span class="comment">#创建子分支</span></span><br><span class="line">  <span class="keyword">if</span> best_gain&gt;<span class="number">0</span>:<span class="comment">#大于0说明混乱程度降低</span></span><br><span class="line">    trueBranch  = buildtree(best_sets[<span class="number">0</span>])</span><br><span class="line">    falseBranch = buildtree(best_sets[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> decisionnode(col=best_criteria[<span class="number">0</span>],value=best_criteria[<span class="number">1</span>],tb=trueBranch,fb=falseBranch)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> dicisionnode(results=uniquecounts(rows))</span><br></pre></td></tr></table></figure><p></p><p>上述函数首先接受一个由数据行构成的列表作为参数。遍历所有的列，该该列下遍历每一条记录中的该列数据的所有取值（最后一列除外，因为它是存放最终结果的），尝试该列的所有取值作为拆分条件，将数据集拆分成两个子集。计算两个子集的加权平均熵与原始数据集的熵的差值。子集的熵的权重为该子集中数据集的大小除上原始数据集的大小。<br>如果两个子集加权平均的熵比原始数据集要大，表示更加混乱，则拆分过程就结束了，针对各个可能结果的计数所得将会被保存起来。否则，算法就会在新生成的子集熵继续调用buildtree函数，并把调用的结果添加到树上。我们把针对每个子集的调用结果分别加到节点的True分支和False分支上，最终整棵树就这样构造出来了。<br>现在，我们可以将算法最终应用到整个原始数据集上了。上述代码足够灵活，它可以同时处理文本类型数据和数值类型数据。代码还假定了数据集的最后一列为对应的目标值，因为我们只需要简单的将数据集传进去，就可以构造出决策树。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tree = buildtree(my_data)</span><br></pre></td></tr></table></figure><p></p><p>现在，变量tree中保存着一个经过训练的决策树。稍后我们将学习如何显示该决策树，以及如何在新数据上做出预测。</p><h2 id="决策树的显示"><a href="#决策树的显示" class="headerlink" title="决策树的显示"></a>决策树的显示</h2><p>先以纯文本的方式显示整个树，虽然输出结果不算很美观，但对于节点不多的树而言，这不失为一种简单的方法:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printtree</span><span class="params">(tree,indent=<span class="string">''</span>)</span>:</span></span><br><span class="line">  <span class="comment">#这是一个叶节点吗？</span></span><br><span class="line">  <span class="keyword">if</span> tree.results != <span class="keyword">None</span>:</span><br><span class="line">    print(str(tree.results))</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="comment">#打印判断条件</span></span><br><span class="line">    print(str(tree.col)+<span class="string">':'</span>+str(tree.value))+<span class="string">'? '</span></span><br><span class="line">    <span class="comment">#打印分支</span></span><br><span class="line">    print(indent+<span class="string">'T-&gt;'</span>)</span><br><span class="line">    printtree(tree.tb,indent+<span class="string">'  '</span>)</span><br><span class="line">    print(indent+<span class="string">'F-&gt;'</span>)</span><br><span class="line">    printtree(tree.fb,indent+<span class="string">'  '</span>)</span><br></pre></td></tr></table></figure><p></p><p>这又是一个递归函数。它接受buildtree产生的树作为参数，然后沿着树向下遍历，当函数到达一个包含结果信息的节点时，它就知道已经达到了一个分支的末端。否则，它就会打印出针对True分支和False分支的判断条件，并针对每一个分支递归的调用printtree。没调用一次，缩进字符串就增加一格。<br>我们针对前面构造的树调用一下此函数，查看结果：<br></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">printtree(tree) </span><br><span class="line">0:google?</span><br><span class="line">  T-&gt; 3:21?</span><br><span class="line">    T-&gt; &#123;'Premium': 3&#125;</span><br><span class="line">    F-&gt; 2:yes?</span><br><span class="line">      T-&gt; &#123;'Basic': 1&#125;</span><br><span class="line">      F-&gt; &#123;'None': 1&#125;</span><br><span class="line">  F-&gt; 0:slashdot?</span><br><span class="line">    T-&gt; &#123;'None': 3&#125;</span><br><span class="line">    F-&gt; 2:yes?</span><br><span class="line">      T-&gt; &#123;'Basic': 4&#125;</span><br><span class="line">      F-&gt; 3:21?</span><br><span class="line">        T-&gt; &#123;'Basic': 1&#125;</span><br><span class="line">        F-&gt; &#123;'None': 3&#125;</span><br></pre></td></tr></table></figure><p></p><p>这是决策树的推理过程的一个可视化表达，位于根节点的判断条件是“来源网站（数据表的第0列）是google吗？”。如果这个条件满足，算法就会走True分支，判断用户“已浏览网页数(数据表中的第3列)超过21个吗？”。如果超过21个，则估计的结果是“该用户会购买基础服务(Basic)”。其他的推理过程以此类推。正如前面提到的，决策树能够直观的看到隐藏在推理过程背后的逻辑，具有易于解释的特点。</p><h2 id="图形显示方式"><a href="#图形显示方式" class="headerlink" title="图形显示方式"></a>图形显示方式</h2><p>文本显示方式对于节点不太多的树而言是可行的，但是随着树的规模逐渐变大，文本的可视化方式来跟踪我们在树上所走的路径可能是非常困难的。此处，我们改用图形化的表现形式。<br>绘制树的代码和之前将聚类时说过的分级聚类可视化的代码是类似的。两者都涉及了绘制具有任意深度节点的二叉树，因此我们首先需要编写函数来确定，一个给定的节点需要占据多少空间——包括所有子节点的总宽度，以及节点所要到达的深度值，深度值会告诉我们，为了容纳所有分支，节点在垂直方向上所需要的空间。</p><ol><li><p>一个分支的总宽度等于其所有子分支的宽度之和，如果节点没有子分支的话，则对应宽度为1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getwidth</span><span class="params">(tree)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> tree.tb == <span class="keyword">None</span> <span class="keyword">and</span> tree.fb == <span class="keyword">None</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> getwidth(tree.tb)+getwidth(tree.fb)</span><br></pre></td></tr></table></figure></li><li><p>一个分支的深度等于其最长子分支的总深度+1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getdepth</span><span class="params">(tree)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> tree.tb == <span class="keyword">None</span> <span class="keyword">and</span> tree.fb==<span class="keyword">None</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  <span class="keyword">return</span> max(getdepth(tree.tb),getdepth(tree.fb))+<span class="number">1</span></span><br></pre></td></tr></table></figure></li></ol><p>为了绘制树，我们需要安装pillow，这是一个绘制图形用的库。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image,ImageDraw</span><br></pre></td></tr></table></figure><p></p><p>函数drawtree为待绘制的树确定一个合理的尺寸，并设置好画布(canvas)。然后将画布和树的根节点传递给drawnode。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">drawtree</span><span class="params">(tree,jpeg=<span class="string">'tree.jpg'</span>)</span>:</span></span><br><span class="line">  w = getwidth(tree)*<span class="number">100</span></span><br><span class="line">  h = getdepth(tree)*<span class="number">100</span>+<span class="number">120</span></span><br><span class="line">  img = Image.new(<span class="string">'RGB'</span>,(w,h),(<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>))</span><br><span class="line">  draw = ImageDraw.Draw(img)</span><br><span class="line">  drawnode = (draw,tree,w/<span class="number">2</span>,<span class="number">20</span>)</span><br><span class="line">  img.save(jped,<span class="string">'JPEG'</span>)</span><br></pre></td></tr></table></figure><p></p><p>函数drawnode实际用于绘制决策树的节点。它以递归的方式工作，首先绘制当前节点，并计算子节点的位置，然后再每个节点上再次调用drawnode。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">drawnode</span><span class="params">(draw,tree,x,y)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> tree.results == <span class="keyword">None</span>: <span class="comment">#如果不是叶子节点</span></span><br><span class="line">    <span class="comment">#得到每个分支的宽度</span></span><br><span class="line">    w1 = getwidth(tree.fb)*<span class="number">100</span></span><br><span class="line">    w2 = getwidth(tree.tb)*<span class="number">100</span></span><br><span class="line">    <span class="comment">#确定此节点所要占据的总空间</span></span><br><span class="line">    left  = x-(w1+w2)/<span class="number">2</span></span><br><span class="line">    right = x+(w1+w2)/<span class="number">2</span></span><br><span class="line">    <span class="comment">#绘制判断条件字符串</span></span><br><span class="line">    draw.next((x<span class="number">-20</span>,y<span class="number">-10</span>),str(tree.col)+<span class="string">':'</span>+str(tree.value),(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">    <span class="comment">#绘制到分支的连线</span></span><br><span class="line">    draw.line((x,y,left+w1/<span class="number">2</span>,y+<span class="number">100</span>),fill=(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">    draw.line((x,y,left-w2/<span class="number">2</span>,y+<span class="number">100</span>),fill=(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">    <span class="comment">#绘制分支的节点</span></span><br><span class="line">    drawnode(draw,tree.fb,left+w1/<span class="number">2</span>,y+<span class="number">100</span>)</span><br><span class="line">    drawnode(draw,tree.tb,left-w2/<span class="number">2</span>,y+<span class="number">100</span>)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    txt = <span class="string">'\n'</span>.join(<span class="string">'%s:%d'</span> %v <span class="keyword">for</span> v <span class="keyword">in</span> tree.results.items())</span><br><span class="line">    draw.text((x<span class="number">-20</span>,y),txt,(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>))</span><br></pre></td></tr></table></figure><p></p><p>调用该函数:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drawtree(tree,jpeg=<span class="string">'treeview.jpg'</span>)</span><br></pre></td></tr></table></figure><p></p><p>得到的图如下所示：<br><img src="/2018/02/02/decision-tree/visual_tree.jpg" title="用于预测客户购买服务的决策树"><br>此时代码没有打印出True和False分支的标签，在更为复杂的图中，这些标签只会造成布局更加混乱。我们规定，True分支总是位于右侧。</p><h2 id="在新数据上进行预测"><a href="#在新数据上进行预测" class="headerlink" title="在新数据上进行预测"></a>在新数据上进行预测</h2><p>目前，我们还需要一个函数，接受新的观测数据作为参数，然后根据决策树对其进行分类。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(observation,tree)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> tree.results!=<span class="keyword">None</span>:</span><br><span class="line">    <span class="keyword">return</span> tree.results</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    v = observation[tree.col]</span><br><span class="line">    branch = <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(v,int) <span class="keyword">or</span> isinstance(v,float):</span><br><span class="line">      <span class="keyword">if</span> v&gt;tree.value:</span><br><span class="line">        branch = tree.tb</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        branch = tree.fb</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">if</span> v == tree.value:</span><br><span class="line">        branch = tree.tb</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        branch = tree.fb</span><br><span class="line">    <span class="keyword">return</span> classify(observation,branch)</span><br></pre></td></tr></table></figure><p></p><p>该函数采用与printtree完全相同的方式对树进行遍历。在每次调用之后，函数会根据调用结果来判断是否到达分支的末端。如果没有到达末端，它会对观测的数据做出评估，以确认列数据是否和参考值相匹配。如果匹配，则会在True分支上再次调用classify；如果不匹配，则在False分支上调用classify。<br>我们现在可以测试一下这个函数：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">classify([<span class="string">'direct'</span>,<span class="string">'USA'</span>,<span class="string">'yes'</span>,<span class="number">5</span>],tree)</span><br><span class="line"><span class="comment"># &#123;‘Basic’:4&#125;</span></span><br></pre></td></tr></table></figure><p></p><p>至此，我们已经拥有了从数据集中构造决策树的函数，图形化决策树的函数、以及对新的观测数据进行分类的函数。我们可以将这些函数应用到任何形式的数据集上，只要数据集是多个数据行组成，并且每一行数据都包含一组观测变量和一个结果值即可。</p></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>如果您觉得读完本文有收获，不妨小额赞助我一下，让我有动力继续写出高质量的教程！</div><button id="rewardButton" disable="enable"><span>打赏</span></button><div id="QR" style="display:block"><div id="wechat" style="display:inline-block"><img id="wechat_qr" src="/images/smacker.jpg" alt="倔强的土豆 微信支付"><p>微信支付</p></div></div></div></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2018/01/22/document-filtering/" rel="next" title="文档过滤"><i class="fa fa-chevron-left"></i> 文档过滤</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"></div></div></footer></div></article><div class="post-spread"></div></div></div><div class="comments" id="comments"><div id="gitment-container"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div id="sidebar-dimmer"></div><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">倔强的土豆</p><p class="site-description motion-element" itemprop="description">分享机器学习、深度学习的点滴</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">8</span> <span class="site-state-item-name">日志</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/laiqun" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:laiqun@msn.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#预测用户的购买行为"><span class="nav-number">1.</span> <span class="nav-text">预测用户的购买行为</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#引入决策树"><span class="nav-number">2.</span> <span class="nav-text">引入决策树</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#对树进行训练"><span class="nav-number">3.</span> <span class="nav-text">对树进行训练</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#对数据进行拆分"><span class="nav-number">3.1.</span> <span class="nav-text">对数据进行拆分</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#选择最合适的拆分方案"><span class="nav-number">3.2.</span> <span class="nav-text">选择最合适的拆分方案</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基尼不纯度"><span class="nav-number">3.3.</span> <span class="nav-text">基尼不纯度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#熵"><span class="nav-number">3.4.</span> <span class="nav-text">熵</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#以递归的方式构造树"><span class="nav-number">3.5.</span> <span class="nav-text">以递归的方式构造树</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树的显示"><span class="nav-number">3.6.</span> <span class="nav-text">决策树的显示</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#图形显示方式"><span class="nav-number">3.7.</span> <span class="nav-text">图形显示方式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#在新数据上进行预测"><span class="nav-number">3.8.</span> <span class="nav-text">在新数据上进行预测</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">倔强的土豆</span></div><div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div><span class="post-meta-divider">|</span><div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.3</div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script><link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css"><script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script><script type="text/javascript">function renderGitment(){var a=new Gitmint({id:window.location.pathname,owner:"laiqun",repo:"laiqun.github.io",lang:navigator.language||navigator.systemLanguage||navigator.userLanguage,oauth:{client_secret:"55aaeb736714431ea52109dd66461b1644ca6177",client_id:"c90dfa80285ea91b9120"}});a.render("gitment-container")}renderGitment()</script></body></html><!-- rebuild by neat -->